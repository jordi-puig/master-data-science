---
title: 'Mineria de dades: PEC2 - Mètodes no supervisats'
author: "Autor: Nom estudiant"
date: "Abril 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PAC-header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
******
# Introducció
******
## Presentació
Aquesta Prova d'Avaluació Continuada cobreix principalment els mòduls 5 i 6 (Mètodes d'agregació i algoritmes d'associació) del programa de l'assignatura

## Competències
Les competències que es treballen en aquesta prova són:

* Ús i aplicació de les TIC en l'àmbit académic i professional
* Capacitat per  innovar i generar noves idees.
* Capacitat per avaluar solucions tecnològiques i elaborar propostes de projectes tenint en compte els recursos, les alternatives disponibles i les condicions de mercat.
* Conèixer les tecnologies de comunicacions actuals i emergents i saber-les aplicar convenientment per a dissenyar i desenvolupar solucions basades en sistemes i tecnologies de la informació.
* Aplicació de les tècniques específiques d'enginyeria del programari a les diferents etapes del cicle de vida d'un projecte.
* Capacitat per aplicar les tècniques específiques de tractament, emmagatzematge i administració de dades.
* Capacitat per proposar i avaluar diferents alternatives tecnològiques per a resoldre un problema concret.

## Objectius
En aquesta PAC treballarem la generació, intepretació i evaluació d'un model d'agregació i d'un model on generarem regles d'associació amb el software de pràctiques. No perdrem de vista les fases de preparació de les dades, qualitat del model i extracció inicial del coneixement.

## Descripció de la PAC a realitzar

## Recursos Bàsics
**Material docent proporcionat per la UOC.** 

Mòdul 5, 6 i 8 del material didàctico.

## Criteris de valoració

**Exercicis teòrics** 

Tots els exercicis han de ser presentats de forma raonada i clara, especificant tots i cadascun dels passos que s'hagin dut a terme per a la seva resolució. No s'acceptarà cap resposta que no estigui clarament justificada.

**Exercicis pràctics**

Per a totes les PAC és necessari documentar en cada apartat de l'exercici pràctic qué s'ha fet i com s'ha fet, quin era l'objectiu i com s'ha desenvolupat.

##Format i data d'entrega

El format de lliurament és: usernameestudiant-PACn.html/doc/docx/odt/pdf

Data de lliurament: 22/04/2020

Cal lliurar la PAC a la bústia de lliuraments de l'aula

## Nota: Propietat intelectual

> Sovint és inevitable, al produir una obra multimèdia, fer és de recursos creats per terceres persones. És per tant comprensible fer-lo en el marc d'una pràctica dels Estudis, sempre que això es documenti clarament i no suposi plagi en la pràctica.

> Per tant, al presentar una pràctica que faci és de recursos aliens, s'ha de presentar juntament amb ella un document en quin es detallin tots ells, especificant el nom de cada recurs, el seu autor, el lloc on es va obtenir i el seu estatus legal: si l'obra està protegida pel copyright o s'acull a alguna altra llicència d'ús (Creative Commons, llicència GNU, GPL ...).

> L'estudiant haurà d'assegurar-se que la llicència no impedeix específicament el seu ús en el marc de la pràctica. En cas de no trobar la informació corresponent haurà d'assumir que l'obra està protegida per copyright.
Hauríeu de, a más, adjuntar els fitxers originals quan les obres utilitzades siguin digitals, i el seu codi font si correspon

******
# Exemplo 1.1
## Mètodos de agregació amb dades autogenerades
******
En aquest exemple generarem un conjunt de mostres aleatòries per a posteriorment fer servir l'algoirtme kmeans per agruparles. Es crearan les mostres al voltant de dos punts concrets. Així doncs, lo lògic serà agrupar en dos clústers. Donat que inicialment, en un problmea rea, no es coneix quin es el número correcte de clústers k, anem a provar primer amb dos (el valor òptim) i posteriorment amb 4 i 8 clústers. Per a evaluar la qualitat de cada procès d'agrupació anem a fer serivr la silueta mitjana. La silueta de cada mostra evalua com de bé o malament està classificada cada mostra en el clúster al que ha estat assignada. Per això es fa servir una fòrmula que té en compte la distància a les mostres del seu clúster i la distància a les mostres del clúster veí més proper.

A l'hora de provar el códi que es mostra, és important tenir en compte que les mostres es generan de forma aleatoria i també que l'algoritme kmeans té una inicialització aleatoria. Així doncs, en cada execució, s'obtindran uns resultats lleugerament diferents.

Lo primer que fem es carregar la llibrería cluster que conté les funcions que es necessiten

```{r message= FALSE, warning=FALSE}
library(cluster)
```
Generem les mostres de forma aleatòria prenent com a centre els punts [0,0] y [5,5].
```{r message= FALSE, warning=FALSE}
n       <- 150 # número de mostres
p       <- 2   # dimensións

sigma <- 1          # variança de la distribució
mean1 <- 0          # centre del primer grupo
mean2 <- 5          # centre del segon grupo
n1    <- round(n/2) # número de mostras del primer grup
n2    <- round(n/2) # número de mostras del segon grup

x1 <- matrix(rnorm(n1*p,mean=mean1,sd=sigma),n1,p)
x2 <- matrix(rnorm(n2*p,mean=mean2,sd=sigma),n2,p)
```

Juntem totes les mostres generades i les mostrem en una gràfica
```{r message= FALSE, warning=FALSE}
x  <- rbind(x1,x2)
plot (x)
```
Com es pot comprovar les mostres estan clarament separades en dos grups. Si es vol complicar el problema es pot modificar els punts centrals (mean1 i mean2) fent que estiguin més propers i/o ampliar la variança (sigma) per a que les mostres estiguin més disperses. 

A continuació anem aplicar l'algoritme kmeans amb 2, 4 i 8 clústers
```{r message= FALSE, warning=FALSE}
fit2       <- kmeans(x, 2)
y_cluster2 <- fit2$cluster

fit4       <- kmeans(x, 4)
y_cluster4 <- fit4$cluster

fit8       <- kmeans(x, 8)
y_cluster8 <- fit8$cluster
```
Les variables y_cluster2, y_cluster4 i y_cluster8 contenen per a cada mostra l'identificador del clúster a les quehan estat asignades. Per exemple, en el cas dels k=2 les mostres s'han assignat al clúster 1 o al 2.

```{r message= FALSE, warning=FALSE}
y_cluster2
```

Per a visualitzar els clústers podem fer serivr la funció clusplot. Mirem l'agrupació amb 2 clústers.
```{r message= FALSE, warning=FALSE}
clusplot(x, fit2$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

amb 4
```{r message= FALSE, warning=FALSE}
clusplot(x, fit4$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

i amb 8
```{r message= FALSE, warning=FALSE}
clusplot(x, fit8$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```
També podem visualitzar el resultat del procès d'agrupament amb el següent codi per al cas de 2 clústers
```{r message= FALSE, warning=FALSE}
plot(x[y_cluster2==1,],col='blue', xlim=c(min(x[,1]), max(x[,1])), ylim=c(min(x[,2]), max(x[,2])))
points(x[y_cluster2==2,],col='red')
```

per a 4
```{r message= FALSE, warning=FALSE}

plot(x[y_cluster4==1,],col='blue', xlim=c(min(x[,1]), max(x[,1])), ylim=c(min(x[,2]), max(x[,2])))
points(x[y_cluster4==2,],col='red')
points(x[y_cluster4==3,],col='green')
points(x[y_cluster4==4,],col='black')
```

i per a 8
```{r message= FALSE, warning=FALSE}
plot(x[y_cluster8==1,],col='blue', xlim=c(min(x[,1]), max(x[,1])), ylim=c(min(x[,2]), max(x[,2])))
points(x[y_cluster8==2,],col='red')
points(x[y_cluster8==3,],col='green')
points(x[y_cluster8==4,],col='black')
points(x[y_cluster8==5,],col='yellow')
points(x[y_cluster8==6,],col='purple')
points(x[y_cluster8==7,],col='cyan')
points(x[y_cluster8==8,],col='orange')
```

Ara anem a evaluar la qualitat de l'agregació. Per això farem servir la funció silhouette que calcula la silueta de cada mostra

```{r message= FALSE, warning=FALSE}
d  <- daisy(x) 
sk2 <- silhouette(y_cluster2, d)
sk4 <- silhouette(y_cluster4, d)
sk8 <- silhouette(y_cluster8, d)
```

La funció silhouette retorna per a cada mostra, el clúster on ha estat assignat, el clúster veí i el valor de la silueta. Així doncs, calculant la mitjana de la tercera columna podem obtenir una estimació de la qualitat de l'agrupament.

```{r message= FALSE, warning=FALSE}
mean(sk2[,3])
mean(sk4[,3])
mean(sk8[,3])
```

Com es pot comprovar, agrupar en dos clústers es millor que en 4 o en 8, lo qual es llògic tenin en compte com s'han generat les dades.

******
# Exemple 1.2
## Mètodes d'agregació amb dades reals
******

A continuació anem a veure un altre exemple de com es fan servir els models d'agregació. Per això farem servir el fitxer iris.csv. Aquesta base de dades es trova descrita en https://archive.ics.uci.edu/ml/datasets/iris. Aquest dataset està previament treballat per tal de que les dades estiguin netas i sense errors. De no ser així hauriem, abans de tot, buscar error, valors nuls o outliers. Hauriem de mirar de discretitzar o eliminar columnes. I aquest últim pas repetir-ho varies vegades per tal de comprovar els resultats de les diferents execucions. De totes formes anem a visualitzar la estructure i el resumen de les dades. 
```{r message= FALSE, warning=FALSE}
iris_data<-read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data", header=T, sep=",")
attach(iris_data)
colnames(iris_data) <- c("sepalLength", "sepalWidth", "petalLength", "petalWidth", "class")
summary(iris_data)
```

Como es pot comprovar, aquesta base de dades, està pensada per a problemes de classificació supervisada que preten classificar cada tipus de flor en un dels tipus existents (Iris-setosa, Iris-versicolor o Iris-virginica). Com en aquest exemple farem servir un mètode no supervisat, transformarem el problema supervisat original en un de no supervistat. Per aconseguir-ho no farem servir la columna class, que és la variable que es vol predeir. Així doncs, intentarem trovar agrupacions fent servir unicament els quatre atributs que caracteritzen a cada flor.
 
Carreguem les dades i ens quedem únicament amb les quatre columnes que defineixen a cada flor
```{r message= FALSE, warning=FALSE}
x <- iris_data[,1:4]
```

Com inicialment no coneixem el número òptim de clústers, provems amb varis valors
```{r message= FALSE, warning=FALSE}
d <- daisy(x) 
resultados <- rep(0, 10)
for (i in c(2,3,4,5,6,7,8,9,10))
{
  fit           <- kmeans(x, i)
  y_cluster     <- fit$cluster
  sk            <- silhouette(y_cluster, d)
  resultados[i] <- mean(sk[,3])
}
```


Mostrem en una gràfica els valors de la silueta mitja de cada prova per comprovar quin número de clusters és el millor
```{r message= FALSE, warning=FALSE}
plot(2:10,resultados[2:10],type="o",col="blue",pch=0,xlab="Numbero de clusters",ylab="Silueta")
```

Encara que el valor esperat es k=3, donat que el conjunt original té 3 classes, el millor valor que s'obten es k=2.

Un altre forma d'evaluar quin és el millor número de clústers és considerar el millor model, aquell que ofereix la menor suma dels quadrats de les distàncies dels punts de cada grup amb respecte al seu centre (withinss), amb la major separació entre centres de grups (betweenss). Com es pot comprovar és una idea conceptualment similar a la silueta. Una manera comú de fer la sel.leció del número de clústers consisteix en aplicar el mètode elbow (colze), que no és més que la sel.leció del número de clústers en base a la inspecció de la gràfica que s'obten al iterar amb el mateix conjunt de dades per a distints valors del número de clústers. Es sel.leccionarà el valor que es trova en el colze de la curva.

```{r message= FALSE, warning=FALSE}
resultados <- rep(0, 10)
for (i in c(2,3,4,5,6,7,8,9,10))
{
  fit           <- kmeans(x, i)
  resultados[i] <- fit$tot.withinss
}
plot(2:10,resultados[2:10],type="o",col="blue",pch=0,xlab="Número de clusters",ylab="tot.tot.withinss")
```

En aquest cas el número òptim de clústers es 4, que és quan la curva comença a estabilitzar-se.

També es pot fer servir la funció kmeansruns del paquet fpc que executarà l'algoritme kmeans com un conjunt de valors i selecciona el valor del número de clústers que millor funcioni d'acord amb dos criteris: la silueta mitja (asw) i Calinski-Harabasz ("ch").

```{r message= FALSE, warning=FALSE}
library(fpc)
fit_ch  <- kmeansruns(x, krange = 1:10, criterion = "ch") 
fit_asw <- kmeansruns(x, krange = 1:10, criterion = "asw") 
```

Podemos comprobar el valor con el que se ha obtenido el mejor resultado y también mostrar el resultado obtenido para todos los valores de k usando ambos criterios

```{r message= FALSE, warning=FALSE}
fit_ch$bestk
fit_asw$bestk

plot(1:10,fit_ch$crit,type="o",col="blue",pch=0,xlab="Número de clústers",ylab="Criterio Calinski-Harabasz")
plot(1:10,fit_asw$crit,type="o",col="blue",pch=0,xlab="Número de clústers",ylab="Criterio silueta media")

```

Els resultats son molt semblants als que hem obntingut anteriorment. Amb el cretir de la silueta mitja s'obté dos clústers i amb el Calinski-Harabasz s'obtenen 3.

Com s'ha comprovat, coneixer el número òptim de clústers no és un problema fàcil. Tampoc ho és la visualització dels models d'agregació.

Com en el cas que estudiem sabem que les dades poden ser agrupades en 3 classes, anem a veure com es comporta el kmeans en aquest cas. Per això comparem visualment els camps dos a dos, amb el valor real que sabem està enmgatzemat en el camp class deldataset original.
```{r message= FALSE, warning=FALSE}
iris3clusters <- kmeans(x, 3)

# sepalLength i sepalWidth
plot(x[c(1,2)], col=iris3clusters$cluster)
plot(x[c(1,2)], col=iris_data$class)
```

Podem observar que el sèpal no és un bon indicador per a diferenziar a les tres subespècies, no amb la metodologia kmeans, donat que dos de les subespecies estan massa mesclades per a poder diferenciar res.
```{r message= FALSE, warning=FALSE}
# petalLength i petalWidth
plot(x[c(3,4)], col=iris3clusters$cluster)
plot(x[c(3,4)], col=iris_data$class)
```
 Per un altre banda, la mida del pètal sembla fer una millor feina per a dividir els tres tipus de flors. El grup format pels punts negres, que ha trovat l'algoritme, coincideix amb els de la flor Iris Setosa. Els altres dos grups,però s'entremesclen més, i hi ha alguns punts que es classifiquen com a Versicolor quan en realitat són Virginica.
 
 Una molt bona tècnica que ajuda a entendre els grupos que s'han format, és precisament donar-ls-hi nom. Com ara:
 
 - Grupo 1: Només setoses
 - Grupo 2: Principalment versicolor
 - Grupo 3: Virgíniques o iris pètal gran
 
 Això ens ajuda a entendre com estan formats els grups i a referir-nos a ells en anàlisis posteriors.
 
Una última cosa que ens queda per fer, és sapiguer quines de les mostres inicials han estat mal clasificades i com. Això ho aconseguirem amb la sigüent comanda.

```{r message= FALSE, warning=FALSE}
table(iris3clusters$cluster,iris_data$class)
```

Ens permet així treure un porcentatge de la precisió del model
```{r message= FALSE, warning=FALSE}
100*(36 + 48 + 49)/(133+(2+14))
```

## Exercici 1.1
Prenent com a punt de partida els exemples, realitza un estudi similar amb un altra conjunt de dades. Poden ser dades reals del vostre àmbit laboral o d'algú repositori de dades d'Internet. Mireu per exemple: http://www.ics.uci.edu/~mlearn/MLSummary.html.

A l'hora de escollir la base de dades tingues en compte que ha de ser apropiada per a problemes no supervisats i que els atributs siguin també apropiats per al seu ús amb el kmeans.

No us oblideu de la fase de preparació i anàlisi de dades.

### Resposta 1.1:
> Escribiu aquí la resposta a la pregunta

## Exercici 1.2
Busqueu informació d'altres mètodes d'agregació diferents al kmeans. Partint del exercici anterior proveu el funcionament de com a mínim 2 mètoes diferents i compareu els resultats obtinguts.

### Resposta 1.2
> Escribiu aquí la resposta a la pregunta

******
# Exemple 2
## Mètodes d'associació
******
En aquest exemple treballarem l'algoirtme "apriori" per a obtenir regles d'associació a partir d'un data set. Aquestes regles ens ajudaran a comprendre com la informació del dataset es relaciona entre si.
Per aquest objectiu anem a treballar amb el dataset Groceries, que ja ve inclós amb la llibreria arules.
```{r message= FALSE, warning=FALSE}
# install.packages("arules")
library(arules)
data("Groceries")
```
Inspeccionem el dataset i veiem que té un llistat d'elements que van ser comprats plegats. Anem a analitzar-ho visualment.
```{r message= FALSE, warning=FALSE}
?Groceries
inspect(head(Groceries, 5))
```
En el següent plt podem veure que els tres elements més venguts son la llet senzera, altres verdures i briocheria. Donada la simplicitat del datset no es poden fer molts més anàlisis. Però per a dataset més complexes mirariam la distribució i freqüència de tots els camps, en busca de possibles patrons o errors.
```{r message= FALSE, warning=FALSE}
itemFrequencyPlot(Groceries,topN=20,type="absolute")
```
Si llancem l'algoritme "apriori", generarem directament un set de regles amb diferent suport, confiança i lift. El suport indica quantes vegades s'han trovat les regles {lsh => rhs} en el dataset, com més alt millor. La confiança ens parla de la probabilitat que {rhs} es trovi en funció de {lhs}. I el lift és un paràmetre que ens indica quan d'aletorietat hi ha a les regles. Un lift de 1 o menys ens indica que la regla és completament fruit de l'atzar.
```{r message= FALSE, warning=FALSE}
grocery_rules <- apriori(Groceries, parameter = list(support = 0.01, confidence = 0.5))

inspect(head(sort(grocery_rules, by = "confidence"), 3))
```
Podem provar a ordenar les regles pels diferents paràmetres, per veure quina informació podem obtenir.
```{r message= FALSE, warning=FALSE}
inspect(head(sort(grocery_rules, by = "support"), 3))
```
 Ordenant per suport veiem que, amb un lift de 1 i una confiança del 51% podem dir que la gent que en la compra hi tenien verdures i iogurt, compren també llet senzera. Hem de tenir en compte també que la llet senzera ´ss el producte més vengut a la botiga.
```{r message= FALSE, warning=FALSE}
inspect(head(sort(grocery_rules, by = "lift"), 3))
```
D'una altra banda, si ordenem per lift, veiem que amb un soport del 1% i una confiança del 58%, la gent que compra cítrics i tubercles compra també verdures.

Aquesta informació ens pot ajudar per donar consells a la direcció de la botiga de la disposició dels elements a la mateixa, o fins i tot de que productes s'han de posar en ofert conjunta. I si tinguessim més informació relativa al client podriem fer anàlisis més profons i veure quin tipus de client compra qué.

## Exercici 2.1:  
En aquest seguireu els passos del cicle de vida d'un projecte de mineria de dades per al cas d'un algoritme de generació de regles d'associació. Ho fareu amb el fitxer Lastfm.csv que trovareu adjunt. Aquest fitxer conté un conjunt de registres que formen l'històric de le cançons que ha sentit un usuari (user), el portal Web de música, "artists" és el nom del grup que ha sentit, sex i country corresponen a variables que descriuen a l'usuario.

### Resposta 2.1:
> Escribe aquí la respuesta a la pregunta

******
# Rúbrica
******

## Exercici 1.1

* 15%. S'expliquen els camps de la base de dades, preparació i anàlisi de dades
* 10%. S'aplica l'algoritme d'agrupament de forma correcata
* 25%. Es proven amb diferents valors de k
* 10%. S'obté una messura de lo bo que és l'agrupament
* 10%. Es posen nom a les agregacions
* 20%. Es descriuen i s'interpreten els diferents clústers obtinguts
* 10%. Es presenta el códi i és fàcilment reproduible

## Exercici 1.2

* 25%. Es prova un altre algoritme diferent al kmeans
* 25%. Es prova un altre algoritme diferent al kmeans
* 40%. Es comparen els resultats del kmeans amb els altres mètodes provats en aquest exercici
* 10%. Es presenta el códi i és fàcilment reproduible

## Exercici 2.1

* 10%. Es realitza un resumen de les dades incluides a la base de dades
* 15%. Es preparen les dades de forma correcta
* 10%. S'aplica l'algoritme de regles de associació
* 20%. Es realitzen diferentes proves variant els paràmetres
* 35%. S'expliquen les conclusions que s'obtenen
* 10%. Es presenta el códi i és fàcilment reproduible
