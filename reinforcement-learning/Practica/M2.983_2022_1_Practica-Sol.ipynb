{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LN0nZwyMGadB"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.983 · Aprenentatge per reforç</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Màster universitari en Ciència de dades (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios d'Informàtica, Multimèdia y Telecomunicació</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PRACTICA: Implementació d'un agent per a la robòtica espacial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Presentació\n",
    "\n",
    "Al llarg de les tres parts de l'assignatura hem entrat en contacte amb diferents classes d'algoritmes d'aprenentatge per reforç que permeten resoldre problemes de control en una gran varietat d'entorns.\n",
    "\n",
    "Aquesta pràctica, que s'estendrà al llarg d'un mes aproximadament, dóna la possibilitat d'enfrontar-se al disseny d'un agent per solucionar un cas específic de robotica.\n",
    "\n",
    "Atacarem el problema a partir de l'exploració de l'entorn i les observacions. Després passarem a la selecció de l'algorisme més oportú per solucionar l'entorn en qüestió amb les observacions seleccionades. Finalment, passarem per l'entrenament i la prova de l'agent fins a arribar a l'anàlisi del rendiment.\n",
    "\n",
    "Per fer-ho, es presentarà abans l'entorn de referència. Posteriorment, es passarà a la implementació d'un agent Deep Q-Network (DQN) que el solucioni. Després d'aquestes dues primeres fases de presa de contacte amb el problema, es cercarà un altre agent que pugui millorar el rendiment de l'agent DQN implementat anteriorment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Competències\n",
    "En aquesta activitat es treballen les següents competències:\n",
    "\n",
    "● Capacitat per analitzar un problema des del punt de vista de l'aprenentatge per reforç.\n",
    "\n",
    "● Capacitat per analitzar un problema en el nivell d'abstracció adequat a cada situació i aplicar les habilitats i coneixements adquirits per resoldre'ls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Objectius\n",
    "Els objectius concrets d'aquesta activitat són:\n",
    "\n",
    "● Conèixer i aprofundir en el desenvolupament d'un entorn real que es pugui resoldre mitjançant tècniques d'aprenentatge per reforç.\n",
    "\n",
    "● Aprendre a aplicar i comparar diferents mètodes d'aprenentatge per reforç per poder seleccionar el més adequat a un entorn i problemàtica concreta.\n",
    "\n",
    "● Saber implementar els diferents mètodes, basats en solucions tabulars i solucions aproximades, per a resoldre un problema concret.\n",
    "\n",
    "● Extreure conclusions a partir dels resultats obtinguts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entorn\n",
    "\n",
    "Estem treballant sobre el problema de guia autònoma i en particular volem solucionar el cas de l’aterratge propi, per exemple, dels drons autònoms.\n",
    "\n",
    "Per això, s'escull lunar-lander com a entorn simplificat. L'entorn es pot trobar al següent enllaç:\n",
    "https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "\n",
    "Lunar Lander consisteix en una nau espacial que ha d'aterrar a un lloc determinat del camp d'observació. L'agent condueix la nau i el seu objectiu és aconseguir aterrar a la pista d'aterratge, coordenades (0,0), i arribar amb velocitat 0.\n",
    "\n",
    "La nau consta de tres motors (esquerra, dreta i el principal que té a sota) que li permeten anar corregint el rumb fins a arribar a la destinació.\n",
    "\n",
    "Les accions que pot fer la nau (espai d'accions) són discretes.\n",
    "\n",
    "Les recompenses obtingudes al llarg del procés d'aterratge depenen de les accions que es prenen i del resultat que se'n deriva.\n",
    "\n",
    "    ● Desplaçar-vos de dalt a baix, fins a la zona d'aterratge pot resultar en [+100,+140] punts\n",
    "\n",
    "    ● Si s'estrella a terra, perd 100 punts (recompensa -100 punts)\n",
    "\n",
    "    ● Si aconsegueix aterrar a la zona d'aterratge (velocitat 0), guanya +100 punts\n",
    "\n",
    "    ● Si aterra, però no a la zona d'aterratge (fora de les banderes grogues) es perden punts\n",
    "\n",
    "    ● El contacte d'una pota amb el terra rep +10 punts (si es perd contacte després d'aterrar, es perden punts)\n",
    "\n",
    "    ● Cada cop que encén el motor principal perd 0.3 punts (recompensa -0.3 punts)\n",
    "\n",
    "    ● Cada cop que encén un dels motors d'esquerra o dreta, perd 0,03 punts (recompensa -0.3 punts)\n",
    "\n",
    "La solució òptima és aquella en què l'agent, amb un desplaçament eficient, aconsegueix aterrar a la zona d'aterratge (0,0), tocant amb les dues potes a terra i amb velocitat nul·la. Es considera que l'agent ha après a fer la tasca (i.e. el “joc” acaba) quan obté una mitjana d'almenys 200 punts durant 100 episodis consecutius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T10:23:07.600452Z",
     "iopub.status.busy": "2022-10-26T10:23:07.599977Z",
     "iopub.status.idle": "2022-10-26T10:23:58.641094Z",
     "shell.execute_reply": "2022-10-26T10:23:58.637388Z",
     "shell.execute_reply.started": "2022-10-26T10:23:07.600419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari]==0.25.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from gym[atari]==0.25.0) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from gym[atari]==0.25.0) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from gym[atari]==0.25.0) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from gym[atari]==0.25.0) (1.20.3)\n",
      "Requirement already satisfied: ale-py~=0.7.5 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from gym[atari]==0.25.0) (0.7.5)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from ale-py~=0.7.5->gym[atari]==0.25.0) (5.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[atari]==0.25.0) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "from time import time\n",
    "from collections import deque, defaultdict, namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "!pip install gym[atari]==0.25.0\n",
    "!pip install autorom[accept-rom-license]\n",
    "!pip install gym[box2d]\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "from copy import deepcopy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprovació de la versió de GYM instal·lada\n",
    "print('La versió de gym instal·lada: ' + gym.__version__)\n",
    "\n",
    "# Comprovació d'entorn amb gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"L'entorn utilitza: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tfo8jleHGadK"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 1.1 (0.5 punts)</strong> Es demana explorar l’entorn i representar una execució aleatòria.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "print(\"- Rang de recompenses o llindar de les recompenses: {} \".format(env.reward_range))\n",
    "print(\"- Màxim nombre de passos per episodi: {} \".format(env.spec.max_episode_steps)) \n",
    "print(\"- Espai d'accions: {} \".format(env.action_space.n))\n",
    "print(\"- Espai d'estats continuu: {} \".format(env.continuous))\n",
    "print(\"- Espai d'observacions: {} \".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hem pogut veure que el llindar de recompenses pot anar des de -infinit fins a +infinit. També que per cada episodi tenim un màxim de 1000 passos. Les accions són 4 que les comentarem en el següent exercici i aquestes són discretas (es pot configurar per a que siguin continues). L'espai d'observacions l'hem visualitzat i també el comentarem més endavant. Ara farem una execució aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicialitzem l'entorn\n",
    "obs = env.reset()\n",
    "t, total_reward, done = 0, 0, False\n",
    "\n",
    "# mostrem informació inicial\n",
    "print(\"Obs: {}\".format(obs))\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # escollim la acció aleatoria\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # executem la acció i obtenim el nou estat, la recompensa i si hem acabat    \n",
    "    new_obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # mostrem informació\n",
    "    print(\"\\nObs: {} \\nAcció: {} \\nRecompensa: {} \\nDone: {}\".format(new_obs, action, reward, done))\n",
    "\n",
    "    # Actualizar variables\n",
    "    obs = new_obs\n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "    \n",
    "print(\"Episodi finalitzat després de {} passos i recompensa de {} \".format(t, total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 1.2 (0.5 punts)</strong>\n",
    "Explicar els espais d'observacions i d'accions possibles (informe escrit).    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Espai d'accions: com hem vist en l'exercici anterior l'espai d'accions és de 4. Hem definit un espai discret (podria ser continu). Les accions són les següents: \n",
    "    - Nop (no fer res)\n",
    "    - Fire left engine (motor esquerra)\n",
    "    - Main engine (motor principal)\n",
    "    - Right engine (motor dret)\n",
    "    \n",
    "\n",
    "- Espai d'observacions: l'espai d'observacions està formada per un vector amb 8 coordenades.\n",
    "    - la coordenada de x\n",
    "    - la coordenada de y\n",
    "    - la velocitat linear de x\n",
    "    - la velocitat linear de y\n",
    "    - l'angle\n",
    "    - la velocitat angular\n",
    "    - dos boolean que diuen si cada una de les potes està en contacte amb el terra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anem a executar 1000 prenent accions de forma aleatòria. Emmagatzerem tant la suma de recompenses de cada partida com la quantitat d'episodis executats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games(num_games):    \n",
    "    steps_list = []\n",
    "    total_reward_list = []    \n",
    "    for i_game in range(num_games): \n",
    "        if i_game % 1 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_game, num_games), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        total_reward, steps, done = 0, 0, False\n",
    "        env.reset()\n",
    "        while not done:         \n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            if done:    \n",
    "                steps_list.append(steps)\n",
    "                total_reward_list.append(total_reward)\n",
    "    return steps_list, total_reward_list      \n",
    "          \n",
    "steps_list, total_reward_list = play_games(1000)\n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(steps_list)\n",
    "plt.title('Histograma de la mitjana de steps')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(total_reward_list)\n",
    "plt.title('Histograma de la mitjana de reward')\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La mitjana de les passes per episodi és de: {} \".format(np.mean(steps_list)))\n",
    "print(\"La mitjana de les recompenses per episodi és de: {} \".format(np.mean(total_reward_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "La mitjana de passes és del voltant de mes de 90 i la recomenpensa mitjana dels episodis és força negatiu. Fent servir algoritmes Deep Q-Networks hauriem d'aconseguir millorar aquests valors de recompensa ja que hem triat accions totalment aleatories. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent de referència\n",
    "\n",
    "A la tercera part de l'assignatura hem introduït l'agent DQN amb replay buffer i target network, que és un bon candidat per a la solució del problema de robòtica que estem analitzant, donat que permet controlar entorns amb un nombre elevat d'estats i accions de forma eficient.\n",
    "\n",
    "Es demana resoldre els 3 exercicis següents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.1 (1.5 punts):</strong> Implementar un agent DQN per a l'entorn lunar-lander.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per entrenar una xarxa neuronal per resoldre el problema de \"LunarLander-v2\" fem servir una arquitectura amb una capa d'entrada amb 8 canals d'entrada (per cada un dels atributs de les observacions), una capa intermitja amb 32 entrades i 64 sortides i finalment una capa de sortida amb les 64 entrades i 4 sortides (una per cada una de les possibles accions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    \"\"\" Deep Q-Network model  per a l'entrenament de l'agent DQN \"\"\"\n",
    " \n",
    "    def __init__(self, n_state, n_action, seed, n_layer1=64, n_layer2=64):\n",
    "        \"\"\"\n",
    "        Inicialització de la xarxa neuronal\n",
    "        Params\n",
    "        =======\n",
    "            n_state (int): Dimensions de l'espai d'estats\n",
    "            n_action (int): Dimensions de l'espai d'accions\n",
    "            n_layer1 (int): Nombre de nodes en la primera capa oculta\n",
    "            n_layer2 (int): Nombre de nodes en la segona capa oculta\n",
    "            seed (int): Random seed per a inicialitzar els valors aleatoris\n",
    "        \"\"\"\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.seed = T.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(n_state, n_layer1)\n",
    "        self.fc2 = nn.Linear(n_layer1, n_layer2)\n",
    "        self.fc3 = nn.Linear(n_layer2, n_action)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass de la xarxa neuronal amb una capa oculta de 64 nodes i una capa de sortida de 4 nodes (una per cada acció)\n",
    "        amb activació ReLU en les dues capes ocultes i activació lineal en la capa de sortida \n",
    "        \"\"\"\n",
    "        state = F.relu(self.fc1(state))\n",
    "        state = F.relu(self.fc2(state))\n",
    "        return self.fc3(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem servir tant l'algorisme d'optimització Adam com mètode e-greedy per a que anem alternant accions aleatories amb accions de l'aprenentatge. Al principi totes són aleatories i posteriorment de forma incremental anem afegint del model.\n",
    "\n",
    "Tal i com demana l'enunciat fem servir un buffer per evitar la repetició de accions seqüencials i evitar la correlació entre ellas. Alhora implementem una xarxa objectiu i una principal que evita que es vicii en excés l'aprenentatge, estabilitzar-lo i que no ens centrem només en uns certs espai d'accions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\" Agent que interactua amb l'entorn i apren a través de DQN\"\"\"    \n",
    "    def __init__(self, env, seed, learning_rate=1e-3, gamma=0.99, tau=1e-3, buffer_size=100000, batch_size=64, dnn_upd=4):\n",
    "        \"\"\" Inicialitza l'agent per a l'aprenentatge per DQN\n",
    "            L'agent inicialitza la xarxa neuronal local i target, el buffer de memòria i l'optimitzador    \n",
    "        Params\n",
    "        ======\n",
    "            env: Entorn de gym\n",
    "            n_state (int): Dimensions de l'espai d'estats\n",
    "            n_action (int): Dimensions de l'espai d'accions\n",
    "            seed (int): Random seed per a inicialitzar els valors aleatoris\n",
    "            learning_rate (float): Velocitat d'aprenentatge\n",
    "            gamma (float): Valor gamma de l'equació de Bellman\n",
    "            tau (float): Valor de tau per a soft update del target network\n",
    "            buffer_size (int): Màxima capacitat del buffer\n",
    "            batch_size (int): Conjunt a agafar del buffer per a la xarxa neuronal     \n",
    "            dnn_upd (int): Freqüència d'actualització de la xarxa neuronal       \n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.seed = seed         \n",
    "        self.n_state = env.observation_space.shape[0] \n",
    "        self.n_action = env.action_space.n\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size        \n",
    "        self.dnn_upd = dnn_upd\n",
    "        self.device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\") # Si hi ha GPU, utilitza-la\n",
    "\n",
    "         \n",
    "        random.seed(seed)   \n",
    "\n",
    "        if T.cuda.is_available():\n",
    "            print(f'Running on {T.cuda.get_device_name(0)}')            \n",
    "        else:\n",
    "            print('Running on CPU')\n",
    "               \n",
    "        # Inicialització de les xarxes locals i target i de l'optimitzador\n",
    "        self.__initialize_networks()\n",
    "\n",
    "    def __initialize_networks(self):\n",
    "        # Inicialització de les xarxes locals i target            \n",
    "        self.qnetwork_local = DQNetwork(self.n_state, self.n_action, self.seed).to(self.device)\n",
    "        self.qnetwork_target = DQNetwork(self.n_state, self.n_action, self.seed).to(self.device)\n",
    "        # Inicialització de l'optimitzador\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr = self.learning_rate)\n",
    "\n",
    "        # Inicialització del buffer de memòria\n",
    "        self.memory = ReplayBuffer(self.n_action, self.buffer_size, self.batch_size, self.seed)\n",
    "        \n",
    "        # Inicialització del comptador de pasos per a l'actualització de la xarxa neuronal\n",
    "        self.t_step = 0\n",
    "\n",
    "    def __take_step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Afegeix l'experiència a la memòria i actualitza la xarxa neuronal\n",
    "        \"\"\"\n",
    "        # emmagatzemar l'experiència en el buffer de memòria\n",
    "        self.memory.append(state, action, reward, next_state, done)\n",
    "\n",
    "        # Actualitzar la xarxa neuronal cada dnn_upd pasos\n",
    "        self.t_step = (self.t_step + 1) % self.dnn_upd\n",
    "        if self.t_step == 0:\n",
    "            # Si hi ha suficients experiències en el buffer, agafar un lot i actualitzar la xarxa neuronal\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample_batch()\n",
    "                self.__update(experiences, self.gamma)\n",
    "\n",
    "    def get_action(self, state, eps):\n",
    "        \"\"\"\n",
    "        Retorna l'acció segons l'estat actual i l'epsilon-greedy\n",
    "        \"\"\"\n",
    "        state = T.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with T.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy per a seleccionar l'acció. \n",
    "        # Si el valor aleatori és més gran que l'epsilon agafar l'acció amb el valor més alt segons la xarxa neuronal\n",
    "        # Si no, agafar una acció aleatòria\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.n_action))\n",
    "\n",
    "    def __update(self, experiences, gamma):\n",
    "        \"\"\"\n",
    "        Actualitza els pesos de la xarxa neuronal local i target\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # obtenir els valors Q de l'estat següent segons la xarxa neuronal target\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # calcular els valors Q segons l'equació de Bellman teni en compte si l'estat és terminal i el parametre gamma\n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "\n",
    "        # obtenir els valors Q de l'estat actual segons la xarxa neuronal local\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # calcular la funció de pèrdua segons l'error quadràtic mitjà\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "\n",
    "        # minimitzar la funció de pèrdua amb l'optimitzador\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "\n",
    "        # actualitzar els pesos de la xarxa neuronal target amb un soft update per a reduir el problema de l'estabilitat\n",
    "        self.__soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
    "\n",
    "    def __soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"\n",
    "        Soft update dels pesos de la xarxa neuronal target\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)    \n",
    "\n",
    "\n",
    "    def train(self, n_episodes=2000, max_t=1000, eps_start=1.0, eps_min=0.01, eps_decay=0.995, nblock =100, min_episodes=250, reward_threshold=200.0):\n",
    "        \"\"\"Deep Q-Learning.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            n_episodes (int): nombre màxim d'episodis\n",
    "            max_t (int): maxim nombre de pasos per episodi\n",
    "            eps_start (float): valor inicial d'epsilon\n",
    "            eps_min (float): valor mínim d'epsilon\n",
    "            eps_decay (float): factor de decaig d'epsilon\n",
    "        \"\"\"\n",
    "        self.reward_threshold = reward_threshold\n",
    "        self.eps = eps_start  # inicialitzar epsilon\n",
    "        self.nblock = nblock\n",
    "        self.total_episodes = 0\n",
    "        \n",
    "        self.update_loss = [] \n",
    "        self.mean_update_loss = [] # llista amb els valors de la funció de pèrdua per episodi\n",
    "        \n",
    "        self.sync_eps = [] \n",
    "\n",
    "        self.training_rewards = []  # llista amb els reward per episodi\n",
    "        self.mean_training_rewards = []  # llista amb la mitjana dels reward per episodi\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        print(\"Training...\")\n",
    "        \n",
    "        for episode in range(1, n_episodes + 1):\n",
    "            state = self.env.reset()\n",
    "            self.total_reward = 0   \n",
    "            self.total_time = 0\n",
    "            \n",
    "            for t in range(max_t):\n",
    "                action = self.get_action(state, self.eps)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.__take_step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                self.total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # actualitzar epsilon\n",
    "            self.eps = max(eps_min, eps_decay * self.eps)  # decrease epsilon            \n",
    "            \n",
    "            # afegir el reward de l'episodi a la llista\n",
    "            self.__save_statistics()\n",
    "            \n",
    "            # mostrar informació de l'episodi actual\n",
    "            self.__log_info(start_time, episode)\n",
    "            \n",
    "            ### comprovar si s'ha assolit el màxim d'episodis\n",
    "            training = not self.__is_solved_by_episode(episode, n_episodes) and not self.__is_solved_by_reward(episode, min_episodes, self.__get_mean_training_rewards())\n",
    "                        \n",
    "            ### si no s'ha assolit el màxim d'episodis, continuar entrenant\n",
    "            if not training:\n",
    "                print('\\nTraining finished.')\n",
    "                self.total_time = datetime.now() - start_time\n",
    "                self.total_episodes = episode\n",
    "                break\n",
    "\n",
    "            if episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tMean Rewards: {:.2f}\\t'.format(episode, self.__get_mean_training_rewards()))\n",
    "  \n",
    "\n",
    "    def __get_mean_training_rewards(self):\n",
    "        return np.mean(self.training_rewards[-self.nblock:])\n",
    "\n",
    "    ######## Emmagatzemar epsilon, training rewards i loss#######\n",
    "    def __save_statistics(self):\n",
    "        self.sync_eps.append(self.eps)              \n",
    "        self.training_rewards.append(self.total_reward)         \n",
    "        self.mean_training_rewards.append(np.mean(self.training_rewards[-self.nblock:]))\n",
    "        self.mean_update_loss.append(np.mean(self.update_loss))                                         \n",
    "        self.update_loss = []\n",
    "   \n",
    "    ######## Comprovar si s'ha arribat al llindar de recompensa i un mínim d'episodis\n",
    "    def __is_solved_by_reward(self, episode, min_episodios, mean_rewards):  \n",
    "        if mean_rewards >= self.reward_threshold and min_episodios <  episode:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode, mean_rewards))\n",
    "            T.save(self.qnetwork_local.state_dict(), 'data.pth')\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    ######## Comprovar si s'ha arribat al màxim d'episodis\n",
    "    def __is_solved_by_episode(self, episode, max_episodes):\n",
    "        if episode >= max_episodes:\n",
    "            print('\\nEpisode limit reached.')\n",
    "            return True\n",
    "        else:\n",
    "            return False        \n",
    "\n",
    "\n",
    "    ######## Mostrar informació de l'episodi actual\n",
    "    def __log_info(self, start_time, episode):\n",
    "        end_time = datetime.now()\n",
    "        # get difference time\n",
    "        delta = end_time - start_time \n",
    "        # time difference in minutes\n",
    "        total_minutes = delta.total_seconds() / 60           \n",
    "        print('\\rEpisode {}\\tMean Rewards: {:.2f}\\tEpsilon {}\\tTime {} minutes\\t'\n",
    "              .format(episode, self.__get_mean_training_rewards(), round(self.eps,4), round(total_minutes,2)), end=\"\")                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch as T\n",
    "\n",
    "class ReplayBuffer:\n",
    "            \n",
    "    \"\"\" Definim la classe ReplayBuffer que ens permetrà guardar les experiències de l'agent i poder-les reutilitzar posteriorment. \"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\" Inicialitzem els paràmetres de la classe ReplayBuffer.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension de l'espai d'accions\n",
    "            buffer_size (int): mida del buffer\n",
    "            batch_size (int): tamany de la mostra del batch\n",
    "            seed (int): seed per inicialitzar el generador de nombres aleatoris\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Afegeix una experiència al buffer de memòria. \"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        \"\"\" Retorna una mostra aleatòria de tamany batch_size experiències del buffer de memòria.  \"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = T.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = T.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = T.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = T.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = T.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Retorna el tamany actual del buffer de memòria. \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.2 (1 punt):</strong> Entreneu l'agent DQN i busqueu els valors dels hiperparàmetres que obtinguin un alt rendiment de l'agent. Per fer-ho, cal llistar els hiperparàmetres sota estudi i presentar les gràfiques de les mètriques que descriuen l'aprenentatge.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afegim els paràmetres base amb els que realitzarem l'entrenament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000    # Màxima capacitat del buffer\n",
    "BATCH_SIZE = 64         # Conjunt a agafar del buffer per a la xarxa neuronal\n",
    "GAMMA = 0.99            # Valor gamma de l'equació de Bellman\n",
    "TAU = 1e-3              # Valor de tau per a soft update del target network\n",
    "LEARNING_RATE = 5e-4    # Velocitat d'aprenentatge\n",
    "DNN_UPD = 3             # Freqüència d'actualització de la xarxa neuronal\n",
    "\n",
    "N_EPISODES=2000\n",
    "MAX_T=1000 \n",
    "EPS_START=1.0\n",
    "EPS_MIN=0.01\n",
    "EPS_DECAY=0.995\n",
    "NBLOCK =100\n",
    "MIN_EPISODES=250\n",
    "REWARD_THRESHOLD = 200  # Valor de recompensa per a considerar l'entrenament com a completat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem una interació per a cercar els millors paràmetres de l'agent en quant a recompensa mitjana, temps d'entrenament i steps.\n",
    "La primera iteració la fem amb el learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# paràmetres a buscar per a millorar l'aprenentatge de l'agent DQN (Deep Q-Network)\n",
    "LEARNING_RATE = [1e-3, 5e-4, 1e-4] # Velocitat d'aprenentatge\n",
    "\n",
    "# per emmagatzemar tots els agents creats\n",
    "agents_lr = []\n",
    "# número de test\n",
    "n_test = 1\n",
    "# iteració per a buscar els millors paràmetres\n",
    "for lr in LEARNING_RATE:\n",
    "    print(\"Test Number: {}\".format(n_test))\n",
    "    print(\"Learning rate: \", lr)\n",
    "    agent = Agent(env, seed=0, learning_rate=lr, gamma=GAMMA, tau=TAU, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, dnn_upd=DNN_UPD)\n",
    "    scores = agent.train(N_EPISODES, MAX_T, EPS_START, EPS_MIN, EPS_DECAY, NBLOCK, MIN_EPISODES, REWARD_THRESHOLD)\n",
    "    agents_lr.append(agent)\n",
    "    print(\"Mean reward: \", agent.mean_training_rewards[-1])\n",
    "    print(\"Mean loss: \", agent.mean_update_loss[-1])\n",
    "    print(\"Epsilon: \", agent.sync_eps[-1])\n",
    "    print(\"Steps: \", agent.total_episodes)\n",
    "    print(\"Total time: \", agent.total_time)\n",
    "    print(\"===========================================================================================\")\n",
    "    n_test += 1    \n",
    "                  \n",
    "# tanquem l'entorn de gym\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot de la recompensa de tots els agents en una mateixa gràfica\n",
    "for idx,agent in enumerate(agents_lr):    \n",
    "        plt.plot(agent.mean_training_rewards, label='Test Number' + str(idx + 1))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [18, 14] \n",
    "plt.legend(prop={'size': 12})   \n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.axhline(agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clarament hi ha un dels paràmetres de learning rate que funciona pitjor que els altres, concretament 0.0001. \n",
    "\n",
    "Anem a iterar ara amb DNN_UPD fixant el learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inicialització de l'entorn de gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "LEARNING_RATE = 0.0005     # Velocitat d'aprenentatge\n",
    "DNN_UPD = [1, 2, 3, 4, 5]              # Freqüència d'actualització de la xarxa neuronal\n",
    "\n",
    "agents_upd = []\n",
    "n_test = 1\n",
    "# iteració per a buscar els millors paràmetres\n",
    "for upd in DNN_UPD:\n",
    "    print(\"Test Number: {}\".format(n_test))\n",
    "    print(\"DNN update: \", upd)\n",
    "    agent = Agent(env, seed=0, learning_rate=LEARNING_RATE, gamma=GAMMA, tau=TAU, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, dnn_upd=upd)\n",
    "    scores = agent.train(N_EPISODES, MAX_T, EPS_START, EPS_MIN, EPS_DECAY, NBLOCK, MIN_EPISODES, REWARD_THRESHOLD)\n",
    "    agents_upd.append(agent)\n",
    "    print(\"Mean reward: \", agent.mean_training_rewards[-1])\n",
    "    print(\"Mean loss: \", agent.mean_update_loss[-1])\n",
    "    print(\"Epsilon: \", agent.sync_eps[-1])\n",
    "    print(\"Steps: \", agent.total_episodes)\n",
    "    print(\"Total time: \", agent.total_time)\n",
    "    print(\"===========================================================================================\")\n",
    "    n_test += 1    \n",
    "                  \n",
    "# tanquem l'entorn de gym\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot de la recompensa de tots els agents en una mateixa gràfica\n",
    "for idx,agent in enumerate(agents_upd):    \n",
    "        plt.plot(agent.mean_training_rewards, label='Test Number' + str(idx + 1))\n",
    "\n",
    "plt.legend()        \n",
    "plt.axhline(agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicialització de l'entorn de gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "LEARNING_RATE = 0.0005     # Velocitat d'aprenentatge\n",
    "DNN_UPD = 2              # Freqüència d'actualització de la xarxa neuronal\n",
    "EPS_DECAY = [0.98, 0.99, 0.995, 0.999]    # Valor de decaig de l'exploració\n",
    "\n",
    "agents_eps = []\n",
    "n_test = 1\n",
    "# iteració per a buscar els millors paràmetres\n",
    "for eps in EPS_DECAY:\n",
    "    print(\"Test Number: {}\".format(n_test))\n",
    "    print(\"Epsilon decay: \", eps)\n",
    "    agent = Agent(env, seed=0, learning_rate=LEARNING_RATE, gamma=GAMMA, tau=TAU, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, dnn_upd=DNN_UPD)\n",
    "    scores = agent.train(N_EPISODES, MAX_T, EPS_START, EPS_MIN, eps, NBLOCK, MIN_EPISODES, REWARD_THRESHOLD)\n",
    "    agents_eps.append(agent)\n",
    "    print(\"Mean reward: \", agent.mean_training_rewards[-1])\n",
    "    print(\"Mean loss: \", agent.mean_update_loss[-1])\n",
    "    print(\"Epsilon: \", agent.sync_eps[-1])\n",
    "    print(\"Steps: \", agent.total_episodes)\n",
    "    print(\"Total time: \", agent.total_time)\n",
    "    print(\"===========================================================================================\")\n",
    "    n_test += 1    \n",
    "                  \n",
    "# tanquem l'entorn de gym\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot de la recompensa de tots els agents en una mateixa gràfica\n",
    "for idx,agent in enumerate(agents_eps):    \n",
    "        plt.plot(agent.mean_training_rewards, label='Test Number' + str(idx + 1))\n",
    "\n",
    "plt.legend()        \n",
    "plt.axhline(agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = [1e-3, 5e-4]\n",
    "DNN_UPD = [1, 2 , 3]\n",
    "EPS_DECAY = [0.99, 0.995]\n",
    "\n",
    "# per emmagatzemar tots els agents creats\n",
    "agents_dqn = []\n",
    "# número de test\n",
    "n_test = 1\n",
    "# iteració per a buscar els millors paràmetres\n",
    "for lr in LEARNING_RATE:\n",
    "    for upd in DNN_UPD:\n",
    "        for eps in EPS_DECAY:\n",
    "            print(\"Test Number: {} \".format(n_test))\n",
    "            print(\"Learning rate: {}, DNN update: {}, Epsilon decay: {}\".format(lr, upd, eps))\n",
    "            agent = Agent(env, seed=0, learning_rate=lr, gamma=GAMMA, tau=TAU, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, dnn_upd=upd)\n",
    "            scores = agent.train(N_EPISODES, MAX_T, EPS_START, EPS_MIN, eps, NBLOCK, MIN_EPISODES, REWARD_THRESHOLD)\n",
    "            agents_dqn.append(agent)\n",
    "            print(\"Mean reward: \", agent.mean_training_rewards[-1])\n",
    "            print(\"Mean loss: \", agent.mean_update_loss[-1])\n",
    "            print(\"Epsilon: \", agent.sync_eps[-1])\n",
    "            print(\"Steps: \", agent.total_episodes)\n",
    "            print(\"Total time: \", agent.total_time)\n",
    "            print(\"===========================================================================================\")\n",
    "            n_test += 1    \n",
    "                  \n",
    "# tanquem l'entorn de gym\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot de la recompensa de tots els agents en una mateixa gràfica\n",
    "for idx,agent in enumerate(agents_dqn):    \n",
    "        plt.plot(agent.mean_training_rewards, label='Test Number' + str(idx + 1))\n",
    "\n",
    "plt.legend()        \n",
    "plt.axhline(agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.3 (0.5 punts):</strong> Visualitzar-ne el comportament (a través de\n",
    "gràfiques de les mètriques més oportunes).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(agent):\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(agent.training_rewards, label='Rewards')\n",
    "        plt.plot(agent.mean_training_rewards, label='Mean Rewards')\n",
    "        plt.axhline(agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "def plot_loss(agent):\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(agent.mean_update_loss, label='Loss')\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()\n",
    "        \n",
    "def plot_epsilon(agent):\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(agent.sync_eps, label='Epsilon')\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Epsilon')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000    # Màxima capacitat del buffer\n",
    "BATCH_SIZE = 64         # Conjunt a agafar del buffer per a la xarxa neuronal\n",
    "GAMMA = 0.99            # Valor gamma de l'equació de Bellman\n",
    "TAU = 1e-3              # Valor de tau per a soft update del target network\n",
    "LEARNING_RATE = 0.001   # Velocitat d'aprenentatge\n",
    "DNN_UPD = 2             # Freqüència d'actualització de la xarxa neuronal\n",
    "\n",
    "N_EPISODES=2000\n",
    "MAX_T=1000 \n",
    "EPS_START=1.0\n",
    "EPS_MIN=0.01\n",
    "EPS_DECAY=0.995\n",
    "NBLOCK =100\n",
    "MIN_EPISODES=250\n",
    "REWARD_THRESHOLD = 200  # Valor de recompensa per a considerar l'entrenament com a completat\n",
    "\n",
    "# inicialització de l'entorn de gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "# inicialització de l'agent amb els paràmetres de l'exercici\n",
    "agent = Agent(env, seed=0, learning_rate=LEARNING_RATE, gamma=GAMMA, tau=TAU, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, dnn_upd=DNN_UPD)\n",
    "# entrenament de l'agent\n",
    "scores = agent.train(N_EPISODES, MAX_T, EPS_START, EPS_MIN, EPS_DECAY, NBLOCK, MIN_EPISODES, REWARD_THRESHOLD)\n",
    "\n",
    "plot_rewards(agent)\n",
    "plot_loss(agent)\n",
    "plot_epsilon(agent)        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "def play_games(env, ag, num_games):    \n",
    "    total_reward_list = []    \n",
    "    for i_game in range(num_games): \n",
    "        if i_game % 1 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_game, num_games), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        env.reset()\n",
    "        state = env.reset()           \n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = ag.get_action(state,eps=0.0)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:    \n",
    "                total_reward_list.append(total_reward)\n",
    "    env.close()\n",
    "    return total_reward_list      \n",
    "          \n",
    "\n",
    "def plot_rewards_min(agent, total_reward_list):\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(total_reward_list, label='Rewards')\n",
    "        plt.axhline(agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "total_reward_list = play_games(env, agent, 50)\n",
    "plot_rewards_min(agent, total_reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Proposta de millora\n",
    "\n",
    "En aquesta part es demana proposar una solució alternativa al problema de robòtica espacial que pugui ser més eficient respecte a allò que s'ha implementat anteriorment. \n",
    "Per assolir aquest objectiu, cal implementar un nou agent, basat en els algoritmes que hem vist al llarg de l’assignatura.\n",
    "\n",
    "Es demana resoldre els 3 exercicis següents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.1 (2 punts):</strong> Implementar l'agent identificat a l'entorn lunar-lander.\n",
    "    \n",
    "Justifiqueu les raons que han portat a provar aquest tipus d'observació entre les disponibles i perquè s'ha triat aquest tipus d'agent. Detalleu quins tipus de problemes s'espera que es puguin solucionar respecte a la implementació anterior\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DuelingDQNetwork(nn.Module):\n",
    "    \"\"\" Deep Q-Network model  per a l'entrenament de l'agent DQN \"\"\"\n",
    " \n",
    "    def __init__(self, n_state, n_action, seed, n_layer1=64, n_layer2=64, n_layer3=64):\n",
    "        \"\"\"\n",
    "        Inicialització de la xarxa neuronal\n",
    "        Params\n",
    "        =======\n",
    "            n_state (int): Dimensions de l'espai d'estats\n",
    "            n_action (int): Dimensions de l'espai d'accions\n",
    "            n_layer1 (int): Nombre de nodes en la primera capa oculta\n",
    "            n_layer2 (int): Nombre de nodes en la segona capa oculta\n",
    "            seed (int): Random seed per a inicialitzar els valors aleatoris\n",
    "        \"\"\"\n",
    "        super(DuelingDQNetwork, self).__init__()\n",
    "        self.seed = T.manual_seed(seed)\n",
    "        self.fl1 = nn.Linear(n_state, n_layer1)\n",
    "        self.fl2 = nn.Linear(n_layer1, n_layer2)\n",
    "\n",
    "        self.advantage1 = nn.Linear(n_layer2, n_layer3)\n",
    "        self.advantage2 = nn.Linear(n_layer3, n_action)\n",
    "\n",
    "        self.value1 = nn.Linear(n_layer2, n_layer3)\n",
    "        self.value2 = nn.Linear(n_layer3, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass de la xarxa neuronal amb una capa oculta de 64 nodes i una capa de sortida de 4 nodes (una per cada acció)\n",
    "        amb activació ReLU en les dues capes ocultes i activació lineal en la capa de sortida \n",
    "        \"\"\"\n",
    "        state = F.relu(self.fl1(state))\n",
    "        state = F.relu(self.fl2(state))\n",
    "\n",
    "        advantage = F.relu(self.advantage1(state))\n",
    "        advantage = self.advantage2(advantage)\n",
    "\n",
    "        value = F.relu(self.value1(state))\n",
    "        value = self.value2(value)\n",
    "\n",
    "        return value + advantage - advantage.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingAgent(Agent):\n",
    "    \"\"\" Agent que interactua amb l'entorn i apren a través de DQN\"\"\"    \n",
    "    def __init__(self, env, seed, learning_rate=1e-3, gamma=0.99, tau=1e-3, buffer_size=100000, batch_size=64, dnn_upd=4):\n",
    "        \"\"\" Inicialitza l'agent per a l'aprenentatge per DQN\n",
    "            L'agent inicialitza la xarxa neuronal local i target, el buffer de memòria i l'optimitzador    \n",
    "        Params\n",
    "        ======\n",
    "            env: Entorn de gym\n",
    "            n_state (int): Dimensions de l'espai d'estats\n",
    "            n_action (int): Dimensions de l'espai d'accions\n",
    "            seed (int): Random seed per a inicialitzar els valors aleatoris\n",
    "            learning_rate (float): Velocitat d'aprenentatge\n",
    "            gamma (float): Valor gamma de l'equació de Bellman\n",
    "            tau (float): Valor de tau per a soft update del target network\n",
    "            buffer_size (int): Màxima capacitat del buffer\n",
    "            batch_size (int): Conjunt a agafar del buffer per a la xarxa neuronal     \n",
    "            dnn_upd (int): Freqüència d'actualització de la xarxa neuronal       \n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.seed = seed         \n",
    "        self.n_state = env.observation_space.shape[0] \n",
    "        self.n_action = env.action_space.n\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size        \n",
    "        self.dnn_upd = dnn_upd\n",
    "        self.device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\") # Si hi ha GPU, utilitza-la\n",
    "\n",
    "         \n",
    "        random.seed(seed)   \n",
    "\n",
    "        if T.cuda.is_available():\n",
    "            print(f'Running on {T.cuda.get_device_name(0)}')            \n",
    "        else:\n",
    "            print('Running on CPU')\n",
    "               \n",
    "        # Inicialització de les xarxes locals i target i de l'optimitzador\n",
    "        self.__initialize_networks()\n",
    "\n",
    "    def __initialize_networks(self):\n",
    "        # Inicialització de les xarxes locals i target            \n",
    "        self.qnetwork_local = DuelingDQNetwork(self.n_state, self.n_action, self.seed).to(self.device)\n",
    "        self.qnetwork_target = DuelingDQNetwork(self.n_state, self.n_action, self.seed).to(self.device)\n",
    "        # Inicialització de l'optimitzador\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr = self.learning_rate)\n",
    "\n",
    "        # Inicialització del buffer de memòria\n",
    "        self.memory = ReplayBuffer(self.n_action, self.buffer_size, self.batch_size, self.seed)\n",
    "        \n",
    "        # Inicialització del comptador de pasos per a l'actualització de la xarxa neuronal\n",
    "        self.t_step = 0\n",
    "\n",
    "    def __take_step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Afegeix l'experiència a la memòria i actualitza la xarxa neuronal\n",
    "        \"\"\"\n",
    "        # emmagatzemar l'experiència en el buffer de memòria\n",
    "        self.memory.append(state, action, reward, next_state, done)\n",
    "\n",
    "        # Actualitzar la xarxa neuronal cada dnn_upd pasos\n",
    "        self.t_step = (self.t_step + 1) % self.dnn_upd\n",
    "        if self.t_step == 0:\n",
    "            # Si hi ha suficients experiències en el buffer, agafar un lot i actualitzar la xarxa neuronal\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample_batch()\n",
    "                self.__update(experiences, self.gamma)\n",
    "\n",
    "    def get_action(self, state, eps):\n",
    "        \"\"\"\n",
    "        Retorna l'acció segons l'estat actual i l'epsilon-greedy\n",
    "        \"\"\"\n",
    "        state = T.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with T.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy per a seleccionar l'acció. \n",
    "        # Si el valor aleatori és més gran que l'epsilon agafar l'acció amb el valor més alt segons la xarxa neuronal\n",
    "        # Si no, agafar una acció aleatòria\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.n_action))\n",
    "\n",
    "    def __update(self, experiences, gamma):\n",
    "        \"\"\"\n",
    "        Actualitza els pesos de la xarxa neuronal local i target\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # obtenir els valors Q de l'estat següent segons la xarxa neuronal target\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # calcular els valors Q segons l'equació de Bellman teni en compte si l'estat és terminal i el parametre gamma\n",
    "        q_targets = rewards + (gamma * q_targets_next * (1 - dones))\n",
    "\n",
    "        # obtenir els valors Q de l'estat actual segons la xarxa neuronal local\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # calcular la funció de pèrdua segons l'error quadràtic mitjà\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "\n",
    "        # minimitzar la funció de pèrdua amb l'optimitzador\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "\n",
    "        # actualitzar els pesos de la xarxa neuronal target amb un soft update per a reduir el problema de l'estabilitat\n",
    "        self.__soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
    "\n",
    "    def __soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"\n",
    "        Soft update dels pesos de la xarxa neuronal target\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)    \n",
    "\n",
    "\n",
    "    def train(self, n_episodes=2000, max_t=1000, eps_start=1.0, eps_min=0.01, eps_decay=0.995, nblock =100, min_episodes=250, reward_threshold=200.0):\n",
    "        \"\"\"Deep Q-Learning.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            n_episodes (int): nombre màxim d'episodis\n",
    "            max_t (int): maxim nombre de pasos per episodi\n",
    "            eps_start (float): valor inicial d'epsilon\n",
    "            eps_min (float): valor mínim d'epsilon\n",
    "            eps_decay (float): factor de decaig d'epsilon\n",
    "        \"\"\"\n",
    "        self.reward_threshold = reward_threshold\n",
    "        self.eps = eps_start  # inicialitzar epsilon\n",
    "        self.nblock = nblock\n",
    "        self.total_episodes = 0\n",
    "        \n",
    "        self.update_loss = [] \n",
    "        self.mean_update_loss = [] # llista amb els valors de la funció de pèrdua per episodi\n",
    "        \n",
    "        self.sync_eps = [] \n",
    "\n",
    "        self.training_rewards = []  # llista amb els reward per episodi\n",
    "        self.mean_training_rewards = []  # llista amb la mitjana dels reward per episodi\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        print(\"Training...\")\n",
    "        \n",
    "        for episode in range(1, n_episodes + 1):\n",
    "            state = self.env.reset()\n",
    "            self.total_reward = 0   \n",
    "            self.total_time = 0\n",
    "            \n",
    "            for t in range(max_t):\n",
    "                action = self.get_action(state, self.eps)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.__take_step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                self.total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # actualitzar epsilon\n",
    "            self.eps = max(eps_min, eps_decay * self.eps)  # decrease epsilon            \n",
    "            \n",
    "            # afegir el reward de l'episodi a la llista\n",
    "            self.__save_statistics()\n",
    "            \n",
    "            # mostrar informació de l'episodi actual\n",
    "            self.__log_info(start_time, episode)\n",
    "            \n",
    "            ### comprovar si s'ha assolit el màxim d'episodis\n",
    "            training = not self.__is_solved_by_episode(episode, n_episodes) and not self.__is_solved_by_reward(episode, min_episodes, self.__get_mean_training_rewards())\n",
    "                        \n",
    "            ### si no s'ha assolit el màxim d'episodis, continuar entrenant\n",
    "            if not training:\n",
    "                print('\\nTraining finished.')\n",
    "                self.total_time = datetime.now() - start_time\n",
    "                self.total_episodes = episode\n",
    "                break\n",
    "\n",
    "            if episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tMean Rewards: {:.2f}\\t'.format(episode, self.__get_mean_training_rewards()))\n",
    "  \n",
    "\n",
    "    def __get_mean_training_rewards(self):\n",
    "        return np.mean(self.training_rewards[-self.nblock:])\n",
    "\n",
    "    ######## Emmagatzemar epsilon, training rewards i loss#######\n",
    "    def __save_statistics(self):\n",
    "        self.sync_eps.append(self.eps)              \n",
    "        self.training_rewards.append(self.total_reward)         \n",
    "        self.mean_training_rewards.append(np.mean(self.training_rewards[-self.nblock:]))\n",
    "        self.mean_update_loss.append(np.mean(self.update_loss))                                         \n",
    "        self.update_loss = []\n",
    "   \n",
    "    ######## Comprovar si s'ha arribat al llindar de recompensa i un mínim d'episodis\n",
    "    def __is_solved_by_reward(self, episode, min_episodios, mean_rewards):  \n",
    "        if mean_rewards >= self.reward_threshold and min_episodios <  episode:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode, mean_rewards))\n",
    "            T.save(self.qnetwork_local.state_dict(), 'data.pth')\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    ######## Comprovar si s'ha arribat al màxim d'episodis\n",
    "    def __is_solved_by_episode(self, episode, max_episodes):\n",
    "        if episode >= max_episodes:\n",
    "            print('\\nEpisode limit reached.')\n",
    "            return True\n",
    "        else:\n",
    "            return False        \n",
    "\n",
    "\n",
    "    ######## Mostrar informació de l'episodi actual\n",
    "    def __log_info(self, start_time, episode):\n",
    "        end_time = datetime.now()\n",
    "        # get difference time\n",
    "        delta = end_time - start_time \n",
    "        # time difference in minutes\n",
    "        total_minutes = delta.total_seconds() / 60           \n",
    "        print('\\rEpisode {}\\tMean Rewards: {:.2f}\\tEpsilon {}\\tTime {} minutes\\t'\n",
    "              .format(episode, self.__get_mean_training_rewards(), round(self.eps,4), round(total_minutes,2)), end=\"\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.2 (2 punts):</strong> Entrenar l'agent identificat i cercar els valors dels hiperparàmetres que obtinguin el\n",
    "rendiment 'òptim' de l'agent.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000    # Màxima capacitat del buffer\n",
    "BATCH_SIZE = 64         # Conjunt a agafar del buffer per a la xarxa neuronal\n",
    "GAMMA = 0.99            # Valor gamma de l'equació de Bellman\n",
    "TAU = 1e-3              # Valor de tau per a soft update del target network\n",
    "\n",
    "N_EPISODES=2000\n",
    "MAX_T=1000 \n",
    "EPS_START=1.0\n",
    "EPS_MIN=0.01\n",
    "EPS_DECAY=0.995\n",
    "NBLOCK =100\n",
    "MIN_EPISODES=250\n",
    "REWARD_THRESHOLD = 200  # Valor de recompensa per a considerar l'entrenament com a completat\n",
    "\n",
    "# inicialització de l'entorn de gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# inicialització de l'agent amb els paràmetres de l'exercici\n",
    "LEARNING_RATE = [1e-3, 5e-4]              # Velocitat d'aprenentatge\n",
    "DNN_UPD = [1, 2, 3]                       # Freqüència d'actualització de la xarxa neuronal\n",
    "EPS_DECAY = [0.99, 0.995]                 # Decaiment de l'exploració\n",
    "\n",
    "dueling_agents = []\n",
    "n_test = 1\n",
    "# iteració per a buscar els millors paràmetres\n",
    "for lr in LEARNING_RATE:\n",
    "    for upd in DNN_UPD:\n",
    "        for eps in EPS_DECAY:\n",
    "            print(\"Test Number: {} \".format(n_test))\n",
    "            print(\"Learning rate: {}, DNN update: {}, Epsilon decay: {}\".format(lr, upd, eps))\n",
    "            duelingAgent = DuelingAgent(env, seed=0, learning_rate=lr, gamma=GAMMA, tau=TAU, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, dnn_upd=upd)\n",
    "            duelingAgent.train(N_EPISODES, MAX_T, EPS_START, EPS_MIN, eps, NBLOCK, MIN_EPISODES, REWARD_THRESHOLD)\n",
    "            dueling_agents.append(duelingAgent)\n",
    "            print(\"Mean reward: \", duelingAgent.mean_training_rewards[-1])\n",
    "            print(\"Mean loss: \", duelingAgent.mean_update_loss[-1])\n",
    "            print(\"Epsilon: \", duelingAgent.sync_eps[-1])\n",
    "            print(\"Steps: \", duelingAgent.total_episodes)\n",
    "            print(\"Total time: \", duelingAgent.total_time)\n",
    "            print(\"===========================================================================================\")\n",
    "            n_test += 1    \n",
    "                  \n",
    "# tanquem l'entorn de gym\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot de la recompensa de tots els agents en una mateixa gràfica\n",
    "for idx,dueling_agent in enumerate(dueling_agents):    \n",
    "        plt.plot(dueling_agent.mean_training_rewards, label='Test Number' + str(idx + 1))\n",
    "\n",
    "plt.legend()        \n",
    "plt.axhline(dueling_agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.3 (2 punts):</strong> Analitzar el comportament de l'agent entrenat a l'entorn de prova i comparar-lo amb\n",
    "l'agent implementat en el punt 2 (a través de gràfiques de les mètriques més oportunes).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000    # Màxima capacitat del buffer\n",
    "BATCH_SIZE = 64         # Conjunt a agafar del buffer per a la xarxa neuronal\n",
    "GAMMA = 0.99            # Valor gamma de l'equació de Bellman\n",
    "TAU = 1e-3              # Valor de tau per a soft update del target network\n",
    "LEARNING_RATE = 0.001   # Velocitat d'aprenentatge\n",
    "DNN_UPD = 2             # Freqüència d'actualització de la xarxa neuronal\n",
    "\n",
    "N_EPISODES=2000\n",
    "MAX_T=1000 \n",
    "EPS_START=1.0\n",
    "EPS_MIN=0.01\n",
    "EPS_DECAY=0.99\n",
    "NBLOCK =100\n",
    "MIN_EPISODES=250\n",
    "REWARD_THRESHOLD = 200  # Valor de recompensa per a considerar l'entrenament com a completat\n",
    "\n",
    "\n",
    "# inicialització de l'entorn de gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "# inicialització de l'agent amb els paràmetres de l'exercici\n",
    "duelingAgent = DuelingAgent(env, seed=0, learning_rate=LEARNING_RATE, gamma=GAMMA, tau=TAU, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, dnn_upd=DNN_UPD)\n",
    "# entrenament de l'agent\n",
    "duelingAgent.train(N_EPISODES, MAX_T, EPS_START, EPS_MIN, EPS_DECAY, NBLOCK, MIN_EPISODES, REWARD_THRESHOLD)\n",
    "\n",
    "plot_rewards(agent)\n",
    "plot_loss(agent)\n",
    "plot_epsilon(agent)        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "def play_games(env, ag, num_games):    \n",
    "    total_reward_list = []    \n",
    "    for i_game in range(num_games): \n",
    "        if i_game % 1 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_game, num_games), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        env.reset()\n",
    "        state = env.reset()           \n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = ag.get_action(state,eps=0.0)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:    \n",
    "                total_reward_list.append(total_reward)\n",
    "    env.close()\n",
    "    return total_reward_list      \n",
    "          \n",
    "\n",
    "def plot_rewards_min(agent, total_reward_list):\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(total_reward_list, label='Rewards')\n",
    "        plt.axhline(agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "total_reward_list_dueling = play_games(env, duelingAgent, 50)\n",
    "plot_rewards_min(duelingAgent, total_reward_list_dueling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,agent in enumerate(agents_dqn):    \n",
    "        plt.plot(agent.mean_training_rewards,color='green', label='DQN Network')\n",
    "\n",
    "for idx,dueling_agent in enumerate(dueling_agents):    \n",
    "        plt.plot(dueling_agent.mean_training_rewards,color='blue', label='Dueling DQN Network')\n",
    "\n",
    "plt.axhline(agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "\n",
    "plt.gca().legend(['DQN Network','Dueling DQN Network'], loc=\"lower right\", labelcolor=['blue', 'green'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import wrappers\n",
    "\n",
    "import glob\n",
    "import base64\n",
    "!pip install -q gym box2d-py pyvirtualdisplay\n",
    "!pip install imageio-ffmpeg\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "\n",
    "def play_games_recording(env, ag, num_games, video_name, random):       \n",
    "    total_reward_list = []    \n",
    "    video = VideoRecorder(env, video_name)\n",
    "    for i_game in range(num_games): \n",
    "        if i_game % 1 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_game, num_games), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        env.reset()\n",
    "        state = env.reset()           \n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            video.capture_frame()\n",
    "            if (random):\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = ag.get_action(state,eps=0.0)            \n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:    \n",
    "                total_reward_list.append(total_reward)\n",
    "    env.close()\n",
    "    video.close()\n",
    "    return video, total_reward_list\n",
    "\n",
    "def render_mp4(videopath: str) -> str:\n",
    "  \"\"\"\n",
    "  Gets a string containing a b4-encoded version of the MP4 video\n",
    "  at the specified path.\n",
    "  \"\"\"\n",
    "  mp4 = open(videopath, 'rb').read()\n",
    "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
    "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
    "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "before_video_path = \"before_training.mp4\"\n",
    "video, total_reward_list = play_games_recording(env, agent, 10, before_video_path, True)\n",
    "html = render_mp4(before_video_path)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "after_video_path = \"after_training.mp4\"\n",
    "video, total_reward_list = play_games_recording(env, agent, 10, after_video_path, False)\n",
    "                                                \n",
    "html = render_mp4(after_video_path)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "space_invader.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
