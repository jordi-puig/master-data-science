{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LN0nZwyMGadB"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.983 · Aprenentatge per reforç</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Màster universitari en Ciència de dades (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios d'Informàtica, Multimèdia y Telecomunicació</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PEC2: Deep Reinforcement Learning\n",
    "\n",
    "\n",
    "En aquesta pràctica s'implementaran tres models de DRL en un mateix entorn, amb l'objectiu d'analitzar diferents formes d'aprenentatge d'un agent i estudiar-ne el rendiment. L'agent serà entrenat amb els mètodes:\n",
    "\n",
    "<ol>\n",
    "    <li>DQN</li>\n",
    "    <li>Dueling DQN</li>\n",
    "    <li>REINFORCE with baseline </li>\n",
    " </ol>\n",
    "\n",
    "**Important: El lliurament s'ha de fer en format notebook i en format html on es vegi el codi i els resultats i els comentaris de cada exercici. Per exportar el notebook a html es pot fer des del menú File → Download as → HTML.**\n",
    "\n",
    "**Cal adjuntar al lliurament els fitxers .pth amb els diferents models entrenats.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Contexte\n",
    "\n",
    "Un dels objectius més actuals del camp de la robòtica és aconseguir que un robot sigui capaç d'aprendre a fer una sèrie d'accions per si sols, de la mateixa manera que ho fa un nen petit. Aquesta és, bàsicament, una de les motivacions principals de l'aprenentatge per reforçament profund. Per això calen sistemes de control eficients en entorns d'alta dimensionalitat com pot ser la inversió en borsa, conducció de cotxes autònoms o, fins i tot, el control de coets espacials. Amb aquesta idea, en aquesta pràctica farem servir un entorn ja predefinit a OpenAI, **Space Invader**.\n",
    "\n",
    "**Space Invader** consisteix en un canó que pot disparar cap amunt i moure's d'esquerra a dreta. L'objectiu del joc és destruir els extraterrestres enemics, que s'acosten cada cop més ràpid al jugador a mesura que aquest els elimina, i maximitza la puntuació. En aquest entorn, lobservació és una imatge RGB de la pantalla representada per una matriu de forma (210, 160, 3) com s'observa a continuació."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](videos/random_agent_space_invader.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rang d'accions:\n",
    "* NOOP: 0 (No operació).\n",
    "* FIRE: 1 (disparar sense moure's)\n",
    "* RIGHT: 2 (moure a la dreta)\n",
    "* LEFT: 3 (moure a l'esquerra)\n",
    "* RIGHTFIRE: 4 (disparar i moure a la dreta)\n",
    "* LEFTFIRE: 5 (disparar i moure a l'esquerra)\n",
    "\n",
    "Recompensa: La recompensa tornada per l'entorn està composta per un valor al rang [0, 30]. Depenent de la nau alienígena destruïda, l'agent rep una puntuació diferent.\n",
    "\n",
    "La nostra tasca és ensenyar-li una política que permeti fer una elecció \"bona\" per a cada estat.\n",
    "\n",
    "Per a més detalls sobre la definició de l'entorn Space Invader, es recomana consultar les pàgines web:\n",
    "\n",
    "<href>https://www.gymlibrary.dev/environments/atari/space_invaders/</href> y  <href>https://atariage.com/manual_html_page.php?SoftwareLabelID=460</href>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inicialització i exploració de l'entorn (1 pt)\n",
    "**IMPORTANT**: l'entorn Space Invader depèn de la instal·lació dels paquets següents\n",
    "<ul>\n",
    "      <li>gym[atari] a la versió 0.25.0</li>\n",
    "      <li>autorom[accept-rom-license]</li>\n",
    "</ul>\n",
    "\n",
    "Aquest entorn pot ser executat tant a local com a Kaggle amb la versió de GPU P100 i a Google Colab. Es recomana utilitzar Kaggle per als estudiants que no disposin d'una GPU en local.\n",
    "\n",
    "Començarem carregant les principals llibreries necessàries per a la pràctica:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-26T10:23:07.600452Z",
     "iopub.status.busy": "2022-10-26T10:23:07.599977Z",
     "iopub.status.idle": "2022-10-26T10:23:58.641094Z",
     "shell.execute_reply": "2022-10-26T10:23:58.637388Z",
     "shell.execute_reply.started": "2022-10-26T10:23:07.600419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari]==0.25.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from gym[atari]==0.25.0) (5.1.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from gym[atari]==0.25.0) (2.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from gym[atari]==0.25.0) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from gym[atari]==0.25.0) (1.20.3)\n",
      "Requirement already satisfied: ale-py~=0.7.5 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from gym[atari]==0.25.0) (0.7.5)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from ale-py~=0.7.5->gym[atari]==0.25.0) (5.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[atari]==0.25.0) (3.6.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license] in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]) (8.0.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]) (4.62.3)\n",
      "Requirement already satisfied: requests in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]) (2.26.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from click->autorom[accept-rom-license]) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]) (2022.9.24)\n",
      "Requirement already satisfied: imageio in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from imageio) (1.20.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from imageio) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (0.18.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from scikit-image) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from scikit-image) (1.7.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from scikit-image) (3.4.3)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from scikit-image) (2.6.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from scikit-image) (8.4.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from scikit-image) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from scikit-image) (2021.7.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from scikit-image) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\jpuig\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#instal·lació de llibreries.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "!pip install gym[atari]==0.25.0\n",
    "!pip install autorom[accept-rom-license]\n",
    "!pip install imageio\n",
    "!pip install matplotlib\n",
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()\n",
    "from ale_py.roms import SpaceInvaders\n",
    "ale.loadROM(SpaceInvaders)\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "!pip install scikit-image\n",
    "from skimage import transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-10-26T10:23:58.642164Z",
     "iopub.status.idle": "2022-10-26T10:23:58.642528Z",
     "shell.execute_reply": "2022-10-26T10:23:58.642375Z",
     "shell.execute_reply.started": "2022-10-26T10:23:58.642358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La versió de gym instal·lada: 0.25.0\n",
      "L'entorn utilitza:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Comprovació de la versió de GYM instal·lada\n",
    "print('La versió de gym instal·lada: ' + gym.__version__)\n",
    "# Comprovació d'entorn amb gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"L'entorn utilitza: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Comproveu que la cel·la anterior indica que la versió instal·lada del Gym és la 0.25.0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tfo8jleHGadK"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>1.1 Exercici (0.2 pts):</strong> Inicialitzar l'entorn SpaceInvaders-v4. Extreure:\n",
    "<ul>\n",
    "  <li> Valor del llindar de recompensa definit a l'entorn</li>\n",
    "  <li> Màxim nombre de passos establerts per a cada episodi</li>\n",
    "  <li> La dimensió de l'espai d'accions</li>\n",
    "  <li> La dimensió de l'espai d'observacions.</li>\n",
    "  </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Rang de recompenses o llindar de les recompenses: (-inf, inf) \n",
      "- Màxim nombre de passos per episodi: 100000 \n",
      "- Espai d'accions: 6 \n",
      "- Espai d'observacions: Box(0, 255, (210, 160, 3), uint8) \n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "env = gym.make('SpaceInvaders-v4')\n",
    "\n",
    "####\n",
    "print(\"- Rang de recompenses o llindar de les recompenses: {} \".format(env.reward_range))\n",
    "print(\"- Màxim nombre de passos per episodi: {} \".format(env.spec.max_episode_steps)) \n",
    "print(\"- Espai d'accions: {} \".format(env.action_space.n))\n",
    "print(\"- Espai d'observacions: {} \".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com podem veure:\n",
    "- Encara que el rang a l'enunciat diu que va des de [0,30], l'environtment ens diu que el rang de recompenses o llindar va des de menys infinit fins a més infinit.\n",
    "- El màxim nombre de passos per episodi és de: 100000\n",
    "- La dimensió de l'espai d'accions és de 6.\n",
    "- La dimensió de l'espai d'observacions. Tenim un espai amb forma (210, 160, 3) i els valors que van des del 0 al 255. Per tant, tenim un ordre de magnitud de 256^(210x160x3) = 256^100800. És un conjunt d'estats elevadíssim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aprenentatge per reforç acostuma a ser molt útil visualitzar el comportament d'un agent al seu entorn. Per a aquesta PAC és interessant poder emmagatzemar el comportament visual d'un agent en forma de fitxer .gif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>1.2 Exercici (0.2 pts):</strong>  En aquest exercici us proposem emmagatzemar en una carpeta 'videos' un exemple d'actuació de l'agent aleatori en forma de fitxer .gif.\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "<b>Nota</b>: se us proporciona el codi pre-implementat. La implementació que es demana a l'enunciat està indicada als blocs <i>TODO</i> i/o amb variables igualades a <i>None</i>.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mètode per generar la imatge a partir dun estat amb un text informatiu\n",
    "def _label_with_text(frame):\n",
    "    '''\n",
    "    frame: estat de l'entorn GYM.\n",
    "    '''\n",
    "    im = Image.fromarray(frame)\n",
    "    im = im.resize((im.size[0]*2,im.size[1]*2))\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "    drawer.text((1, 1), \"Uoc Aprenentage Per Reforç.\", fill=(255, 255, 255, 128))\n",
    "    return im\n",
    "\n",
    "#Mètode que permet crear un gif amb l'evolució d'una partida donat un entorn GYM.\n",
    "def save_random_agent_gif(env):\n",
    "    frames = []\n",
    "    done = False\n",
    "    env.reset()\n",
    "    ###########################################   \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        frames.append(_label_with_text(frame))\n",
    "        state, _, done, _ = env.step(action)       \n",
    "    ##############################################\n",
    "\n",
    "    env.close()\n",
    "    imageio.mimwrite(os.path.join('./videos/', 'random_agent_space_invader_usuari.gif'), frames, fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v4', render_mode='rgb_array')\n",
    "try:\n",
    "    os.makedirs('videos')\n",
    "except:\n",
    "    pass\n",
    "save_random_agent_gif(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>1.3 Exercici (0.3 pts):</strong> Executar 1000 episodis amb el màxim de passos establert a l'entorn de Space Invader, prenent accions de forma aleatòria. Emmagatzemeu la suma de recompenses de cada partida i la quantitat d'episodis executats. Mostra:\n",
    "     <ul>\n",
    "        <li>Histograma amb la suma de recompenses de cada partida</li>\n",
    "        <li>Histograma amb la quantitat de passos per resoldre cada partida. </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 998/1000."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "env = gym.make('SpaceInvaders-v4')\n",
    "\n",
    "def play_games(num_games):    \n",
    "    steps_list = []\n",
    "    total_reward_list = []    \n",
    "    for i_game in range(num_games): \n",
    "        if i_game % 2 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_game, num_games), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        total_reward, steps, done = 0, 0, False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            env.render(mode='rgb_array')\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            if done:    \n",
    "                steps_list.append(steps)\n",
    "                total_reward_list.append(total_reward)\n",
    "    return steps_list, total_reward_list      \n",
    "          \n",
    "steps_list, total_reward_list = play_games(1000)\n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(steps_list)\n",
    "plt.title('Histograma de la mitjana de steps')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(total_reward_list)\n",
    "plt.title('Histograma de la mitjana de reward')\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>1.4 Anàlisi (0.3 pts):</strong> Quina és la mitjana de recompenses obtinguda? I la mitjana de passos per episodi? Comenta els resultats obtinguts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Solució:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mitjana de les passes per episodi és de: 686.56 \n",
      "La mitjana de les recompenses per episodi és de: 147.91 \n"
     ]
    }
   ],
   "source": [
    "print(\"La mitjana de les passes per episodi és de: {} \".format(np.mean(steps_list)))\n",
    "print(\"La mitjana de les recompenses per episodi és de: {} \".format(np.mean(total_reward_list)))\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agent DQN (2.8 pts)\n",
    "En aquest apartat implementarem una solució DQN per intentar obtenir un model que ens permeti solucionar aquest entorn. Primer definirem el model de xarxa neuronal, després descriurem el comportament de l'agent, l'entrenarem i, finalment, testejarem el funcionament de l'agent entrenat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Processament de les observacions.\n",
    "\n",
    "\n",
    "El primer pas és entendre l'estructura de la informació que ens proporciona l'entorn. Els jocs d'Atari utilitzen un espai d'observacions amb una estructura de la forma següent (210, 160, 3), és a dir, 210 píxels d'amplada, 160 d'alçada i 3 colors (RGB). Cadascun d'aquests punts de l'estructura és un píxel de color i té un rang de valors que van des del 0 fins a 255, cosa que ens dóna $256^{(210x160x3)}$ = $256^{100800}$ possibilitats (a mode de comparació, tenim aproximadament $10^{80}$ àtoms a l'univers observable)\n",
    "\n",
    "<p></p>\n",
    "<img src=\"imatges/atomos.jpg\"  width=\"1000\">\n",
    "\n",
    "Font: <href>https://huggingface.co/blog/deep-rl-dqn</href>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>2.1 Anàlisi (0.2 pts):</strong> És possible implementar una solució tabular vista durant la PAC1 en aquest entorn de Space Invader? Per què?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Resposta:</strong>\n",
    "<br><br>\n",
    "No perquè el nombre d'estats és molt gran i tenim una limitació tant computacional com d’emmagatzematge. Hi ha massa estats i que no es poden representar amb una taula i tampoc no podrem trobar una política o una funció de valor òptimes. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest cas, la millor idea és, atès un estat, aproximar els valors Q per a cada possible acció en aquest estat. És a dir, implementarem la solució que pots observar a la figura següent.\n",
    "\n",
    "<p></p>\n",
    "<img src=\"imatges/deep.jpg\"  width=\"1000\">\n",
    "\n",
    "Fuente: <href>https://huggingface.co/blog/deep-rl-dqn</href>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "Ara bé, al joc Space Invader no tot l'espai té utilitat. El marcador, el marc o la quantitat de vides no és necessari per al desenvolupament del joc. Una pràctica molt habitual en l'aprenentatge per reforç és reduir les dimensions de l'entorn amb l'objectiu de disminuir la quantitat de càlculs necessaris per obtenir un model útil per a l'entorn.\n",
    "\n",
    "Exactament, per a aquesta PAC us demanarem reduir l'espai dels frames a 84 x 84 píxels, reduir els nostres tres canals de color (RGB) a 1 (blanc i negre), normalitzar el resultat i, finalment, emmagatzemar els 4 últims frames com es mostra a la figura següent:\n",
    "<p></p>\n",
    "<img src=\"imatges/preprocessing.jpg\"  width=\"1000\">\n",
    "\n",
    "Font: <href>https://huggingface.co/blog/deep-rl-dqn</href>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>2.2 Exercici (0.1 pts):</strong> Defineix el mètode 'scale_lumininance' per, donat un estat, transformar els punts del sistema RGB al sistema binari blanc i negre.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_lumininance(obs):\n",
    "     \"\"\"\n",
    "     Separem cada una de les dimensions de RGB    \n",
    "     Posteriorment calculem el valor del gris en una única dimensió          \n",
    "     \"\"\"      \n",
    "     r, g, b = obs[:,:,0], obs[:,:,1], obs[:,:,2]\n",
    "     gray = 0.2989 * r + 0.5870 * g + 0.1140 * b \n",
    "     return gray.astype(np.uint8)\n",
    "\n",
    "    # altre implementació\n",
    "    # return rgb2gray(obs).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>2.3 Exercici (0.1 pts):</strong> Defineix el mètode 'resize' per, donat un estat, redimensionar la seva mida a 84*84\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(obs):\n",
    "    \"\"\" \n",
    "    Resize imatge a 84 * 84\n",
    "    Fem servir la funció resize del package transform (inclós a skimage)\n",
    "    \"\"\"   \n",
    "    return transform.resize(obs,(84,84))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>2.4 Exercici (0.1 pts):</strong> Defineix el mètode 'normalize' per, donat un estat, normalitzar la imatge\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(obs):\n",
    "    \"\"\" \n",
    "    Normalitzem la imatge\n",
    "    \"\"\"\n",
    "    obs /= (obs.max()/255.0)\n",
    "    return obs.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultat que obtindràs serà el següent:\n",
    "<p></p>\n",
    "<table  border=\"0\">\n",
    "<thead>\n",
    "  <tr style='border:none;' >\n",
    "    <th style='border:none;' ><img src=\"imatges/preprocess_grey.png\"  width=\"500\"></th>\n",
    "    <th style='border:none;'><img src=\"imatges/preprocess_normalize.png\"  width=\"500\"></th>\n",
    "    <th style='border:none;' ><img src=\"imatges/preprocess_size.png\"  width=\"500\"></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr style='border:none;'>\n",
    "    <td style='text-align: center; border:none;'> 1.scale_lumininance(obs) </td>\n",
    "    <td style='text-align: center; border:none;'> 2.resize(obs) </td>\n",
    "    <td style='text-align: center; border:none;'> 3.normalize(obs)</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode='rgb_array')\n",
    "action = env.action_space.sample()\n",
    "state, reward, done, _ = env.step(action)\n",
    "\n",
    "# Funció que realitza tot el pre-processament d'una observació\n",
    "def preprocess_observation(obs):\n",
    "    obs_proc = scale_lumininance(obs)\n",
    "    obs_proc = resize(obs_proc)\n",
    "    obs_proc = normalize(obs_proc)\n",
    "    return obs_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per últim, com interpretem el moviment? Les imatges corresponen a informació estàtica de les partides i, en aquest entorn, és important conèixer la direcció del tret o el moviment dels invasors. Per això, una forma de gestionar aquesta informació és apilant fotogrames per poder proporcionar a l'algorisme informació sobre la progressió de la partida.\n",
    "\n",
    "\n",
    "Utilitzem un exemple molt clar, el Ping Pong:\n",
    "<p></p>\n",
    "<img src=\"imatges/temporal-limitation-2.png\"  width=\"1000\">\n",
    "\n",
    "Fixa't com la pilota es desplaça cap a la dreta. L'agrupació dels frames ens permet traslladar la informació espacial al nostre algorisme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>2.5 Exercici (0.1 pts):</strong> Implementa la funció stack_frame per apilar 4 frames duna partida. Aquesta funció ha d'apilar inicialment (quan is_new = True) el mateix frame 4 vegades per, posteriorment, conforme es vagin introduint nous frames anar substituint els més antics.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frame(stacked_frames, frame, is_new):\n",
    "        \"\"\"Stacking Frames.\n",
    "        Params\n",
    "        ======\n",
    "            stacked_frames (array): array de frames (al retornar-lo ha de contenir 4 frames)\n",
    "            frame: Nova imatge per a afegir a l'array (s'ha d'esborrar la més antiga)\n",
    "            is_new: Primera vegada que s'utilitza l'array.\n",
    "        \"\"\"\n",
    "        if is_new:\n",
    "            stacked_frames = np.array([frame, frame, frame, frame])                                    \n",
    "        else:\n",
    "            stacked_frames[:-1] = stacked_frames[1:]\n",
    "            stacked_frames[-1] = frame                                \n",
    "        return stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un cop creada la funció la testegem jugant una partida, emmagatzemant els 4 frames més recents a cada pas, i finalment mostrem els 4 últims frames de la partida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcIAAAGaCAYAAACcxUPfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABQvElEQVR4nO3deXgc1Zkv/u+pqt671eqWWpslS7Jly8aOsfEmL4DNZrMFEuMBMiRAhkySee7cMDeZy52ZzJ2B5F5u7vObZ8g2YTJPCL9JhpBkAiRsAyEEEybEYMAxOOy2ZVv73t1q9VZ97h/VarUt9SJZS7f6+3meenCr63Sdri7e99SpqnOElBJERESlSlnoChARES0kJkIiIippTIRERFTSmAiJiKikMRESEVFJYyIkIqKSxkRIREQlreASoRDihBBiTAgRTFvqsqy/Swhxej7rOJ+EEP9FCHFICBERQjy40PUhoswYvyYIISxCiO8JIdqFEAEhxBtCiCsXul5TKbhEmHStlNKZtnQuVEWEEOpCbTupE8BXATywwPUgovwwfhk0AKcAXAzADeBvAfxECNG0gHWaUqEmwjMkWxb3CSE6k8t9yb85ADwNoC699ZVp/bTP++9CiK7ke3cIIaQQoiX53oNCiO8IIZ4SQowC2C2EuDrZmvELIU4JIf4+7bOakuVvT743JIT4nBBisxDiiBBiWAjxrbT1lwshnhdCDAgh+oUQ/yaEKM/03aWUj0gpHwMwMOs7lojmXKnGLynlqJTy76WUJ6SUCSnlEwCOA9g4Jzv6XEgpC2oBcALAZWf97R4AvwNQBcAH4LcAvpJ8bxeA09NYfy+AbgBrANgB/ACABNCSfP9BACMAdsBoKFiT2/hI8vU6AD0Ark+u35Qsf39y3SsAhAE8ltz+EgC9AC5Ort8C4HIAlmTdXgRwX1rd/wnAP02xX74K4MGF/n24cOGSeWH8mjp+Jd+rTn72qoX+nSbVbaErkOFACgIYTi6PAfgQwFVp6+wBcCLLgZRt/QcA3Jv2XssUB9K/5qjjfQD+8awDaUna+wMAbkx7/TMAd2b4rOsBvJHHfmEi5MKlwBfGr4zbNAF4DsA/L/RvNNWioTBdL6V8bvyFEGIMQHva++0AMl6ATr6Xaf06AIfS3js1Rfkz/iaE2Arg/wBYC8AMozX007PK9KT9e2yK187kZ1UB+AaACwG4YLTShrJ8FyIqLoxfZ25fgXHmGgXwX7Ktu1CK4hohjBtGGtNeL03+DTBaM9NZvwtAfdp7DVOUP/szHwLwCwANUko3jG4EkVfNJ7s3+fnrpJRlAG45h88iosJXsvFLCCEAfA9Gt+g+KWVshtudU8WSCH8E4MtCCJ8QohLA/wTww+R7PQAqhBDuPNf/CYDbhRCrhRD25Hu5uAAMSinDQogtAD5xDt/FhWTXiRBiCYC/zLayEEITQlgBqABUIYRVCFGoZ/JENFnJxi8A3wGwGsadtGPnsN05VSyJ8KswugOOAHgTwOvJv0FK+Q6MA+dY8g6nuhzrPw3j1P7XAD4A8HJyG5Es2/8zAPcIIQIwDryfnMN3uRvABTAuaD8J4JH0N4UQ9wsh7k/705dhdE38Dxitr7Hk34ioOJRk/BJCNAL4LID1ALrT7oz943PY/pwQyQuZJUsIsRrAWwAsUsr4QteHiChfjF+zo1jOCGeVEOJjQgizEMID4GsAHudBRETFgPFr9pVkIoRxut4H4zZlHcDnF7Y6RER5Y/yaZSXfNUpERKWtVM8IiYiIACD7A/VCCJ4uUoqUks87UlFhDKN0mWIYzwiJiKikMRESEVFJYyIkIqKSxkRIREQljYmQiIhKGhMhERGVNCZCIiIqaXlP59NwbQPs9fac6x3/0XFEh6PnVKnpUq0qWm5vybledDCK4z8+fk7buvWjTVhW78y53jcfeg/987wf7FYVd316dc71egfD+PbDH8xDjYgKB2OYgTFssqxDrKU/jKo5NSgm4wQyEUsgEUkkVwI0x0Q+jY3EIBNGMd92H6raqnJWouM/OjD8h2EAgKPRgaZ9TTnLDL05hM5fdqbqYC43p96Lh+JAsnqKWYFiNuotdYmY/9zmhXS7TDBrxudF4wmEw7pRBQG4HKbUeoMjUejJ/bBnew0u316T87MffvokDh0dBACsbHLhT29YnrPMwSMD+Omzp1J1qCy3pN4LhuJIJOtgMSuwmFUAQFyXGPJP/wDnA/VUbBjDJmMMmyzvRLhkzxLY6mwAAP97fvT+Zy8AQLEoWPaJZaky7Y+0IzZi/FBmjxkWj/Gl9KiO0OlQaj1nkxNCMeo01juGeNAYPF21qbDXGq02CYngsWDOL5etDt4NXnjXewEA0aEoTj52MufnZXPT3qVorHMAAH7/3jD+46UuAIDNouK//vHK1Hr/8rMPMThi/FA+jwU+r7EfItEEPjw18Z1am1xQVWM/dPSOYSRg1Nth09BYl9wPEnj7mD9VprHWDofd+B93yB9FV184Zx12XlCJHet9AIC+oQgeePTYtL87EyEVG8awyRjDJsu7azTUGTJaKQDCveGJD9YlAh8EUq8T0UTq3xavBa7lLgBAPBg/8yBqdqZaZ/FwPHUQaXYNrhZX8sOB4PEgkGOQpGx1iAxGUu/FR899ppLjHaMIJPdDR8/E94nrEm99MDKx3bQ6VFVYsWZ5GQBgJBg74yBavawM5uR+CI3pqYPIadewtsWYtFpK4J3jfoy3WZqWOFFdYRyUx06Ppg6ibHXoHYik3guMnluLkqgYMYYZGMMmyzsRxvwT3QWx9EpI44dKvdQnfnHNpsGSbEUI9cxEbPFYUqf7qkVN/V0xKakyANB8Y/OU9Ql8GED/q/0566CH9NR7erIL4FwMB6KpU/VA2kEppUTvwMT/XLo+8QM6bRqqKqwAAE098/4kn8cKi8X4mzVtP5hNSqrM+PbGecpMqff6h9K+d5Y6BMfiqffGIue+H4iKDWOYgTFssrwTYcXGCjgajNPpobeGMNY5BsD40esuq0utF+oMIRo1TmX9x/wIdRstjvQfFgC6D3QDyeMqFpg4KMMDYXQ+2zl1JdL2f/oBka0OruUu+Lb6Up8d+HCi1TUTF270oaXBaO29+tYATnSOAjB+9H2XN6TWO9E5inDU+IGPfjiCk93GevH4mfvhFy90QEl+r+G0vv/egTB+9kuj31xKIL0H+8XX+mAxT7TAxmWrw5rlZbh0q9HH3zMQPqPVRVQKGMMMjGGT5X2N0OQ2pboBII3MnVpPmWgpRYeiqQPG1+Y74wc89sOJPt3Wz7WmWlEdz3Zg5O3cX6rx441nHMhdv+pKVgCwVEy0wJAw+uaT3yF1sMq4POe7wbxuMyzjF9zP2g9q2n7oHYpAT+6Hy7fV4NK2agBAT38Y//iDd1Pr/d3n18JmNfbDT585idf+MAQAWNHowp983LhmkEhI/M03jqQOpDv2LUPL0uSB/OYAfvbc6eR3BWqSrSwAqQvd4/thvHoxXZ7RCssXrxFSsWEMm4wxbLK8E2HN7hrYqm05N3T6ydOp1pHVZ4W1ypqjBBDqCKV+XM2hwdmU+9beyFBkokVnUdD4scacZaIjUXQ83ZFzvWyuv2QJ6mty34L9b0+cwFCydVRXZcMSX+59d6wjiIHkfihzmtDa6MpZpm8okmrRWS0K7tiX+y6tweEoHnqqPed6Z2MipGLDGDYZY9hkeXeNdv+6e9obVe3qma2cDML9E33CilnJq0wimsAYjIMoEUng+MPn9mxNvh57fvoHodOmoboi9/9MXf1jqX+bTUpeZSKxiT70cCSBbz30/rTrR1QKGMMMjGGT5X1GSMQzQio2jGGUjhPzEhERTYGJkIiIShoTIRERlTQmQiIiKmlMhEREVNKYCImIqKQxERIRUUkr6ES4aZNj2mVWrbLCZpve16qtNaG6Ou+xBQAADoeCFStyPyyaTgjgggtyj+hwtpnsByJaeIxhhkKPYQWdCAMBHVdfXX7G3669tnzKdceNjiawd687NQgsAFx8sQt2e+avGolIbNjgQHn5xMjp551nQ329OWMZXZeorTWhpWViBImqKg0bNmQ/SBRFYNu2ieGXTCaByy93Zy0zk/1ARAuPMcxQ6DGsoBPhu++GcfToGK6+uhxCAPv2eXHkSAi33FKRscypU1G88IIft97qgxDA9u1ODA3p+MQnMpcZHIzjpZcCuO46D8xmgaYmC2prTWhrc8LtVqcsEw5LHDwYxMqVNtTWmmC3K7j0UjecThWtrVO3sqQE3nhjFLGYxIYNdggBfOpTlTh1KoJLLimb1f1ARAuPMWzm+2E+FXQiBAC/P47BwTjuuacex46F0dUVM0ZjzyKRAF54wY977qmHx6Ph7bfHkGUkOQDGD/zkk8P4q7+qw2WXleHll4OIxXKPznTwYBD79nlx5501eOqpYYRCCeSoHo4di2DZMivuuaceTz89jJERPWeZmewHIlp4jGGGQo5hBT3WqKIAf/M3S9DfH4PZrMDhUPD008N4441Q1nJ33lmDkREdNTUmjI0lcPRoCL/+dWDSPFrp9u3zorxcRUODGZ2dMQSDOh5/fAiBQCJjmU2bHGhrc8Lj0XDqVARut4aHHx5AT0/m2ZOrqzXceqsPui7h9+uoqTHjhz/sx/HjmacUmel+mG0ca5SKDWMYY1i6oh1rdHxnB4M6YjGJ667z5FVucDCOSCSBUCiB3bvdcDhyf9WhoTh0HRgZ0bF2rR0NDblHkB+v1+CgjupqEzZuzH1ROBKRGBuTGB7WoWkCe/Zk718HZr4fiGhhMYYZCjmGTe82owVgtytYs2bi4m00mrl1M04IYOtW42JuZWX+29q0yShz/vn53xW1cqUxR1dbW+75x8ZVV5sAIOuF7LPNZD8Q0cJjDDMUcgwr6ESoaQKDg3F897u9qdef+UxV1jKqavSVf+1rndB1CVUV2L/fm7WMohjL977Xi56eGBRFYMeO7AeFEICqiuTp/SiEAJYts2LJkuwHhqYJvPVWCL/4hTGLs9Op4vrrs7eMZrIfiGjhMYZNlCnkGFbQXaM33liB3t44gsEEgsEELr64DH6/nrXMpZe6EYtJBAI6gsEEWlutsFoV6HrmvvU1a2yorjZheNgo43arWL7cinA4c4ulokLD5s0ODA4a9YvFJHbudGFkJJ6xjKoC11/vRV9fLPWdrrmmHP39mcvMdD8Q0cJjDJv5fphPBX2zDBUW3ixDxYYxjNIV7c0yREREc4mJkIiIShoTIRERlTQmQiIiKmlMhEREVNIK+jlCAHA6FVxzzcQzKo8+OohIJPeNYPv3e6Gqxg1CBw740dWVecigcXv2uOHxGLvk6NEQ3nxzLGeZTZscaGkxBqjt7Y3h+ef9Ocs0NpqxbZsLgPFQ6SOPDOUsM9P9QEQLizHMUMgxrKAToaIAn/1sFTweDYoikEhIVFVV4Rvf6Mla7rbbKtHSYoWiCEgp0dhoxre/3YNgMPMzNXv2uLFtmxMmk3HgLV9uwdhYAh98kHn8vLVrbdi7txw2m1EmGrVC1yUOHAhkLFNRoeHmmyvhcikQQkDXJcxmBQ8/PDDr+4GIFhZj2Lnth/lS0F2jxvQhAk89NQwAeOSRQVitCsrKpp5WBDAmm/R6NTz4YB8A4OWXgxgcjMPrzZzzLRaBigoNP/7xAOJxoL09ijfeCMHr1c6YEyydqhoHxG9/G0BPTwxjYxI///kQvF4NZnPmx+28Xg0dHdHUYLPf/34fKiu1rHONzWQ/ENHCYwyb+X6YTwX9QP0Xv1iDoSEdPp+G06ejaGy0oKsrBodDwbe+NXVL4sYbvXA4jBHYP/ggjKoqE0KhBKqqTPjKVzqmLNPW5sTGjQ5UVGg4fjwCu12B3a7AYlHwz//cg6GhySMg1NaacMstlZASGB6OIxaTqK83IxDQ8cILARw5MnlUdVUF/vqvl2BgIA5NAwYG4lixwoqOjii6u2N4/PHhWdsPc4EP1FOxYQxjDEuXKYYVdNdoIgH8+McD2LrVCVUV6OyM4ZVXgrjtNl/Wco8/PoTzzrPBbFbQ1RXD4cMh3H579pFrX345AEUR8Hg0SCnx4YcRbN+efay+Dz4wJptcutQMIQR+//tQzhHiR0bi+NnPBrB2rR2KItDe7kd7ewTr12ceJHem+4GIFhZjmKHQY1hBd40+9tgQ9uxxo7c3ht27y3DyZARXXVWeGux1KgcOBLB7dxnefDOE3buNGZM3brTj6adHMpZ5++0x1NSYcexYGDt3OtHQYIHLpeLDDyMZ5/IaHIyjry8OIYCVK63YssWB7u4YHA4Vx46FpyyTSBj1a221wWpVUvXcssWJ3/0uOKv7gYgWHmPYzPfDfCroRNjeHsHSpWfOp9XYaMGpU9GMZbq7Y6itNZ8x83FdnRkdHZnLjIzosNmMboRxXq8Kv1/POBFmJCIRDieSfd8Gm02BpomMF7SlBLq7o6kpTADjIrLPp6GvL/OgtTPZD0S08BjDDIUewwq6a3RcOJxAZ2d0Wrfa6rpEZ2cUgcD0Rjjv6ophYCD7SOpn6++PIxqd3qUIv19HZ2cUiWlMyTWT/UBEC48xzFCwMUxKmXEBIBdyEQLyiivcZ/xt7153znKXX+6WijLxescOp7TZlKxlNm1ySLdbTb1ubbXKujpT1jJNTRbZ3GxJva6s1OS6dbasZcrKVLl5syP12mQScteusjnZD7O9ZDtWuHApxIUxjDEsnxhW0HeNUmGRvGuUigxjGKXLFMMK+hohERHRXGMiJCKiksZESEREJY2JkIiIShoTIRERlbRFnQjFDO5xFGL65WZSZrzcfJQhouLEGDY/FmUidLkUlJeruPvueuza5YLDoeTc+Xa7AlUFvvzlJfjkJythsYjUdCaZmM0CZrPAn/5pFe66qw6ahtR0JpkoirGtq64qx91318NmU+B05v4ZXC4Fa9bYcPfd9Vi2zFIwo7YT0exjDJtfRTGyzHQsWWJCW5sLJ09GoGkC1dUmXHJJGdrbIzhyZOpJKj0eFW1tTvT2xmEyCZSVqdi+3YVwOIFDh0YRi01+FMlqFdi82QkhjGlTzGaBCy5wwO3WcPBgEH7/5NEghADWr7ejpsaMykoNmiawerUNjY1mvPxyEN3dU0+82dJixfr1dgSDOjRNoLHRgo0bHTh0aBTHj2eea4yIig9j2PxbdGeEO3e68OKL/tTcXQ6Hiv/8zyAuv9ydsUxrqw09PTEoivFDm80CJ05EsGyZJWNLx+PRUFmpobs7BpNJQFGAaFQiFNLR1GSZsoyiGPV77bXR1NxdbreKQ4dGsXmzI2P9rr22HE8/PQy32/hOXq+Kp54axlVXleezS4ioiDCGzb9FlwgBQEqkRkI/cSKCwcH8xt179dVRJBISQ0N63q2U994LIxDQEYtJHD48ef6uqfT0xFKDzf7ud5lngk4XCiVw9Kjx+YcPh6Y9LiARFQ/GsPm16BKhrhstoptvrsC993ZACGDZMgviWY6jREIikQBuv92H++7rxiuvBHHRRa6sZaQ0trVnjxvPPDOCf/mXXnziExVIJIzPyyQel1izxobh4TjuvbcDt93mg5TIOnBtPC7h9WpYtcqGe+/twLZtTjgcCnS9cA4kIpodjGHzb9Elwn//90EARusjEEggGEygrEzFt77VnbHMK68Y/dTxuEQgoCMQ0GE2C/z854NTzuwMGFOl/PKXI9A0gUBAh9+vI5EA3norhLfemrofX9eB7363Dw6HgmDQqF84LBEKJfDkk8MZ6/fNb/bA41ExOqqf8Z3uv783/x1DREWBMWz+LbpECAAbNjjw+uujAICTJyOoqzPDbM7+VVessOL998PQdWBoKI5EAqk++kyqq00IBnUEgwnEYhInT0bQ3GzNWsZuV+DxaOjsNLoV3nhjFOefn3lmZ8BoHa5ebcM77xiTZb71VihnGSIqXoxh86yQpzA5l+X88+0SgKyvN8vycjWvMh/5iE0qCqTbrcr6enNeZZqbLdLhUKSmQa5enX36kvGlokKTNTXG9Cjr19vzKmO1CrlihVUCkKtWWaWmiXnfpws9pQ4XLtNdFjoOMYYVRwzjNEyUN8lpmKjIMIZRukwxbFF2jRIREeWLiZCIiEoaEyEREZU0JkIiIippi26s0XHpA9RmuR/onMukl5tJmemUm2n9iKj4MIbNn0WZCMvKVPzFX9RAVY29/uijgzh8OJR1x9vtCv7kT3zw+UwAgIMHg3j22ZEpB6sdZzYLfPSjHqxbZzwPc+pUFD/4QR/C4cxlFAXYutWJK68sBwBEIgl8/evdCAazDMsAoKnJgk9/2pd6/Z3v9KCra+oBbomouDGGza9F1zVaV2fC5z9fhVOnohgbS2BoKI79+yuwYUPmhzc9HhU331yBsbEEEgmJzs4oduxwYffusozTmFgsAnv2uFFba0IsJnHiRATNzRbccENFxulFhAC2bHHiootc8Pt1nD4dhdWq4DOfqUJ1tSlj/VassOJTn6pEd3cMg4NxRCIJfO5z1WhunnpgXCIqXoxh82/RJUKPR4OqCvT1xRCJJBAMGsMLjbeSpmK3K7DZFPT3x6HrSA1w6/GoUDNMmWUyCbjdGkZGjMFqe3uNlo3DocBqzfy4nc+nYWwsgdFRHQMDxugPxmdlnpvL59Og6xJDQ3H4/ToiEQlFASorF+UJPVFJYwybf4vygfrmZuMUfLxb4Ze/HMFvfuPPOgBtdbUJn/xkZWpIoiNHQnjiiaGsp/tlZSo+9jEPWlttAIDu7ih+9KMB9PVl3pDZLHDJJWW46KIyAEA0msD3vteXGsk9k40bHdi3z5t6/dBD/RnHA5wrfKCeig1jGGNYukwxrDDS8Sw7fTqKgYE4HnigD9u3O9HVFc16AAFAX18MoZCOBx7oxZIlZtTUmDE6mr3POxDQMTSk48EH+zAyouPqq8sxMJB9Q9GoRE9PDM8+O4zXXw/hjjt8qTH7sjl1KoKjR0N4/PFh7N/vzXnQEVHxYgybZ4txnL4vfKFGms1CVlebpNOpyFtvrZRlZdnH6tu3zytra02yqkqTHo8qd+1yyfPOyz7u3qZNDrlli0N6vZr0+TTZ3GyRV17pzlqmqkqT+/d7pculyOpqk7TbFfnZz1ZlLaMokHfeWSOtVuM7mc1CfvGLtQUzTh8XLoW6LHQsYgwrjhi2KM8I3357DOvX27F0qQWDg3H09cWz3jkFGCO8NzZasGSJGWNjCYTDCYyMZG8ZDQzEsXSpGTt2OKFpAj09MXR0ZL8LKhyW8Pt1tLW54HaraG+P4L33wlnLSAkcOxbG5s1OVFeb0NkZxXvvzW+XAhHNH8awebYYW1PAxIjo0xm5fd26iZHbGxqmP3J7rtbX+FJZOTFy+4YN0x+5ffVqmzSZCmfkdi5cCnVZ6DjEGFYcMWxR3ixDc0PyZhkqMoxhlC5TDFt0j08QERFNBxMhERGVNCZCIiIqaUyERERU0pgIiYiopC3K5wgBnDHu3eionnNUBgBwOpXUkEaRSCLrCOzjbDYBs9loT8TjMudIDoAxLp/dbpSREvD79ZxlFAVwuSa+k9+vI8sNv0RU5BjD5s+iTITLlllw++0T4/T96lcjOHAg+zh9NTUm3HLL9Mbpc7tVXH/9meP0PfzwAHp7s4/Td+mlZbjwwolx+r7//T60t2cfbuiCCxz4+Mcnxul7+OEBHDkSylqGiIoTY9j8WnRdo2vX2nDTTRU4cMCPw4dHAQCXXurGZZe5M5aprzdj/34v3n8/jN/8xg8AWLfOjmuu8cBmm/rROadTwUc/6kEiATz33AjicYmaGjM+/nFvxulIhAD27nWjpcWK554bwfBwHGazgptuqkBrqzVj/XbscGLPHjeee24EJ05EAAD793uxZYsjr31CRMWDMWz+LbpEuGaNDXa7gl//2n9Ga2P7dlfGMvX1ZtTWmnHwYBAvvRRI/X3dOjus1ql3kculYvVqG959dwzPP++Hrhvn+EuXWjIeROMTWnZ3x/D8834MDxvdCW63hpUrMx9E27a5MDaWwPPP+9HebhxEqiqwZYszYxkiKk6MYfNv0XWN6rqxfOlLtdA0AV2XEMLo+84kkZCIxSRuv90HKY3XufqupQRiMYnLL3enJr+Mx405thKJzIXjcYnVq224665aOBwq4nEJVQUSWbrl43GJ8nIVd91VC6tVSZXJ9p2IqDgxhs2/RTnEmqIAf/u3SzA0pOPgwQBWrbLhsceGMDKS/YLun/95NRRFoL09guFhY6Dbo0ezDwx7/fUeLF9uRSCg47nnRrBqlQ1PPTWctcz69Xbs3VuOUCiBf/3XPtx0UwXuv783a5nKSg1/9mfVGB7W8eijg7jxxgr8wz90zevFZg6xRsWGMYwxLF2mGLbozggBo6UTCOj4xje6AQCrVtnyLvftb/cgHpfYtcuV1w8kpXHRt6MjiuZmS951fPFFP37722DWmaDP3s7774fxox8NnPE3Ilp8GMPm16K7RkhERDQdi+6M8NJLy7BihRWqKtDYaMbeveWoqpr6wu+4NWts2LnThYoKDQ6HcQeU263iiSeGM5apqNBwww1eeL0aXnttFJ/+tA8ul4r33888L5eqAnfcUQWHQ8HLLwdx3XUe1NWZcraKbr65AhUVGgYG4ti61Yn16+2w2diGIVqMGMPm36K7RmgyCajqxIVgs9k4bc/2YKmqGuUAIBKRsFiMf0ejMuMFYCGQWi+9TDye/QLweDdCLCahqiJ5YdrYViYWi4AQxgV0KQEt2XzJ52HZ2cRrhFRsGMMYw9JlimGLLhHS3GEipGLDGEbpOB8hERHRFJgIiYiopDEREhFRSWMiJCKikrZoE6GmCbS2WlFRMb0nRFassKK+3jytMo2NZrS05P8gKgBUVWlobbVCmcYv4HIpaG21pqY/IaLFizFs/iy65wgBoK3NiYoKDTt2uHDsWBjHj0fw0ksBRCKZbyA77zwbamtN2LWrDMPDOg4fHsWRIyH09WWejqSuzoTVq23YvNkBq1XBb34TwMmT0azP4TgcCtranFi50oqGBgsOHPBjYCCOQ4dGM5YRAti9uwzV1SZ85CN2vP76KPr7Y3jhhUDGMkRUvBjD5ldhpeVZcOGFLlxxhRu9vTEAxuSPra3GtCaZtLZace21xrh5UhoTWno8Gvbvr4DTOfUu8npV7NvnhcOhpJ7VicclrrvOg6amqVtWimI8WNrcbEnNETYwEMeVV5ZnnY7kYx/zYPNmBwYHjQO6vz+GHTtcuOaa8nx2CREVEcaw+bfozghrakywWAR27SpDPC7R0mKF2SyQSGT+qm63CpdLxYUXGmPzeTwaystVWCxK6iHVs1ksCqqqTHC5VGia8VBpW5sTLpcKl2vqA08IoKHBgkRCIh43lt27y2C1Cvh8mUeOWLrUAqdTxYYNdsTjElu3OmGzKWhomF73BxEVPsawBSClzLgAkMW4fOELNVLTJl7femuldLvVrGVuuMEra2tNqde7dpXJNWtsWcts3uyQW7Y4Uq+XLbPIq64qz1qmutok/+iPvKnXNpuQn/tcVdYyqgp55501Z/ztS1+qlULM737Ndqxw4VKIy0LHIsaw4ohhi65rFADeeWcMra35jdY+rr09goYGM0Sq8SRh/H+UWX9/HDabkrrwm2t9ABgbS8Dv11FZqeVdJpEAjh+PoKnpzNZTPmWJqPgwhs2vRZkIn3lmBG63is2bjT7rV14Joq0t+0zIr746ikhE4pJLygAA778fQX29OevdTcePR3DqVBQXXeSCqhp95eFwAnV1mbsI/H4dhw6NYv16O7xeFbGYxNGjY1i7NvNBLyXwzDPDaGy0YPlyo+/+wAE/Lrww84zVRFS8GMPm16JMhABw9OhYag6vt98OZ/2Rxv3+9yGsX28ceB0dUfh8Wmog2kyOHYugudkCVRXw+3VEIglUVmYfKb6/Pw6LRYHLpULXgRMnIjnnAQuHJQYG4liyxGhRHT4cwrp19pzfiYiKE2PY/Fm0idBotQSxbZvRisp3lPMHH+zDDTd4ARijqedz6v7AA3249VYfAGN0dV3PXejZZ0ewc6cLFotI3a2Vy9tvj0HTBJYuNSfrl2FYeSIqeoxh84ezT1DeJGefoCLDGEbpMsWwRXtGSERElA8mQiIiKmlMhEREVNKYCImIqKQxERIRUUljIiQiopLGREhERCWNiZCIiEoaEyEREZU0JkIiIippTIRERFTSmAiJiKikMRESEVFJYyIkIqKSxkRIREQljYmQiIhKGhMhERGVNCZCIiIqaUyERERU0pgIiYiopGkLXYHp+m+bauEwKXijdxRPfDg8J9tY5rbgj8+rBAAkJPC/f9cBOSdbAv5ycy2smtEe+fkHQzjSF5qjLRFRIWAMKzxFlwgvqHbAY9UwEtHnbBtem4a2OhcAIJ6QMCkC0cTsH0aqADbVOOE0qwCAg11BHOmb9c0QUQFhDCs87Bo9i8eq4u+216deqwL45mVNc7Kt/3XhUjhMEz/Bn19Qgya3ZU62RUSlgTFs+ooqEa7z2WFWBQJRHe8Ojs3ZdhQhUv8WQkBAZFn7XLZjfH76dudmS0RUCBjDClNRJcIbV1XAYVLRG4rhmRMjC10dIqJpYQwrTEWTCK9sLkdLuRURPYGvv9aFzTUObK11LnS1iIjywhhWuIomEVY7THCaVUgJdASi8Fg1eK1Fd68PEZUoxrDCVTSJcJxVU/B/dzUudDWIiGaEMazwFF0iTLfWZ4fbos7qZ+5ucE/6m8Ok4IIqx6xuh4iIMawwFHUi3NVQhiq7aVY/85bkQ6jpyq0armiefHAREZ0LxrDCUFQd1N98vQvvDoYR0RMIRHRsrGYLh4iKB2NYYSqaM8JQLIGT/iiOj0QQjCYQTUgMhXXE52C0hPlg1xRoyuQnbma7m4SICgNjWOEqmkT4k3cH8IeBMVhUga/sbMDFDWVo90dwfCSy0FWbkVvWVKLVa5v096/ubIBZLcZHUokoG8awwlVUXaMAoCcknj0xPKfb6ApGceCUH4AxUsINrd4529Yj7w0gqhstwq11TjSVFd/wRESUP8awwlN0iTAugSePDeOKprm78Ns1GsUP/tAPwBinb9/KuTuIfvzOAIKxBACg0m4qyoOIiPLHGFZ4hJSZ+6eFEAXbeW3VjPHzxuKJWf3cKruGqC4xnDYyfJVdQ0SXszpavNOkwG5S0ReKpaZHKTOrsGoKekOxWdvObJJSFld/B5U8xrCJvzGGZY5hRZsIaf4xEVKxYQyjdJliWNHcLENERDQXmAiJiKikMRESEVFJYyIkIqKSxkRIREQljYmQiIhKWkEmwrLWMtjqbKi7vA7gDftEVGQYw4pLQSbCUEcIvi0+BE8EsezmZQtdHSKiaWEMKy4FN8Sao9EB1zIXul/oRsttLQtdHSKiaWEMKz4FN7KMalNRv7cetjob4qNx6GM6+g/1IzIQQXQ4Ot/VoTQcWYaKDWMYpSuakWXsS+ywVFow/IdhBI4FcOrxU/Bt8cHzEc9CV42IKCfGsOJTcIkQADS7Bj2sw7fVB6EJmMpMC10lIqK8MYYVl4JLhEIIBD4MwLHEAZPThOYbmzH4+0Fk68IlIioUjGHFp6ASoebU4NvmQzQQhed8D6L+KBxLHXA2OtH7Uu9CV4+IKCvGsOJ0TneN+qpNMFlmL5dqVhVuGYfFCjj8Yxj6j9Mov7IeEHHULS2+yR6JqLAxhhEww7tGPV4NdUstaGiywmpTZ6Ui/gonpCLg7gvMyuctRmVWM5aWO3F80I/RaHzet//oQz28a5SKCmNYYSnUGJb1jHDHJeVT/t1qU+Aqm91HEEfL7dBVlQdRFi6zCSt95egJhBbkICIqNoxhhaVQY1jWI8FXbZ6vehARzTrGMMpHQd0sQ0RENN8KZog1u38MCYV5OZvRaAzHB/wYi+sLXRUiOgtjWG6FGsMKJhGyXz234XAUb3T2L3Q1iGgKjGG5FWoMY/OFiIhKGhMhERGVNCZCIiIqaUyERERU0pgIiYiopDEREhFRScs61igREdFixzNCIiIqaUyERERU0pgIiYiopDEREhFRSWMiJCKiksZESEREJY2JkIiIShoTIRERlTQmQiIiKmlMhEREVNKYCImIqKQxERIRUUljIiQiopLGREhERCWNiZCIiEoaEyEREZU0JkIiIippTIRERFTSmAiJiKikMRESEVFJK7hEKIQ4IYQYE0IE05a6LOvvEkKcns86zichxA+FEF1CCL8Q4j0hxB0LXScimhrj19SEECuEEGEhxA8Xui5TKbhEmHStlNKZtnQuVEWEEOpCbTvpXgBNUsoyAB8F8FUhxMYFrhMRZcb4Ndm3Aby60JXIpFAT4RmEEBYhxH1CiM7kcl/ybw4ATwOoS299ZVo/7fP+e/Isq1MIcYcQQgohWpLvPSiE+I4Q4ikhxCiA3UKIq4UQbyTPyk4JIf4+7bOakuVvT743JIT4nBBisxDiiBBiWAjxrbT1lwshnhdCDAgh+oUQ/yaEKM/03aWUR6WUkfGXyWX5bO5fIpo7pRy/kmVuAjAM4FezuV9nlZSyoBYAJwBcdtbf7gHwOwBVAHwAfgvgK8n3dgE4PY319wLoBrAGgB3AD2Akl5bk+w8CGAGwA0ZDwZrcxkeSr9cB6AFwfXL9pmT5+5PrXgEgDOCx5PaXAOgFcHFy/RYAlwOwJOv2IoD70ur+TwD+6azv808AQsntvA7AudC/ExcuXCYvjF9nxi8AZQDeA9AA4O8B/HChf6Mpf7eFrkCGAykIowUxnPxBPgRwVdo6ewCcyHIgZVv/AQD3pr3XMsWB9K856ngfgH8860Bakvb+AIAb017/DMCdGT7regBv5LFfVAA7AXwZgGmhfycuXLhMXhi/Jr3/dQB3Jf9dsIlQQ2G6Xkr53PgLIcQYgPa099sBZLwAnXwv0/p1AA6lvXdqivJn/E0IsRXA/wGwFoAZRmvop2eV6Un799gUr53Jz6oC8A0AFwJwwWilDWX5LgAAKaUO4CUhxC0APp/8DCIqPIxfxrrrAVwGYMNU7xeSorhGCKATQGPa66XJvwFGa2Y663cBqE97r2GK8md/5kMAfgGgQUrphtGNIPKq+WT3Jj9/nTRugLllmp+lgdcIiYpJqcavXTDOOE8KIboBfAnAPiHE6zPc9pwplkT4IwBfFkL4hBCVAP4ngPHbcHsAVAgh3Hmu/xMAtwshVgsh7Mn3cnEBGJRShoUQWwB84hy+iwvJrhMhxBIAf5lpRSFElRDiJiGEUwihCiH2ALgZwPPnsH0iml8lGb8AfBdGo319crkfwJMwunoLSrEkwq/C6A44AuBNGDeMfBUApJTvwDhwjiXvcKrLsf7TME7tfw3gAwAvJ7cxfmfmVP4MwD1CiACMA+8n5/Bd7gZwAYwL2k8CeCT9TSHE/UKI+5MvJYxu0NMwuh/+Pxh99T8/h+0T0fwqyfglpQxJKbvHFxgJNCyl7DuH7c8JkbyIWbKEEKsBvAXAIqWML3R9iIjyxfg1O4rljHBWCSE+JoQwCyE8AL4G4HEeRERUDBi/Zl9JJkIAnwXQB+M2ZR1G9yMRUTFg/JplJd81SkREpa1UzwiJiIgAIPsD9UIIni5SipRyps8eES0IxjBKlymG8YyQiIhKGhMhERGVNCZCIiIqaUyERERU0pgIiYiopDEREhFRSWMiJCKikpb3xLwN1zbAXm/Pud7xHx1HdDh6TpWaLtWqouX2lpzrRQejOP7j4+e0rVs/2oRl9c6c633zoffQP8/7wW5VcdenV+dcr3cwjG8//ME81IiocDCGGRjDJss6xFr6w6iaU4NiMk4gE7EEEpFEciVAc0zk09hIDDJhFPNt96GqrSpnJTr+owPDfxgGADgaHWja15SzzNCbQ+j8ZWeqDuZyc+q9eCgOJKunmBUoZqPeUpeI+WM5Pzsbt8sEs2Z8XjSeQDisG1UQgMthSq03OBKFntwPe7bX4PLtNTk/++GnT+LQ0UEAwMomF/70htzz7x48MoCfPnsqVYfKckvqvWAojkSyDhazAotZBQDEdYkh//QPcD5QT8WGMWwyxrDJ8k6ES/Ysga3OBgDwv+dH73/2AgAUi4Jln1iWKtP+SDtiI8YPZfaYYfEYX0qP6gidDqXWczY5IRSjTmO9Y4gHjcHTVZsKe63RapOQCB4L5vxy2erg3eCFd70XABAdiuLkYydzfl42N+1disY6BwDg9+8N4z9e6gIA2Cwq/usfr0yt9y8/+xCDI8YP5fNY4PMa+yESTeDDUxPfqbXJBVU19kNH7xhGAka9HTYNjXXJ/SCBt4/5U2Uaa+1w2I3/cYf8UXT1hXPWYecFldix3gcA6BuK4IFHj037uzMRUrFhDJuMMWyyvLtGQ50ho5UCINwbnvhgXSLwQSD1OhFNpP5t8VrgWu4CAMSD8TMPomZnqnUWD8dTB5Fm1+BqcSU/HAgeDxrT02b7clnqEBmMpN6Lj577TCXHO0YRSO6Hjp6J7xPXJd76YGRiu2l1qKqwYs3yMgDASDB2xkG0elkZzMn9EBrTUweR065hbYsxabWUwDvH/RhvszQtcaK6wjgoj50eTR1E2erQOxBJvRcYPbcWJVExYgwzMIZNlncijPknugti6ZWQxg+VeqlP/OKaTYMl2YoQ6pmJ2OKxpE73VYua+rtiUlJlAKD5xuYp6xP4MID+V/tz1kEP6an39GQXwLkYDkRTp+qBtINSSonegYn/uXR94gd02jRUVVgBAJp65v1JPo8VFovxN2vafjCblFSZ8e2N85SZUu/1D6V97yx1CI7FU++NRc59PxAVG8YwA2PYZHknwoqNFXA0GKfTQ28NYaxzDIDxo9ddVpdaL9QZQjRqnMr6j/kR6jZaHOk/LAB0H+gGksdVLDBxUIYHwuh8tnPqSqTt//QDIlsdXMtd8G31pT478OFEq2smLtzoQ0uD0dp79a0BnOgcBWD86Psub0itd6JzFOGo8QMf/XAEJ7uN9eLxM/fDL17ogJL8XsNpff+9A2H87JdGv7mUQHoP9ouv9cFinmiBjctWhzXLy3DpVqOPv2cgfEari6gUMIYZGMMmy/saocltSnUDQBqZO7WeMtFSig5FUweMr813xg947IcTfbqtn2tNtaI6nu3AyNu5v1TjxxvPOJC7ftWVrABgqZhogSFh9M0nv0PqYJVxec53g3ndZljGL7iftR/UtP3QOxSBntwPl2+rwaVt1QCAnv4w/vEH76bW+7vPr4XNauyHnz5zEq/9YQgAsKLRhT/5uHHNIJGQ+JtvHEkdSHfsW4aWpckD+c0B/Oy508nvCtQkW1kAUhe6x/fDePViujyjFZYvXiOkYsMYNhlj2GR5J8Ka3TWwVdtybuj0k6dTrSOrzwprlTVHCSDUEUr9uJpDg7Mp9629kaHIRIvOoqDxY405y0RHouh4uiPnetlcf8kS1NfkvgX73544gaFk66iuyoYlvtz77lhHEAPJ/VDmNKG10ZWzTN9QJNWis1oU3LEv911ag8NRPPRUe871zsZESMWGMWwyxrDJ8u4a7f5197Q3qtrVM1s5GYT7J/qEFbOSV5lENIExGAdRIpLA8YfP7dmafD32/PQPQqdNQ3VF7v+ZuvrHUv82m5S8ykRiE33o4UgC33ro/WnXj6gUMIYZGMMmy/uMkIhnhFRsGMMoHSfmJSIimgITIRERlTQmQiIiKmlMhEREVNKYCImIqKQxERIRUUljIiQiopJW0Ilw0ybHtMusWmWFzTa9r1Vba0J1dd5jCwAAHA4FK1bkflg0nRDABRfkHtHhbDPZD0S08BjDDIUewwo6EQYCOq6+uvyMv117bfmU644bHU1g7153ahBYALj4Yhfs9sxfNRKR2LDBgfLyiZHTzzvPhvp6c8Yyui5RW2tCS8vECBJVVRo2bMh+kCiKwLZtE8MvmUwCl1/uzlpmJvuBiBYeY5ih0GNYQSfCd98N4+jRMVx9dTmEAPbt8+LIkRBuuaUiY5lTp6J44QU/br3VByGA7dudGBrS8YlPZC4zOBjHSy8FcN11HpjNAk1NFtTWmtDW5oTbrU5ZJhyWOHgwiJUrbaitNcFuV3DppW44nSpaW6duZUkJvPHGKGIxiQ0b7BAC+NSnKnHqVASXXFI2q/uBiBYeY9jM98N8KuhECAB+fxyDg3Hcc089jh0Lo6srZozGnkUiAbzwgh/33FMPj0fD22+PIctIcgCMH/jJJ4fxV39Vh8suK8PLLwcRi+UenengwSD27fPizjtr8NRTwwiFEshRPRw7FsGyZVbcc089nn56GCMjes4yM9kPRLTwGMMMhRzDCnqsUUUB/uZvlqC/PwazWYHDoeDpp4fxxhuhrOXuvLMGIyM6ampMGBtL4OjREH7968CkebTS7dvnRXm5ioYGMzo7YwgGdTz++BACgUTGMps2OdDW5oTHo+HUqQjcbg0PPzyAnp7MsydXV2u49VYfdF3C79dRU2PGD3/Yj+PHM08pMtP9MNs41igVG8YwxrB0RTvW6PjODgZ1xGIS113nyavc4GAckUgCoVACu3e74XDk/qpDQ3HoOjAyomPtWjsaGnKPID9er8FBHdXVJmzcmPuicCQiMTYmMTysQ9ME9uzJ3r8OzHw/ENHCYgwzFHIMm95tRgvAblewZs3ExdtoNHPrZpwQwNatxsXcysr8t7Vpk1Hm/PPzvytq5Upjjq62ttzzj42rrjYBQNYL2WebyX4gooXHGGYo5BhW0IlQ0wQGB+P47nd7U68/85mqrGVU1egr/9rXOqHrEqoqsH+/N2sZRTGW732vFz09MSiKwI4d2Q8KIQBVFcnT+1EIASxbZsWSJdkPDE0TeOutEH7xC2MWZ6dTxfXXZ28ZzWQ/ENHCYwybKFPIMaygu0ZvvLECvb1xBIMJBIMJXHxxGfx+PWuZSy91IxaTCAR0BIMJtLZaYbUq0PXMfetr1thQXW3C8LBRxu1WsXy5FeFw5hZLRYWGzZsdGBw06heLSezc6cLISDxjGVUFrr/ei76+WOo7XXNNOfr7M5eZ6X4gooXHGDbz/TCfCvpmGSosvFmGig1jGKUr2ptliIiI5hITIRERlTQmQiIiKmlMhEREVNKYCImIqKQV9HOEAOB0KrjmmolnVB59dBCRSO4bwfbv90JVjRuEDhzwo6sr85BB4/bsccPjMXbJ0aMhvPnmWM4ymzY50NJiDFDb2xvD88/7c5ZpbDRj2zYXAOOh0kceGcpZZqb7gYgWFmOYoZBjWEEnQkUBPvvZKng8GhRFIJGQqKqqwje+0ZO13G23VaKlxQpFEZBSorHRjG9/uwfBYOZnavbscWPbNidMJuPAW77cgrGxBD74IPP4eWvX2rB3bzlsNqNMNGqFrkscOBDIWKaiQsPNN1fC5VIghICuS5jNCh5+eGDW9wMRLSzGsHPbD/OloLtGjelDBJ56ahgA8Mgjg7BaFZSVTT2tCGBMNun1anjwwT4AwMsvBzE4GIfXmznnWywCFRUafvzjAcTjQHt7FG+8EYLXq50xJ1g6VTUOiN/+NoCenhjGxiR+/vMheL0azObMj9t5vRo6OqKpwWa///0+VFZqWecam8l+IKKFxxg28/0wnwr6gfovfrEGQ0M6fD4Np09H0dhoQVdXDA6Hgm99a+qWxI03euFwGCOwf/BBGFVVJoRCCVRVmfCVr3RMWaatzYmNGx2oqNBw/HgEdrsCu12BxaLgn/+5B0NDk0dAqK014ZZbKiElMDwcRywmUV9vRiCg44UXAjhyZPKo6qoK/PVfL8HAQByaBgwMxLFihRUdHVF0d8fw+OPDs7Yf5gIfqKdiwxjGGJYuUwwr6K7RRAL48Y8HsHWrE6oq0NkZwyuvBHHbbb6s5R5/fAjnnWeD2aygqyuGw4dDuP327CPXvvxyAIoi4PFokFLiww8j2L49+1h9H3xgTDa5dKkZQgj8/vehnCPEj4zE8bOfDWDtWjsURaC93Y/29gjWr888SO5M9wMRLSzGMEOhx7CC7hp97LEh7NnjRm9vDLt3l+HkyQiuuqo8NdjrVA4cCGD37jK8+WYIu3cbMyZv3GjH00+PZCzz9ttjqKkx49ixMHbudKKhwQKXS8WHH0YyzuU1OBhHX18cQgArV1qxZYsD3d0xOBwqjh0LT1kmkTDq19pqg9WqpOq5ZYsTv/tdcFb3AxEtPMawme+H+VTQibC9PYKlS8+cT6ux0YJTp6IZy3R3x1Bbaz5j5uO6OjM6OjKXGRnRYbMZ3QjjvF4Vfr+ecSLMSEQiHE4k+74NNpsCTRMZL2hLCXR3R1NTmADGRWSfT0NfX+ZBa2eyH4ho4TGGGQo9hhV01+i4cDiBzs7otG611XWJzs4oAoHpjXDe1RXDwED2kdTP1t8fRzQ6vUsRfr+Ozs4oEtOYkmsm+4GIFh5jmKFgY5iUMuMCQC7kIgTkFVe4z/jb3r3unOUuv9wtFWXi9Y4dTmmzKVnLbNrkkG63mnrd2mqVdXWmrGWamiyyudmSel1Zqcl162xZy5SVqXLzZkfqtckk5K5dZXOyH2Z7yXascOFSiAtjGGNYPjGsoO8apcIiedcoFRnGMEqXKYYV9DVCIiKiucZESEREJY2JkIiIShoTIRERlTQmQiIiKmmLOhGKGdzjKMT0y82kzHi5+ShDRMWJMWx+LMpE6HIpKC9Xcffd9di1ywWHQ8m58+12BaoKfPnLS/DJT1bCYhGp6UwyMZsFzGaBP/3TKtx1Vx00DanpTDJRFGNbV11VjrvvrofNpsDpzP0zuFwK1qyx4e6767FsmaVgRm0notnHGDa/imJkmelYssSEtjYXTp6MQNMEqqtNuOSSMrS3R3DkyNSTVHo8KtranOjtjcNkEigrU7F9uwvhcAKHDo0iFpv8KJLVKrB5sxNCGNOmmM0CF1zggNut4eDBIPz+yaNBCAGsX29HTY0ZlZUaNE1g9WobGhvNePnlILq7p554s6XFivXr7QgGdWiaQGOjBRs3OnDo0CiOH8881xgRFR/GsPm36M4Id+504cUX/am5uxwOFf/5n0Fcfrk7Y5nWVht6emJQFOOHNpsFTpyIYNkyS8aWjsejobJSQ3d3DCaTgKIA0ahEKKSjqckyZRlFMer32mujqbm73G4Vhw6NYvNmR8b6XXttOZ5+ehhut/GdvF4VTz01jKuuKs9nlxBREWEMm3+LLhECgJRIjYR+4kQEg4P5jbv36qujSCQkhob0vFsp770XRiCgIxaTOHx48vxdU+npiaUGm/3d7zLPBJ0uFErg6FHj8w8fDk17XEAiKh6MYfNr0SVCXTdaRDffXIF77+2AEMCyZRbEsxxHiYREIgHcfrsP993XjVdeCeKii1xZy0hpbGvPHjeeeWYE//IvvfjEJyqQSBifl0k8LrFmjQ3Dw3Hce28HbrvNBymRdeDaeFzC69WwapUN997bgW3bnHA4FOh64RxIRDQ7GMPm36JLhP/+74MAjNZHIJBAMJhAWZmKb32rO2OZV14x+qnjcYlAQEcgoMNsFvj5zwennNkZMKZK+eUvR6BpAoGADr9fRyIBvPVWCG+9NXU/vq4D3/1uHxwOBcGgUb9wWCIUSuDJJ4cz1u+b3+yBx6NidFQ/4zvdf39v/juGiIoCY9j8W3SJEAA2bHDg9ddHAQAnT0ZQV2eG2Zz9q65YYcX774eh68DQUByJBFJ99JlUV5sQDOoIBhOIxSROnoygudmatYzdrsDj0dDZaXQrvPHGKM4/P/PMzoDROly92oZ33jEmy3zrrVDOMkRUvBjD5lkhT2FyLsv559slAFlfb5bl5WpeZT7yEZtUFEi3W5X19ea8yjQ3W6TDoUhNg1y9Ovv0JeNLRYUma2qM6VHWr7fnVcZqFXLFCqsEIFetskpNE/O+Txd6Sh0uXKa7LHQcYgwrjhjGaZgob5LTMFGRYQyjdJli2KLsGiUiIsoXEyEREZU0JkIiIippTIRERFTSFt1Yo+PSB6jNcj/QOZdJLzeTMtMpN9P6EVHxYQybP4syEZaVqfiLv6iBqhp7/dFHB3H4cCjrjrfbFfzJn/jg85kAAAcPBvHssyNTDlY7zmwW+OhHPVi3znge5tSpKH7wgz6Ew5nLKAqwdasTV15ZDgCIRBL4+te7EQxmGZYBQFOTBZ/+tC/1+jvf6UFX19QD3BJRcWMMm1+Lrmu0rs6Ez3++CqdORTE2lsDQUBz791dgw4bMD296PCpuvrkCY2MJJBISnZ1R7Njhwu7dZRmnMbFYBPbscaO21oRYTOLEiQiamy244YaKjNOLCAFs2eLERRe54PfrOH06CqtVwWc+U4XqalPG+q1YYcWnPlWJ7u4YBgfjiEQS+NznqtHcPPXAuERUvBjD5t+iS4QejwZVFejriyESSSAYNIYXGm8lTcVuV2CzKejvj0PXkRrg1uNRoWaYMstkEnC7NYyMGIPV9vYaLRuHQ4HVmvlxO59Pw9hYAqOjOgYGjNEfjM/KPDeXz6dB1yWGhuLw+3VEIhKKAlRWLsoTeqKSxhg2/xblA/XNzcYp+Hi3wi9/OYLf/MafdQDa6moTPvnJytSQREeOhPDEE0NZT/fLylR87GMetLbaAADd3VH86EcD6OvLvCGzWeCSS8pw0UVlAIBoNIHvfa8vNZJ7Jhs3OrBvnzf1+qGH+jOOBzhX+EA9FRvGMMawdJliWGGk41l2+nQUAwNxPPBAH7Zvd6KrK5r1AAKAvr4YQiEdDzzQiyVLzKipMWN0NHufdyCgY2hIx4MP9mFkRMfVV5djYCD7hqJRiZ6eGJ59dhivvx7CHXf4UmP2ZXPqVARHj4bw+OPD2L/fm/OgI6LixRg2zxbjOH1f+EKNNJuFrK42SadTkbfeWinLyrKP1bdvn1fW1ppkVZUmPR5V7trlkuedl33cvU2bHHLLFof0ejXp82myudkir7zSnbVMVZUm9+/3SpdLkdXVJmm3K/Kzn63KWkZRIO+8s0ZarcZ3MpuF/OIXawtmnD4uXAp1WehYxBhWHDFsUZ4Rvv32GNavt2PpUgsGB+Po64tnvXMKMEZ4b2y0YMkSM8bGEgiHExgZyd4yGhiIY+lSM3bscELTBHp6YujoyH4XVDgs4ffraGtzwe1W0d4ewXvvhbOWkRI4diyMzZudqK42obMzivfem98uBSKaP4xh82wxtqaAiRHRpzNy+7p1EyO3NzRMf+T2XK2v8aWycmLk9g0bpj9y++rVNmkyFc7I7Vy4FOqy0HGIMaw4YtiivFmG5obkzTJUZBjDKF2mGLboHp8gIiKaDiZCIiIqaUyERERU0pgIiYiopDEREhFRSVuUzxECOGPcu9FRPeeoDADgdCqpIY0ikUTWEdjH2WwCZrPRnojHZc6RHABjXD673SgjJeD36znLKArgck18J79fR5YbfomoyDGGzZ9FmQiXLbPg9tsnxun71a9GcOBA9nH6ampMuOWW6Y3T53aruP76M8fpe/jhAfT2Zh+n79JLy3DhhRPj9H3/+31ob88+3NAFFzjw8Y9PjNP38MMDOHIklLUMERUnxrD5tei6RteuteGmmypw4IAfhw+PAgAuvdSNyy5zZyxTX2/G/v1evP9+GL/5jR8AsG6dHddc44HNNvWjc06ngo9+1INEAnjuuRHE4xI1NWZ8/OPejNORCAHs3etGS4sVzz03guHhOMxmBTfdVIHWVmvG+u3Y4cSePW4899wITpyIAAD27/diyxZHXvuEiIoHY9j8W3SJcM0aG+x2Bb/+tf+M1sb27a6MZerrzaitNePgwSBeeimQ+vu6dXZYrVPvIpdLxerVNrz77hief94PXTfO8ZcutWQ8iMYntOzujuH55/0YHja6E9xuDStXZj6Itm1zYWwsgeef96O93TiIVFVgyxZnxjJEVJwYw+bfousa1XVj+dKXaqFpArouIYTR951JIiERi0ncfrsPUhqvc/VdSwnEYhKXX+5OTX4ZjxtzbCUSmQvH4xKrV9tw1121cDhUxOMSqgoksnTLx+MS5eUq7rqrFlarkiqT7TsRUXFiDJt/i3KINUUB/vZvl2BoSMfBgwGsWmXDY48NYWQk+wXdP//zaiiKQHt7BMPDxkC3R49mHxj2+us9WL7cikBAx3PPjWDVKhueemo4a5n16+3Yu7ccoVAC//qvfbjppgrcf39v1jKVlRr+7M+qMTys49FHB3HjjRX4h3/omteLzRxijYoNYxhjWLpMMWzRnRECRksnENDxjW90AwBWrbLlXe7b3+5BPC6xa5crrx9ISuOib0dHFM3Nlrzr+OKLfvz2t8GsM0GfvZ333w/jRz8aOONvRLT4MIbNr0V3jZCIiGg6Ft0Z4aWXlmHFCitUVaCx0Yy9e8tRVTX1hd9xa9bYsHOnCxUVGhwO4w4ot1vFE08MZyxTUaHhhhu88Ho1vPbaKD79aR9cLhXvv595Xi5VBe64owoOh4KXXw7iuus8qKsz5WwV3XxzBSoqNAwMxLF1qxPr19ths7ENQ7QYMYbNv0V3jdBkElDViQvBZrNx2p7twVJVNcoBQCQiYbEY/45GZcYLwEIgtV56mXg8+wXg8W6EWExCVUXywrSxrUwsFgEhjAvoUgJasvmSz8Oys4nXCKnYMIYxhqXLFMMWXSKkucNESMWGMYzScT5CIiKiKTAREhFRSWMiJCKiksZESEREJW3RJkJNE2httaKiYnpPiKxYYUV9vXlaZRobzWhpyf9BVACoqtLQ2mqFMo1fwOVS0NpqTU1/QkSLF2PY/Fl0zxECQFubExUVGnbscOHYsTCOH4/gpZcCiEQy30B23nk21NaasGtXGYaHdRw+PIojR0Lo68s8HUldnQmrV9uwebMDVquC3/wmgJMno1mfw3E4FLS1ObFypRUNDRYcOODHwEAchw6NZiwjBLB7dxmqq034yEfseP31UfT3x/DCC4GMZYioeDGGza/CSsuz4MILXbjiCjd6e2MAjMkfW1uNaU0yaW214tprjXHzpDQmtPR4NOzfXwGnc+pd5PWq2LfPC4dDST2rE49LXHedB01NU7esFMV4sLS52ZKaI2xgII4rryzPOh3Jxz7mwebNDgwOGgd0f38MO3a4cM015fnsEiIqIoxh82/RnRHW1JhgsQjs2lWGeFyipcUKs1kgkcj8Vd1uFS6XigsvNMbm83g0lJersFiU1EOqZ7NYFFRVmeByqdA046HStjYnXC4VLtfUB54QQEODBYmERDxuLLt3l8FqFfD5Mo8csXSpBU6nig0b7IjHJbZudcJmU9DQML3uDyIqfIxhC0BKmXEBIItx+cIXaqSmTby+9dZK6XarWcvccINX1taaUq937SqTa9bYspbZvNkht2xxpF4vW2aRV11VnrVMdbVJ/tEfeVOvbTYhP/e5qqxlVBXyzjtrzvjbl75UK4WY3/2a7VjhwqUQl4WORYxhxRHDFl3XKAC8884YWlvzG619XHt7BA0NZohU40nC+P8os/7+OGw2JXXhN9f6ADA2loDfr6OyUsu7TCIBHD8eQVPTma2nfMoSUfFhDJtfizIRPvPMCNxuFZs3G33Wr7wSRFtb9pmQX311FJGIxCWXlAEA3n8/gvp6c9a7m44fj+DUqSguusgFVTX6ysPhBOrqMncR+P06Dh0axfr1dni9KmIxiaNHx7B2beaDXkrgmWeG0dhowfLlRt/9gQN+XHhh5hmriah4MYbNr0WZCAHg6NGx1Bxeb78dzvojjfv970NYv9448Do6ovD5tNRAtJkcOxZBc7MFqirg9+uIRBKorMw+Unx/fxwWiwKXS4WuAydORHLOAxYOSwwMxLFkidGiOnw4hHXr7Dm/ExEVJ8aw+bNoE6HRagli2zajFZXvKOcPPtiHG27wAjBGU8/n1P2BB/pw660+AMbo6rqeu9Czz45g504XLBaRulsrl7ffHoOmCSxdak7WL8Ow8kRU9BjD5g9nn6C8Sc4+QUWGMYzSZYphi/aMkIiIKB9MhEREVNKYCImIqKQxERIRUUljIiQiopLGREhERCWNiZCIiEoaEyEREZU0JkIiIippTIRERFTSmAiJiKikMRESEVFJYyIkIqKSxkRIREQljYmQiIhKGhMhERGVNCZCIiIqaUyERERU0pgIiYiopDEREhFRSdMWugLT9d821cJhUvBG7yie+HB4TraxzG3BH59XCQBISOB//64Dck62BPzl5lpYNaM98vMPhnCkLzRHWyKiQsAYVniKLhFeUO2Ax6phJKLP2Ta8Ng1tdS4AQDwhYVIEoonZP4xUAWyqccJpVgEAB7uCONI365shogLCGFZ42DV6Fo9Vxd9tr0+9VgXwzcua5mRb/+vCpXCYJn6CP7+gBk1uy5xsi4hKA2PY9BVVIlzns8OsCgSiOt4dHJuz7ShCpP4thICAyLL2uWzH+Pz07c7NloioEDCGFaaiSoQ3rqqAw6SiNxTDMydGFro6RETTwhhWmIomEV7ZXI6WcisiegJff60Lm2sc2FrrXOhqERHlhTGscBVNIqx2mOA0q5AS6AhE4bFq8FqL7l4fIipRjGGFq2gS4TirpuD/7mpc6GoQEc0IY1jhKbpEmG6tzw63RZ3Vz9zd4J70N4dJwQVVjlndDhERY1hhKOpEuKuhDFV206x+5i3Jh1DTlVs1XNE8+eAiIjoXjGGFoag6qL/5ehfeHQwjoicQiOjYWM0WDhEVD8awwlQ0Z4ShWAIn/VEcH4kgGE0gmpAYCuuIz8FoCfPBrinQlMlP3Mx2NwkRFQbGsMJVNInwJ+8O4A8DY7CoAl/Z2YCLG8rQ7o/g+Ehkoas2I7esqUSr1zbp71/d2QCzWoyPpBJRNoxhhauoukYBQE9IPHtieE630RWM4sApPwBjpIQbWr1ztq1H3htAVDdahFvrnGgqK77hiYgof4xhhafoEmFcAk8eG8YVTXN34bdrNIof/KEfgDFO376Vc3cQ/fidAQRjCQBApd1UlAcREeWPMazwCCkz908LIQq289qqGePnjcUTs/q5VXYNUV1iOG1k+Cq7hoguZ3W0eKdJgd2koi8US02PUmZWYdUU9IZis7ad2SSlLK7+Dip5jGETf2MMyxzDijYR0vxjIqRiwxhG6TLFsKK5WYaIiGguMBESEVFJYyIkIqKSxkRIREQljYmQiIhKGhMhERGVtKJ7oH6hXXlhLZb4Jg8rdLafPnsKI8HCfJaGiEoXY9hkTITTtG5FOda0GCNCnP0MphATj6g8+WJnyRxERFQ8GMMmYyI8By+93of//xcnAABOu4b77rpgYStERDQNjGEGJsJzoCeAaHKMvfH/EhEVC8YwA2+WISKiksZESEREJY1do+dACEBNztCsFtlElEREjGEGJsJzcNFGH3Ze4AMAlO4hRETFijHMcE6J0FdtgslSWr2rAT2C08PBnOt5q00Qds4AQ1TIGMMyK6UYNqP5CD1eDXVLLWhossJqU2elIv4KJ6Qi4O4LzMrnLUZlVjOWljtxfNCP0Wh83rf/6EM9pdxopCLEGFZYCjWGZT0j3HFJ+ZR/t9oUuMpmt1d1tNwOXVV5EGXhMpuw0leOnkBoQQ4iomLDGFZYCjWGZT0SfNXm+aoHEdGsYwyjfJRW5zgREdFZCuauUbt/DAmFeTmb0WgMxwf8GIvrC10VIjoLY1huhRrDCiYRsl89t+FwFG909i90NYhoCoxhuRVqDGPzhYiIShoTIRERlTQmQiIiKmlMhEREVNKYCImIqKQxERIRUUnLOtYoERHRYsczQiIiKmlMhEREVNKYCImIqKQxERIRUUljIiQiopLGREhERCXt/wEpEtAjsSkeKQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x504 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Omplim 'state_stack' amb 4 còpies idèntiques del frame inicial de la partida\n",
    "state_stack= stack_frame(None, env.reset(), True)\n",
    "\n",
    "# Juguem una partida aleatòria i anem actualitzant els 4 frames de 'state_stack'\n",
    "# conforme es van produint de nous\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    state_stack = stack_frame(state_stack, next_state, False)\n",
    "\n",
    "    if done:\n",
    "         break\n",
    "\n",
    "# Mostrem els darrers 4 frames de la partida que han quedat enmagatzemats a 'state_stack'\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "rows = 2\n",
    "columns = 2\n",
    "cont=1\n",
    "\n",
    "for i in state_stack:\n",
    "    fig.add_subplot(rows, columns, cont)\n",
    "    plt.imshow(i)\n",
    "    plt.axis('off')\n",
    "    plt.title('Fotograma:' + str(cont))\n",
    "    cont = cont +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara amb aquestes modificacions podem passar al següent punt, la creació de la Xarxa\n",
    "<p></p>\n",
    "<img src=\"imatges/deep-q-network.jpg\"  width=\"1000\">\n",
    "\n",
    "Font: <href>https://huggingface.co/blog/deep-rl-dqn</href>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com s'observa a la imatge, farem servir un model convolucional (perfecte per detectar patrons en imatges) juntament amb un sistema lineal completament connectat. Per a la xarxa convolucional us proposem la següent configuració:\n",
    "\n",
    "<ul>\n",
    "    <li>Una primera capa del tipus Conv2d amb els paràmetres següents in_channels=4, out_channels=32, kernel_size=8, stride=4 amb activació ReLU.</li>\n",
    "    <li>Una segona capa del tipus Conv2d amb els paràmetres següents in_channels=32, out_channels=64, kernel_size=4, stride=2 amb activació ReLU.</li>\n",
    "    <li>Una tercera capa del tipus Conv2d amb els paràmetres següents in_channels=64, out_channels=64, kernel_size=3, stride=1 amb activació ReLU.</li>\n",
    "</ul>\n",
    "\n",
    "El resultat d'aquesta primera xarxa (3D) el connectarem amb un model complementament connectat (1D) amb la següent estructura lineal:\n",
    "\n",
    "* Una primera capa completament connectada (representada en pytorch per nn.Lineal) de 512 neurones, amb activació ReLU\n",
    "* Una darrera capa completament connectada. Aquesta serà la nostra capa de sortida i, per tant, tindrà tantes neurones com dimensions tingui el nostre espai d‟accions (una sortida per cada acció possible).\n",
    "\n",
    "Finalment, farem servir l'optimitzador Adam per entrenar la xarxa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>2.6 Exercici (0.5 pts): </strong>Implementar la classe <code>DQN_CNN()</code>. Inicialitzar les variables necessàries i definir el model CNN i lineal indicat.\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se us proporciona el codi pre-implementat. La implementació que es demana a l'enunciat està indicada als blocs <i>TODO</i> i/o amb variables igualades a <i>None</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class DQN_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-3):\n",
    "        super(DQN_CNN, self).__init__()\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        n_inputs: mida de l'espai d'estats\n",
    "        n_outputs: mida de l'espai d'accions\n",
    "        actions: array d'acciones possibles\n",
    "        device: cpu o cuda\n",
    "        red_cnn: definició de la xarxa convolucional\n",
    "        red_lineal: definició de la xarxa lineal\n",
    "        \"\"\"\n",
    "        #######################################\n",
    "        ###TODO: Inicialització i model ###        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_shape = (4,84,84)\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        #######################################\n",
    "        ##TODO: Construcció de la xarxa neuronal convolucional\n",
    "        self.red_cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.red_cnn.cuda()\n",
    "\n",
    "        #######################################\n",
    "        ##TODO: Construcció de la xarxa neuronal lineal completament connectada\n",
    "        self.fc_layer_inputs = self.feature_size()\n",
    "        self.red_lineal = nn.Sequential(\n",
    "            nn.Linear(self.fc_layer_inputs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.n_outputs)\n",
    "        )\n",
    "\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.red_lineal.cuda()\n",
    "\n",
    "        #######################################\n",
    "        ##TODO: Inicialitzar l'optimitzador\n",
    "        ## Recupera els paràmetres de la xarxa neuronal i inicialitza l'optimitzador amb el learning rate indicat en el constructor      \n",
    "        self.optimizer = optim.Adam(self.parameters(), self.learning_rate)\n",
    "\n",
    "    ### Mètode e-greedy\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:            \n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)\n",
    "            action= torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array(state)\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        cnn_out = self.red_cnn(state_t).reshape(-1,  self.fc_layer_inputs)\n",
    "        return self.red_lineal(cnn_out)\n",
    "\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.red_cnn(autograd.Variable(torch.zeros(1, *self.input_shape)).to(device=self.device)).view(1, -1).size(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim la classe per al *buffer* de repetició d'experiències:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "class experienceReplayBuffer:\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.buffer = namedtuple('Buffer',\n",
    "            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size,\n",
    "                                   replace=False)\n",
    "        # Ús de l'operador asterisc per desempaquetar deque\n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(\n",
    "            self.buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Definició de l'agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuació, implementarem una classe que defineixi l'entrenament de l'agent tenint en compte:\n",
    "     <ul>\n",
    "         <li>L'exploració/explotació (decaïment d'epsilon)</li>\n",
    "         <li>L'actualització i sincronització de la xarxa principal i la xarxa objectiu (pèrdua)</li>\n",
    "     </ul>\n",
    "\n",
    "A més, considerarem que l'agent ha superat l'entorn quan obtingui una puntuació superior a 350 en 100 partides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>2.7 Exercici (1 pt):</strong> Implementar els punts següents de la classe <code>DQNAgent()</code>:\n",
    "    <ol>\n",
    "        <li>Declarar les variables de la classe</li>\n",
    "        <li>Inicialitzar les variables necessàries</li>\n",
    "        <li>Implementar l'acció que cal prendre</li>\n",
    "        <li>Actualitzar la xarxa principal segons la freqüència establerta als hiperparàmetres</li>\n",
    "        <li>Calcular l'equació de Bellman</li>\n",
    "        <li>Sincronitzar la xarxa objectiu segons la freqüència establerta als hiperparàmetres</li>\n",
    "        <li>Calcular la mitjana de recompenses dels darrers 100 episodis</li>\n",
    "        <li>Actualitzar epsilon segons: $$\\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, 0.01)$$ </li>\n",
    "    </ol>\n",
    "A més, durant el procés s'han d'emmagatzemar (*):\n",
    "    <ul>\n",
    "        <li>Les recompenses obtingudes a cada pas de l'entrenament</li>\n",
    "        <li>Les recompenses mitjanes dels 100 episodis anteriors</li>\n",
    "        <li>La pèrdua durant l'entrenament</li>\n",
    "        <li>L'evolució d'epsilon durant l'entrenament</li>\n",
    "    </ul>\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se us proporciona el codi pre-implementat. La implementació que es demana a l'enunciat està indicada als blocs <i>TODO</i> i/o amb variables igualades a <i>None</i>, excepte en quin moment emmagatzemar les variables que s'indiquen a (*).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy, copy\n",
    "\n",
    "class DQNAgent:\n",
    "    ###################################################\n",
    "    ######TODO1: Declarar variables ##################\n",
    "\n",
    "    def __init__(self, env, main_network,\n",
    "                 buffer, reward_threshold,\n",
    "                 epsilon=0.1, eps_decay=0.99, batch_size=32, nblock=100):\n",
    "        \"\"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorn\n",
    "        main_network: classe amb la xarxa neuronal dissenyada\n",
    "        target_network: xarxa objectiu\n",
    "        buffer: classe amb el buffer de repetició d'experiències\n",
    "        epsilon: epsilon\n",
    "        eps_decay: epsilon decay\n",
    "        batch_size: batch size\n",
    "        nblock: bloc dels X darrers episodis dels quals es calcularà la mitjana de recompensa\n",
    "        reward_threshold: llindar de recompensa on es considera que s'ha assolit el problema\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        self.env = env\n",
    "        self.main_network = main_network\n",
    "        self.target_network = deepcopy(main_network) # xarxa objectiu (còpia de la principal)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.nblock = nblock # són els blocs dels X episodis dels quals es calcularà la mitjana de recompensa\n",
    "        self.reward_threshold = reward_threshold # llindar de recompensa, si es supera es para el procés d'entrenament\n",
    "        self.initialize()\n",
    "\n",
    "    ###################################################################\n",
    "    #####TODO2: Inicialitzar variables extra que són necessàries######\n",
    "    def initialize(self):\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        inicio_juego = preprocess_observation(self.env.reset())\n",
    "        self.state0 = stack_frame(None, inicio_juego, True) # inicialitzem l'stack d'estats amb el valor inicial de la primera observació\n",
    "\n",
    "\n",
    "    #################################################################################\n",
    "    ######TODO3: Prendre una nova acció ############################################\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = self.env.action_space.sample()  # acció aleatòria al burn-in\n",
    "        else:\n",
    "            action = self.main_network.get_action(self.state0, eps) # acció a partir del valor de Q (elecció de l'acció amb millor Q)\n",
    "            self.step_count += 1\n",
    "\n",
    "        #TODO: Realització de l'acció i obtenció del nou estat i la recompensa.\n",
    "        new_state, reward, done, _ = self.env.step(action) \n",
    "        new_state = stack_frame(self.state0, preprocess_observation(new_state), False)  # Recordar preprocessar els estats\n",
    "        self.total_reward += reward\n",
    "        self.buffer.append(self.state0, action, reward, done, new_state) # guardar experiència al buffer\n",
    "        self.state0 = new_state\n",
    "\n",
    "        #TODO: Resetejar entorn 'if done'\n",
    "        if done:\n",
    "            self.state0 = stack_frame(None, preprocess_observation(self.env.reset()), True)\n",
    "        return done\n",
    "    \n",
    "    ## Entrenament\n",
    "    def train(self, gamma=0.99, max_episodes=50000,\n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000, min_episodios=250, min_epsilon = 0.01):\n",
    "        self.gamma = gamma\n",
    "        # Omplim el buffer amb N experiències aleatòries ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        maximo = 0\n",
    "        while training:\n",
    "            self.state0 = stack_frame(None, preprocess_observation(self.env.reset()), True)\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # L'agent pren una acció\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "                #################################################################################\n",
    "                ####TODO 4: Actualitzar xarxa principal segons la freqüència establerta #########\n",
    "                if self.step_count % dnn_update_frequency == 0:\n",
    "                    self.update()\n",
    "                \n",
    "                ########################################################################################\n",
    "                ###TODO 6: Sincronitzar xarxa principal i xarxa objectiu segons la freqüència establerta\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(self.main_network.state_dict())                                        \n",
    "                                       \n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    ##################################################################\n",
    "                    ########TODO: Emmagatzemar epsilon, training rewards i loss#######\n",
    "                    self.sync_eps.append(self.epsilon)\n",
    "                    self.training_rewards.append(self.total_reward)                                                  \n",
    "                    self.update_loss = []\n",
    "\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ###TODO 7: Calcular la mitjana de recompensa dels últims X episodis, i emmagatzemar####\n",
    "                    mean_rewards = np.mean(self.training_rewards[-self.nblock:])\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "\n",
    "                    ##################################################################\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {} , Maxim {:.2f}\\t\\t\".format(episode, mean_rewards, self.epsilon,maximo), end=\"\")\n",
    "\n",
    "                    # Comprovar si s'ha arribat al màxim d'episodis\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "\n",
    "\n",
    "                    # Acaba el joc si la mitjana de recompenses ha arribat al llindar fixat per a aquest joc\n",
    "                    # i s'ha entrenat un mínim d'episodis\n",
    "                    if mean_rewards >= self.reward_threshold and min_episodios <  episode:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "\n",
    "                    #################################################################################\n",
    "                    ######TODO 8: Actualitzar epsilon ########\n",
    "                    # actualitzar epsilon segons la velocitat de descens fixada on no pot ser inferior a min_epsilon\n",
    "                    self.epsilon = max(self.epsilon * self.eps_decay, min_epsilon)\n",
    "\n",
    "\n",
    "    ####################################\n",
    "    ####TODO 5: Càlcul de la pèrdua ####\n",
    "    def calculate_loss(self, batch):\n",
    "        # Separem les variables de l'experiència i les convertim a tensors\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(device=self.device)\n",
    "        actions_vals = torch.LongTensor(np.array(actions)).to(device=self.device).reshape(-1,1)\n",
    "        dones_t = torch.ByteTensor(dones).to(device=self.device)\n",
    "\n",
    "        # Obtenim els valors de Q de la xarxa principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states), 1, actions_vals)\n",
    "        # Obtenim els valors de Q de la xarxa objectiu\n",
    "        # El paràmetre detach() evita que aquests valors actualitzin la xarxa objectiu\n",
    "        qvals_next = torch.max(self.target_network.get_qvals(next_states), dim=-1)[0].detach()\n",
    "        qvals_next[dones_t] = 0 # 0 en estats terminals\n",
    "\n",
    "        #################################################################################\n",
    "        ###TODO: Calcular l'equació de Bellman\n",
    "        expected_qvals = rewards_vals + self.gamma * qvals_next\n",
    "        \n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        return loss\n",
    "\n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminem qualsevol gradient passat\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) # seleccionem un conjunt del buffer\n",
    "        loss = self.calculate_loss(batch)# calculem la pèrdua\n",
    "        loss.backward() # calculem la diferència per obtenir els gradients\n",
    "        self.main_network.optimizer.step() # apliquem els gradients a la xarxa neuronal\n",
    "        # Guardem els valors de pèrdua\n",
    "        if self.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Entrenament\n",
    "A continuació entrenarem el model amb els següents hiperparàmetres:\n",
    "   <ul>\n",
    "        <li>Velocitat d'aprenentatge: 0.001 </li>\n",
    "        <li>Mida del batch: 32</li>\n",
    "        <li>Capacitat màxima del buffer: 8000</li>\n",
    "        <li>Gamma: 0.99</li>\n",
    "        <li>Epsilon: 1, amb decaïment de 0.995 amb un mínim de 0.01</li>\n",
    "        <li>Nombre d'experiències per emplenar inicialment el buffer: 100</li>\n",
    "        <li>Nombre màxim d'episodis: 5000</li>\n",
    "        <li>Nombre mínim d'episodis: 250</li>\n",
    "        <li>Freqüència d'actualització de la xarxa neuronal: 100 </li>\n",
    "        <li>Freqüència de sincronització amb la xarxa objectiu: 5000</li>\n",
    "    </ul>\n",
    "\n",
    "És probable que amb els hiperparàmetres proposats no s'arribi a solucionar l'entorn (aconseguir una puntuació de 350 a la mitjana de 100 partides). L'objectiu principal és millorar els resultats de l'agent aleatori.\n",
    "\n",
    "Està permès realitzar modificacions als paràmetres presentats encara que, el darrer exercici d'aquesta PAC, consisteix a millorar els resultats obtinguts amb aquest entrenament. Per tant, encara que sigui permès, no és recomanable.\n",
    "\n",
    "Si l'alumne no disposa de temps suficient pot escurçar el temps d'entrenament fixant el nombre màxim d'episodis = 3000 en lloc dels 5000 proposats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>2.8 Exercici (0.1 pts):</strong> Declarar els hiperparàmetres, carregueu el model de xarxa neuronal i entreneu l'agent\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001          #Velocitat d'aprenentatge\n",
    "BATCH_SIZE = 32     #Conjunt a agafar del buffer per a la xarxa neuronal\n",
    "MEMORY_SIZE = 8000  #Màxima capacitat del buffer\n",
    "GAMMA = 0.99        #Valor gamma de l'equació de Bellman\n",
    "EPSILON = 1         #Valor inicial d'epsilon\n",
    "EPSILON_DECAY = .99 #Decaïment d'epsilon\n",
    "EPSILON_MIN = 0.01  #Valor mínim d'epsilon\n",
    "BURN_IN = 100       #Nombre d'episodis inicials utilitzats per emplenar el buffer abans d'entrenar\n",
    "MAX_EPISODES = 5000 #Nombre màxim d'episodis (l'agent ha d'aprendre abans d'arribar a aquest valor)\n",
    "MIN_EPISODES = 250  #Nombre mínim d'episodis\n",
    "DNN_UPD = 100       #Freqüència d'actualització de la xarxa neuronal\n",
    "DNN_SYNC = 5000     #Freqüència de sincronització de pesos entre la xarxa neuronal i la xarxa objectiu\n",
    "\n",
    "REWARD_THRESHOLD = 350 #Llindar de recompensa on es considera que s'ha assolit el problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling replay buffer...\n",
      "Training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21620/2690432967.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# entrenem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_EPISODES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnn_update_frequency\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDNN_UPD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnn_sync_frequency\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDNN_SYNC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21620/1542222085.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, gamma, max_episodes, batch_size, dnn_update_frequency, dnn_sync_frequency, min_episodios, min_epsilon)\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mgamedone\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[1;31m# L'agent pren una acció\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m                 \u001b[0mgamedone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[1;31m#################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21620/1542222085.py\u001b[0m in \u001b[0;36mtake_step\u001b[1;34m(self, eps, mode)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m#TODO: Resetejar entorn 'if done'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21620/1460043782.py\u001b[0m in \u001b[0;36mstack_frame\u001b[1;34m(stacked_frames, frame, is_new)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mstacked_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mstacked_frames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstacked_frames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mstacked_frames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstacked_frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# instanciem la classe del buffer\n",
    "buffer = experienceReplayBuffer(memory_size=MEMORY_SIZE, burn_in=BURN_IN)\n",
    "\n",
    "# instanciem el model de xarxa neuronal\n",
    "dqn = DQN_CNN(env, learning_rate=lr)\n",
    "\n",
    "# instanciem l'agent\n",
    "agent = DQNAgent(env, dqn, buffer, REWARD_THRESHOLD, EPSILON, EPSILON_DECAY, BATCH_SIZE)\n",
    "\n",
    "# entrenem\n",
    "agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES, batch_size=BATCH_SIZE, dnn_update_frequency=DNN_UPD, dnn_sync_frequency=DNN_SYNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>2.9 Exercici (0.1 pts):</strong> Representar:\n",
    "    <ol>\n",
    "        <li>Gràfic amb les recompenses obtingudes al llarg de l'entrenament, l'evolució de les recompenses mitjanes i el llindar de recompensa establert per l'entorn.</li>\n",
    "        <li>Gràfic amb l'evolució de la pèrdua al llarg de l'entrenament</li>\n",
    "        <li>Gràfic amb l'evolució d'epsilon durant l'entrenament</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>2.10 Exercici (0.1 pts):</strong> Comenta els resultats\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Solució:</strong>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>2.11 Exercici (0.1 pts):</strong> Guardar el model entrenat en format \".pth\" amb el nom \"agentDQN_Trained_Model_dqn_cnn.pth\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Test de l'agent entrenat\n",
    "\n",
    "Un cop entrenat l'agent, ens interessa comprovar com ha après de bé, si el \"robot\" és capaç de realitzar les tasques apreses. Per això, recuperem el model entrenat i deixem que l'agent prengui accions aleatòries segons aquest model i n'observem el comportament."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>2.12 Exercici (0.2 pts):</strong> Carregar el model entrenat i executar l'agent entrenat durant 20 episodis consecutius. Calcular la suma de recompenses per a cada episodi. Mostrar en un gràfic la suma de les recompenses respecte dels episodis, incloent-hi el llindar de recompensa establert per l'entorn.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentDQN.main_network.load_state_dict(torch.load(\"agentDQN_Trained_Model_dqn_cnn.pth\"))\n",
    "##TODO: Calcular la suma de recompenses\n",
    "\n",
    "##TODO: realitzar les gràfiques que es demanen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>2.13 Anàlisi (0.1 pts):</strong> Emmagatzema una partida d'exemple de l'agent a la carpeta vídeos en format GIF per poder visualitzar-ne el comportament (es dóna el codi fet) i comenta el comportament de l'agent entrenat.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v4', render_mode='rgb_array')\n",
    "\n",
    "# Funció per a jugar i enmagatzemar una partida en format GIF amb un agent entrenat\n",
    "def save_agent_gif(env, ag, nombre_fichero):\n",
    "    '''\n",
    "    :param env:  entorn GYM\n",
    "    :param ag:  agent entrenat\n",
    "    :param nombre_fichero:  nom del fitxer\n",
    "    :return:\n",
    "    '''\n",
    "    frames = []\n",
    "    env.reset()\n",
    "    observation = env.reset()\n",
    "    incio_juego = preprocess_observation(observation)\n",
    "    state= stack_frame(None, incio_juego, True)\n",
    "    total_reward = 0\n",
    "    t=0\n",
    "    while True:\n",
    "            state= stack_frame(state, preprocess_observation(observation), False)\n",
    "            frame = env.render(mode='rgb_array')\n",
    "            frames.append(_label_with_text(frame))\n",
    "            action = ag.main_network.get_action(state,epsilon=0.0)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            t=t+1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "    try:\n",
    "        os.makedirs('videos')\n",
    "    except:\n",
    "        pass\n",
    "    imageio.mimwrite(os.path.join('./videos/', nombre_fichero), frames, fps=60)\n",
    "\n",
    "# Juguem i enmagatzemem una partida amb l'agent entrenat \n",
    "save_agent_gif(env,agentDQN, 'agent_dqn.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Resposta (comentaris):</strong>\n",
    "<br><br>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pot ser que el resultat sigui molt semblant al següent GIF. No us preocupeu, amb els paràmetres escollits és difícil obtenir un millor rendiment.\n",
    "\n",
    "![title](videos/space_invader_dqn_comportamiento_continuo.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent Dueling DQN (2.2 pts)\n",
    "\n",
    "En aquest apartat resoldrem el mateix entorn amb les mateixes característiques per a l'agent, però usant una dueling DQN. Com en el cas anterior, primer definirem el model de xarxa neuronal, després descriurem el comportament de l'agent, l'entrenarem i, finalment, en testejarem el funcionament.\n",
    "\n",
    "### 3.1 Definició de l'arquitectura de la xarxa neuronal\n",
    "\n",
    "\n",
    "L'objectiu principal de les dueling DQN és \"estalviar-se\" el càlcul del valor de Q en aquells estats on és irrellevant l'acció que es prengui. Per això es descompon la funció Q en dos components:\n",
    "\n",
    "\n",
    "$$Q(s, a) = A(s, a) + V (s)$$\n",
    "\n",
    "\n",
    "Aquesta descomposició es fa a nivell de l'arquitectura de la xarxa neuronal. Les primeres capes que teníem a la DQN seran comuns, i després la xarxa es dividirà en dues parts separades definides per la resta de capes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La descomposició en subxarxes del model de la DQN implementada a l'apartat anterior serà llavors:\n",
    "<ol>\n",
    "     <li>Bloc comú, la xarxa CNN</li>\n",
    "     <li>Xarxa advantage A(s,a):</li>\n",
    "          <ul>\n",
    "              <li>Una primera capa completament connectada (representada en pytorch per nn.Lineal) de 512 neurones, amb activació ReLU</li>\n",
    "              <li>Una darrera capa completament connectada. Aquesta serà la nostra capa de sortida i, per tant, el nombre de neurones de sortida dependrà del tipus de xarxa, A(s,a) en aquest cas, i tindrà tantes neurones com dimensions tingui l'espai d'accions.</li>\n",
    "              </ul>\n",
    "     <li>Xarxa value V(s):</li>\n",
    "             <ul>\n",
    "              <li>Una primera capa completament connectada (representada en pytorch per nn.Lineal) de 512 neurones, amb activació ReLU</li>\n",
    "              <li>Una darrera capa completament connectada. Aquesta serà la nostra capa de sortida amb un valor per estat.</li>\n",
    "              </ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>3.1 Exercici (0.75 pts):</strong> Implementar la classe <code>duelingDQN()</code>. Inicialitzar les variables necessàries i definir el model funcional de xarxa neuronal indicat.\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se us proporciona el codi pre-implementat. La implementació que es demana a l'enunciat està indicada als blocs <i>TODO</i> i/o amb variables igualades a <i>None</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class duelingDQN(torch.nn.Module):\n",
    "    ###################################\n",
    "    ###TODO: inicialització i model ###\n",
    "    def __init__(self, env, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        input_shape: mida de l'espai d'estats\n",
    "        n_outputs: mida de l'espai d'accions\n",
    "        actions: array d'accions possibles\n",
    "        device: cpu o cuda\n",
    "        red_cnn: definició de la xarxa convolucional\n",
    "        value: definició de la xarxa neuronal value\n",
    "        advantage: definició de la xarxa neuronal advantage\n",
    "        \"\"\"\n",
    "        ###################################\n",
    "        ###TODO: Inicialitzar variables####\n",
    "        super(duelingDQN, self).__init__()\n",
    "        self.input_shape = None\n",
    "        self.n_outputs = None\n",
    "        self.actions = None\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "        ########################################\n",
    "        ##TODO: Construcció de la xarxa neuronal\n",
    "        self.red_cnn = None\n",
    "        if torch.cuda.is_available():\n",
    "            self.red_cnn.cuda()\n",
    "        self.advantage = None\n",
    "        self.value =None\n",
    "\n",
    "        ### S'ofereix l'opció de treballar amb cuda\n",
    "        if self.device == 'cuda':\n",
    "            self.value.cuda()\n",
    "            self.advantage.cuda()\n",
    "        self.fc_layer_inputs = self.feature_size()\n",
    "        \n",
    "        ### Inicialitzem l'optimitzador\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    #######################################\n",
    "    #####TODO: funció forward#############\n",
    "    def forward(self, x):\n",
    "        return\n",
    "\n",
    "    ### Métode e-greedy\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)  # acció aleatòria\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)  # acció a partir del càlcul del valor de Q per a aquesta acció\n",
    "            action= torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array(state)\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        return self.forward(state_t)\n",
    "\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.red_cnn(autograd.Variable( torch.zeros(1, * self.input_shape)).to(device=self.device)).view(1, -1).size(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per al buffer de repetició d'experiències podem fer servir exactament la mateixa classe 'experienceReplayBuffer' descrita a l'apartat anterior de la DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Definició de l'agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diferència entre la DQN i la dueling DQN se centra, com hem vist, a la definició de l'arquitectura de la xarxa. Però el procés daprenentatge i actualització és exactament el mateix. Així, podem recuperar la classe implementada a l'apartat anterior, DQNAgent() i reutilitzar-la aquí sota el nom de duelingDQNAgent(). L'únic que cal fer és afegir l'optimitzador entre les variables a declarar i adaptar la funció de pèrdua al format Functional de pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>3.2 Exercici (0.5 pts):</strong> Implementar la classe <code>duelingDQNAgent()</code> com la <code>DQNAgent()</code>\n",
    "<p>\n",
    "</p>\n",
    "De nou, durant el procés s'han d'emmagatzemar (*):\n",
    "     <ul>\n",
    "         <li>Les recompenses obtingudes a cada pas de l'entrenament</li>\n",
    "         <li>Les recompenses mitjanes dels 100 episodis anteriors</li>\n",
    "         <li>La pèrdua durant l'entrenament</li>\n",
    "         <li>L'evolució d'epsilon durant l'entrenament</li>\n",
    "     </ul>\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se us proporciona el codi pre-implementat. La implementació que es demana a l'enunciat està indicada als blocs <i>TODO</i> i/o amb variables igualades a <i>None</i>, excepte en quin moment emmagatzemar les variables que s'indiquen a (*).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class duelingDQNAgent:\n",
    " ###################################################\n",
    "    ######TODO 1: declarar variables ##################\n",
    "    def __init__(self, env, main_network,\n",
    "                 buffer, reward_threshold,\n",
    "                 epsilon=0.1, eps_decay=0.99, batch_size=32):\n",
    "        \"\"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorn\n",
    "        dnnetwork: classe amb la xarxa neuronal dissenyada\n",
    "        target_network: xarxa objectiu\n",
    "        buffer: classe amb el buffer de repetició d'experiències\n",
    "        epsilon: epsilon\n",
    "        eps_decay: epsilon decay\n",
    "        batch_size: batch size\n",
    "        nblock: bloc dels X darrers episodis dels quals es calcularà la mitjana de recompensa\n",
    "        reward_threshold: llindar de recompensa definit a l'entorn\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        self.env = env\n",
    "        self.main_network = main_network\n",
    "        self.target_network = deepcopy(main_network) # xarxa objectiu (còpia de la principal)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.nblock = None\n",
    "        self.reward_threshold = None\n",
    "        self.initialize()\n",
    "\n",
    "    ###############################################################\n",
    "    ####TODO 2: inicialitzar variables extra que es necessiten ####\n",
    "    def initialize(self):\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.state0 = None\n",
    "\n",
    "\n",
    "    #################################################################################\n",
    "    ######TODO 3: Prendre una nova acció ############################################\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = self.env.action_space.sample()  # acció aleatòria al burn-in\n",
    "        else:\n",
    "            action = self.main_network.get_action(self.state0, eps)# acció a partir del valor de Q (elecció de lacció amb millor Q)\n",
    "            self.step_count += 1\n",
    "\n",
    "        #TODO: Realització de l'acció i l'obtenció del nou estat i la recompensa\n",
    "        new_state, reward, done, _ = None  \n",
    "\n",
    "        self.total_reward += reward\n",
    "        self.buffer.append(self.state0, action, reward, done, new_state) # guardar experiència al buffer\n",
    "        self.state0 = new_state.copy()\n",
    "\n",
    "        #TODO: resetejar entorn 'if done'\n",
    "        if done:\n",
    "            incio_juego = preprocess_observation(self.env.reset())\n",
    "            self.state0 =None\n",
    "        return done\n",
    "\n",
    "\n",
    "\n",
    "    ## Entrenament\n",
    "    def train(self, gamma=0.99, max_episodes=50000,\n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000, min_episodios=250, min_epsilon = 0.01):\n",
    "        self.gamma = gamma\n",
    "        # Omplim el buffer amb N experiències aleatòries ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        maximo = 0\n",
    "        while training:\n",
    "            self.state0 = None\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # L'agent pren una acció\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "\n",
    "                #################################################################################\n",
    "                #####TODO 4: Actualitzar la xarxa principal segons la freqüència establerta #####\n",
    "\n",
    "\n",
    "                ########################################################################################\n",
    "                ###TODO 6: Sincronitzar xarxa principal i xarxa objectiu segons la freqüència establerta\n",
    "\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    ##################################################################\n",
    "                    ########TODO: Emmagatzemar epsilon, training rewards i loss#######\n",
    "\n",
    "                    ####\n",
    "                    self.update_loss = []\n",
    "\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ###TODO 7: calcular la mitjana de recompensa dels últims X episodis, i emmagatzemar####\n",
    "                    mean_rewards = None\n",
    "                    ###\n",
    "\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {} , Maxim {:.2f}\\t\\t\".format(\n",
    "                        episode, mean_rewards, self.epsilon,maximo), end=\"\")\n",
    "\n",
    "                    # Comprovar si s'ha arribat al màxim d'episodis\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "\n",
    "\n",
    "                    # Acaba el joc si la mitjana de recompenses ha arribat al llindar fixat per a aquest joc\n",
    "                    # i s'ha entrenat un mínim d'episodis\n",
    "                    if mean_rewards >= self.reward_threshold and min_episodios <  episode:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "\n",
    "                    #################################################################################\n",
    "                    ######TODO 8: Actualitzar epsilon ########\n",
    "                    self.epsilon = None\n",
    "\n",
    "\n",
    "     ## Càlcul de la pèrdua\n",
    "    def calculate_loss(self, batch):\n",
    "        # Separem les variables de l'experiència i les convertim a tensors\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(device=self.device).reshape(-1,1)\n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(\n",
    "            device=self.device)\n",
    "        dones_t = torch.ByteTensor(dones).to(device=self.device)\n",
    "\n",
    "        # Obtenim els valors de Q de la xarxa principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states), 1, actions_vals)\n",
    "\n",
    "        #DQN update#\n",
    "        next_actions = torch.max(self.main_network.get_qvals(next_states), dim=-1)[1]\n",
    "        if self.device == 'cuda':\n",
    "            next_actions_vals = next_actions.reshape(-1,1).to(\n",
    "                device=self.device)\n",
    "        else:\n",
    "            next_actions = torch.max(self.dnnetwork.get_qvals(next_states), dim=-1)[1]\n",
    "        # Obtenim els valors de Q de la xarxa objectiu\n",
    "        target_qvals = self.target_network.get_qvals(next_states)\n",
    "        qvals_next = torch.gather(target_qvals, 1, next_actions_vals).detach()\n",
    "        #####\n",
    "\n",
    "        qvals_next[dones_t] = 0 # 0 en estats terminals\n",
    "\n",
    "        #TODO: Calculem l'equació de Bellman\n",
    "        expected_qvals = None\n",
    "        \n",
    "        # Modifiquem la funció de Loss per al mode Functional######\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "      \n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminem qualsevol gradient passat\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) # seleccionem un conjunt del buffer\n",
    "        loss = self.calculate_loss(batch) # calculem la pèrdua\n",
    "        loss.backward() # calculem la diferència per obtenir els gradients\n",
    "        self.main_network.optimizer.step() # apliquem els gradients a la xarxa neuronal\n",
    "        # Desem els valors de pèrdua\n",
    "        if self.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Entrenamient\n",
    "A continuació entrenarem el model dueling DQN amb els mateixos hiperparàmetres amb què entrenem la DQN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>3.3 Exercici (0.1 pts):</strong> Carregar el model de xarxa neuronal i entrenar l'agent amb els mateixos hiperparàmetres usats per a la DQN\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>3.4 Exercici (0.2 ptos):</strong> Mostrar els mateixos gràfics que amb la DQN:\n",
    "     <ol>\n",
    "         <li>Recompenses obtingudes al llarg de l'entrenament i l'evolució de les recompenses mitjanes cada 100 episodis, juntament amb el llindar de recompensa establert per l'entorn</li>\n",
    "         <li>Pèrdua durant l'entrenament</li>\n",
    "         <li>Evolució d'epsilon durant l'entrenament</li>\n",
    "     </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>3.5 Exercici (0.1 pts):</strong> Guardar el model entrenat en format “.pth”.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>3.6 Anàlisi (0.1 pto):</strong> Comenta els resultats obtinguts\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Solució (Comentaris):</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Test de l'agent\n",
    "Finalment, analitzem el comportament de l'agent entrenat.\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>3.7 Exercici (0.2 pts):</strong> Carregar el model entrenat i executar l'agent entrenat durant 20 episodis consecutius. Calculeu la suma de recompenses per cada episodi. Mostrar en un gràfic la suma de les recompenses respecte dels episodis, incloent-hi el llindar de recompensa establert per l'entorn.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>3.8 Anàlisi (0.25 pts):</strong> Emmagatzema una partida d'exemple de l'agent a la carpeta de vídeos en format GIF per poder visualitzar el seu comportament i comenta el comportament de l'agent entrenat.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v4', render_mode='rgb_array')\n",
    "save_agent_gif(env,duelingDQNAgent , 'space_invader_duelingDQNAgent_comportamiento.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Solució (Comentaris):</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. REINFORCE with baseline (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Definició de l'arquitectura de la xarxa neuronal\n",
    "Utilitzarem un model Seqüencial amb la configuració següent:\n",
    "\n",
    "<ul>\n",
    "    <li>Una primera capa del tipus Conv2d amb els paràmetres següents in_channels=(8,64,64), out_channels=32, kernel_size=8, stride=4 amb activació ReLU.</li>\n",
    "    <li>Una segona capa del tipus Conv2d amb els paràmetres següents in_channels=32, out_channels=64, kernel_size=4, stride=2 amb activació ReLU.</li>\n",
    "    <li>Una tercera capa del tipus Conv2d amb els paràmetres següents in_channels=64, out_channels=64, kernel_size=3, stride=1 amb activació ReLU.</li>\n",
    "</ul>\n",
    "\n",
    "El resultat d´aquesta primera xarxa el connectarem amb un model complementament connectat amb la següent estructura:\n",
    "\n",
    "* Una primera capa completament connectada (representada en pytorch per nn.Lineal) de 512 neurones, bias=True i amb activació Tanh\n",
    "* Una darrera capa completament connectada. Aquesta serà la nostra capa de sortida i, per tant, tindrà tantes neurones com dimensions tingui el nostre espai d‟accions (una sortida per cada acció possible), bias=True i activació Softmax (dim=-1).\n",
    "\n",
    "Finalment, farem servir l'optimitzador Adam per entrenar la xarxa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.1 (0.5 ptos):</strong> Implementar la classe <code>PGReinforce()</code>. Inicialitzar les variables necessàries i definir el model Seqüencial de xarxa neuronal indicat.\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se us proporciona el codi pre-implementat. La implementació que es demana a l'enunciat està indicada als blocs <i>TODO</i> i/o amb variables igualades a <i>None</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGReinforce(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env, learning_rate=1e-3, device='cpu'):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        n_inputs: mida de l'espai d'estats\n",
    "        n_outputs: mida de l'espai d'accions\n",
    "        actions: array d'accions possibles\n",
    "        \"\"\"\n",
    "        super(PGReinforce, self).__init__()\n",
    "         ###################################\n",
    "        ####TODO: Inicialitzar variables####\n",
    "        self.input_shape =None\n",
    "        self.n_outputs = None\n",
    "        self.actions = None\n",
    "        self.device = 'cpu'\n",
    "        self.learning_rate = learning_rate\n",
    "        ######\n",
    "\n",
    "        ########################################\n",
    "        ##TODO: Construcció de la xarxa neuronal\n",
    "        self.red_cnn = None\n",
    "        if self.device == 'cuda':\n",
    "            self.red_cnn.cuda()\n",
    "\n",
    "\n",
    "        self.red_lineal = None\n",
    "\n",
    "        ### S'ofereix l'opció de treballar amb cuda\n",
    "        if self.device == 'cuda':\n",
    "            self.red_lineal.cuda()\n",
    "        self.fc_layer_inputs = self.feature_size()\n",
    "\n",
    "        #######################################\n",
    "        ##TODO: Inicialitzar l'optimizador\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        #####\n",
    "\n",
    "    # Obtenció de les probabilitats de les possibles accions\n",
    "    def get_action_prob(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array(state)\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        cnn_out = self.red_cnn(state_t).reshape(-1,  self.fc_layer_inputs)\n",
    "        return self.red_lineal(cnn_out)\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.red_cnn(autograd.Variable( torch.zeros(1, * self.input_shape)).to(device=self.device)).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Definición del agente\n",
    "<p></p>\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>4.2 Exercici (0.55 pts):</strong> Implementar els punts següents de la classe <code>reinforceAgent()</code>:\n",
    "<ol>\n",
    "         <li>Declarar les variables de la classe</li>\n",
    "         <li>Inicialitzar les variables necessàries</li>\n",
    "         <li>Implementar l'acció que cal prendre</li>\n",
    "         <li>Calcular el <i>discounted reward</i> usant com a línia de base la mitjana estandarditzada del retorn\n",
    "                $$ \\frac{x_i - \\bar{x}}{\\sigma_x}$$</li>\n",
    "         <li>Calcular la mitjana de recompenses dels últims 100 episodis</li>\n",
    "         <li>Implementar la pèrdua per actualització</li>\n",
    "     </ol>\n",
    "A més, durant el procés s'han d'emmagatzemar (*):\n",
    "     <ul>\n",
    "         <li>Les recompenses obtingudes a cada pas de l'entrenament</li>\n",
    "         <li>Les recompenses mitjanes dels 100 episodis anteriors</li>\n",
    "         <li>La pèrdua durant l'entrenament</li>\n",
    "     </ul>\n",
    "\n",
    "    -----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se us proporciona el codi pre-implementat. La implementació que es demana a l'enunciat està indicada als blocs <i>TODO</i> i/o amb variables igualades a <i>None</i>, excepte en quin moment emmagatzemar les variables que s'indiquen a (*).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reinforceAgent:\n",
    "\n",
    "    ###################################################\n",
    "    ######TODO 1: declarar variables ##################\n",
    "    def __init__(self, env, dnnetwork):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorno\n",
    "        dnnetwork: clase con la red neuronal diseñada\n",
    "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        reward_threshold: umbral de recompensa definido en el entorno\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.dnnetwork = None\n",
    "        self.nblock = None\n",
    "        self.reward_threshold =None\n",
    "        self.initialize()\n",
    "     #######\n",
    "\n",
    "    ###############################################################\n",
    "    ####TODO 2: inicialitzar variables extra que es necessitin ####\n",
    "    def initialize(self):\n",
    "       self.batch_rewards = []\n",
    "       self.batch_actions = []\n",
    "       self.batch_states = []\n",
    "       self.batch_counter = 1\n",
    "    ######\n",
    "\n",
    "    ## Entrenament\n",
    "    def train(self, gamma=0.99, max_episodes=2000, batch_size=10):\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        episode = 0\n",
    "        action_space = np.arange(self.env.action_space.n)\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            incio_juego = preprocess_observation(self.env.reset())\n",
    "            state0 = stack_frame(None, incio_juego, True)\n",
    "            episode_states = []\n",
    "            episode_rewards = []\n",
    "            episode_actions = []\n",
    "            gamedone = False\n",
    "\n",
    "            while gamedone == False:\n",
    "                ##########################################################\n",
    "                ######TODO 3: Prendre una nova acció #####################\n",
    "                action_probs = None  #distribució de probabilitat de les accions donat l'estat actual\n",
    "                action = None   #acció aleatòria de la distribució de probabilitat\n",
    "                next_state, reward, gamedone, _ = None\n",
    "                #######\n",
    "\n",
    "\n",
    "                # Emmagatzemem experiències que es van obtenint en aquest episodi\n",
    "                episode_states.append(state0)\n",
    "                episode_rewards.append(reward)\n",
    "                episode_actions.append(action)\n",
    "                next_state = stack_frame(state0, preprocess_observation(next_state), False)\n",
    "                state0 = next_state\n",
    "\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    # Calculem el terme del retorn menys la línia de base\n",
    "                    self.batch_rewards.extend(self.discount_rewards(episode_rewards))\n",
    "                    self.batch_states.extend(episode_states)\n",
    "                    self.batch_actions.extend(episode_actions)\n",
    "                    self.batch_counter += 1\n",
    "\n",
    "                    #####################################################################################\n",
    "                    ###TODO 5: calcular mitjana de recompenses dels últims X episodis, i emmagatzemar####\n",
    "                    mean_rewards = None\n",
    "                    ######\n",
    "\n",
    "\n",
    "                    # Actualitzem la xarxa quan es completa la mida del batch\n",
    "                    if self.batch_counter == self.batch_size:\n",
    "                        self.update(self.batch_states, self.batch_rewards, self.batch_actions)\n",
    "\n",
    "                        #######################################\n",
    "                        ###TODO : emmagatzemar training_loss###\n",
    "                        ###########\n",
    "\n",
    "                        self.update_loss = []\n",
    "\n",
    "                        # Resetejem les variables de l'episodi\n",
    "                        self.batch_rewards = []\n",
    "                        self.batch_actions = []\n",
    "                        self.batch_states = []\n",
    "                        self.batch_counter = 1\n",
    "\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f}\\t\\t\".format(\n",
    "                        episode, mean_rewards), end=\"\")\n",
    "\n",
    "                    # Comprovem que encara queden episodis\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "\n",
    "                    # Acaba el joc si la mitjana de recompenses ha arribat al llindar fixat per a aquest joc\n",
    "                    if mean_rewards >= self.reward_threshold:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "\n",
    "    ########################################################\n",
    "    ###TODO 4: càlcul del retorn menys la línia de base ####\n",
    "    def discount_rewards(self, rewards):\n",
    "        discount_r = np.zeros_like(rewards)\n",
    "        timesteps = range(len(rewards))\n",
    "        reward_sum = 0\n",
    "        for i in reversed(timesteps):\n",
    "            reward_sum = rewards[i] + self.gamma*reward_sum\n",
    "            discount_r[i] = reward_sum\n",
    "        baseline = None\n",
    "        return baseline\n",
    "    ##########\n",
    "\n",
    "\n",
    "    ## Actualizació\n",
    "    def update(self, batch_s, batch_r, batch_a):\n",
    "        self.dnnetwork.optimizer.zero_grad()  # eliminem qualsevol gradient passat\n",
    "        state_t = torch.FloatTensor(batch_s)\n",
    "        reward_t = torch.FloatTensor(batch_r)\n",
    "        action_t = torch.LongTensor(batch_a)\n",
    "        loss = self.calculate_loss(state_t, action_t, reward_t) # calculem la pèrdua\n",
    "        loss.backward() # calculem la diferència per obtenir els gradients\n",
    "        self.dnnetwork.optimizer.step() # apliquem els gradients a la xarxa neuronal\n",
    "        #Desem els valors de pèrdua\n",
    "        if self.dnnetwork.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "\n",
    "    #################################################\n",
    "    ###TODO 6: Càlcul de la pèrdua ##################\n",
    "    # Recordatori: cada actualització és proporcional al producte del retorn i el gradient de la probabilitat\n",
    "    # de prendre l'acció presa, dividit per la probabilitat de prendre aquesta acció (logaritme natural)\n",
    "    def calculate_loss(self, state_t, action_t, reward_t):\n",
    "        logprob = None\n",
    "        selected_logprobs = None\n",
    "        loss = None\n",
    "        return loss\n",
    "     ########\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Entrenament\n",
    "A continuació entrenarem el model amb els següents hiperparàmetres:\n",
    "<ul>\n",
    "     <li>Velocitat d'aprenentatge: 0.005</li>\n",
    "     <li>Mida del batch: 8</li>\n",
    "     <li>Gamma: 0.99</li>\n",
    "     <li>Nombre màxim d'episodis: 5000</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>4.3 Exercici (0.2 pts):</strong> Definir els hiperparàmetres, carregar el model de xarxa neuronal i entrenar l'agent.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>4.4 Exercici (0.1 pts):</strong> Mostra els gràfics següents:\n",
    "     <ol>\n",
    "         <li>Recompenses obtingudes al llarg de l'entrenament i l'evolució de les recompenses mitjanes cada 100 episodis, juntament amb el llindar de recompensa establert per l'entorn</li>\n",
    "         <li>Pèrdua durant l'entrenament</li>\n",
    "     </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>4.5 Anàlisi (0.2 ptos):</strong> Guardar el model entrenat en format “.pth” i comentar els resultats obtinguts. Què ha passat amb aquest model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>4.6 Exercici (0.15 pts):</strong> Carregar el model entrenat i executar l'agent entrenat durant 20 episodis consecutius. Calculeu la suma de recompenses per cada episodi. Mostrar en un gràfic la suma de les recompenses respecte dels episodis, incloent-hi el llindar de recompensa establert per l'entorn. Comentar-ne els resultats.\n",
    "<p>Emmagatzema una partida d'exemple de l'agent a la carpeta de vídeos en format GIF per poder visualitzar el seu comportament i comenta'l.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparació de models (1 pt)\n",
    "<p></p>\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>5.1 Exercici (0.5 pts):</strong> Mostra en un mateix gràfic l'evolució de la mitjana de recompenses dels tres models juntament amb el llindar de recompensa.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>5.2 Anàlisi (0.5 pts):</strong> Analitzar els resultats obtinguts tenint en compte el nombre depisodis, el temps dentrenament i el rendiment dels tres agents entrenats observat en apartats anteriors. Quin agent presenta un comportament millor? Per què?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Solució (Comentaris):</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimizació (1 pto)\n",
    "\n",
    "En aquest apartat volem trobar la millor arquitectura i hiperparàmetres per optimitzar la precisió del model. Els punts que volem tenir en compte per a la cerca del millor model són els següents:\n",
    "<ul>\n",
    "     <li>Nombre d'unitats de les capes</li>\n",
    "     <li>Learning rate</li>\n",
    "     <li>Actualització de la xarxa principal</li>\n",
    "     <li>Sincronització de la xarxa objectiu</li>\n",
    "     <li>Batch size</li>\n",
    "     <li>Gamma</li>\n",
    "     <li>Nombre de partides de l'entrenament</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podeu modificar o adaptar la configuració com considereu oportú sempre que justifiqueu el benefici i el canvi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>6.1 Exercici (1 pt):</strong> Triar un dels dos models de DQN implementats (DQN o dueling DQN) i experimentar amb diferents combinacions de valors per als paràmetres especificats. L'objectiu és aconseguir un model amb millors resultats que el presentat a la PAC. <b>Indicar</b> les proves realitzades, però presentar <b>únicament</b> l'execució i els resultats de la millor opció. Justificar els valors escollits d'hiperparàmetres per a les diferents proves realitzades i comentar els resultats d'entrenament i rendiment de l'agent entrenat.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Solució (Comentaris):</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "space_invader.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
